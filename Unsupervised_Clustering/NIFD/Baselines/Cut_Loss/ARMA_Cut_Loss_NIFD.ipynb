{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNDrmUWj/67iQmS02m+Yrts"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import torch.optim as optim\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as nnFn\n","from torch_geometric.data import Data\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss\n","import random\n","from torch_geometric.nn import ARMAConv\n","\n","fa_patients_path = \"/home/snu/Downloads/NIFD_Patients_FA_Histogram_Feature.npy\"\n","Patients_FA_array = np.load(fa_patients_path, allow_pickle=True)\n","\n","fa_controls_path = \"/home/snu/Downloads/NIFD_Control_FA_Histogram_Feature.npy\"\n","Controls_FA_array = np.load(fa_controls_path, allow_pickle=True)\n","\n","print(\"Patients Shape:\", Patients_FA_array.shape)\n","print(\"Controls Shape:\", Controls_FA_array.shape)\n","\n","X = np.vstack([Controls_FA_array, Patients_FA_array])\n","y = np.hstack([\n","    np.zeros(Controls_FA_array.shape[0], dtype=np.int64),  # 0 = Control\n","    np.ones(Patients_FA_array.shape[0], dtype=np.int64)    # 1 = Patient\n","])\n","\n","np.random.seed(42)\n","perm = np.random.permutation(X.shape[0])\n","X = X[perm]\n","y = y[perm]\n","\n","class MLP(nn.Module):\n","    def __init__(self, inp_size, outp_size, hidden_size):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(inp_size, hidden_size),\n","            nn.BatchNorm1d(hidden_size),\n","            nn.PReLU(),\n","            nn.Dropout(0.3),\n","            nn.Linear(hidden_size, outp_size)\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","class ARMAEncoder(torch.nn.Module):\n","    def __init__(self, input_dim, hidden_dim, device, activ, stacks=2, layers=1):\n","        super(ARMAEncoder, self).__init__()\n","        self.device = device\n","        self.arma = ARMAConv(input_dim, hidden_dim, num_stacks=stacks, num_layers=layers)\n","        self.batchnorm = nn.BatchNorm1d(hidden_dim)\n","        self.dropout = nn.Dropout(0.3)\n","        self.mlp = nn.Linear(hidden_dim, hidden_dim)\n","\n","    def forward(self, data):\n","        x, edge_index = data.x, data.edge_index\n","        x = self.arma(x, edge_index)\n","        x = self.dropout(x)\n","        x = self.batchnorm(x)\n","        logits = self.mlp(x)\n","        return logits\n","\n","class ARMA(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_clusters, device, activ):\n","        super(ARMA, self).__init__()\n","        self.device = device\n","        self.num_clusters = num_clusters\n","\n","        self.online_encoder = ARMAEncoder(input_dim, hidden_dim, device, activ)\n","\n","        activations = {\n","            \"SELU\": nnFn.selu,\n","            \"SiLU\": nnFn.silu,\n","            \"GELU\": nnFn.gelu,\n","            \"RELU\": nnFn.relu\n","        }\n","        self.act = activations.get(activ, nnFn.elu)\n","\n","        self.online_predictor = MLP(hidden_dim, num_clusters, hidden_dim)\n","\n","        # only modularity loss\n","        self.loss = self.cut_loss\n","\n","    def forward(self, data):\n","        x = self.online_encoder(data)\n","        logits = self.online_predictor(x)\n","        S = nnFn.softmax(logits, dim=1)\n","        return S, logits\n","\n","    def cut_loss(self, A, S):\n","        S = nnFn.softmax(S, dim=1)\n","        A_pool = torch.matmul(torch.matmul(A, S).t(), S)\n","        num = torch.trace(A_pool)\n","\n","        D = torch.diag(torch.sum(A, dim=-1))\n","        D_pooled = torch.matmul(torch.matmul(D, S).t(), S)\n","        den = torch.trace(D_pooled)\n","        mincut_loss = -(num / den)\n","\n","        St_S = torch.matmul(S.t(), S)\n","        I_S = torch.eye(self.num_clusters, device=self.device)\n","        ortho_loss = torch.norm(St_S / torch.norm(St_S) - I_S / torch.norm(I_S))\n","\n","        return mincut_loss + ortho_loss\n","\n","def create_adj(F, cut, alpha=1):\n","    F_norm = F / np.linalg.norm(F, axis=1, keepdims=True)\n","    W = np.dot(F_norm, F_norm.T)\n","    W = np.where(W >= alpha, 1, 0).astype(np.float32)\n","    W = (W / W.max()).astype(np.float32)\n","    return W\n","\n","def load_data(adj, node_feats):\n","    node_feats = torch.from_numpy(node_feats)\n","    edge_index = torch.from_numpy(np.array(np.nonzero((adj > 0))))\n","    row, col = edge_index\n","    edge_weight = torch.from_numpy(adj[row, col])\n","    return node_feats, edge_index, edge_weight\n","\n","\n","from torch.optim.lr_scheduler import StepLR\n","from torch.optim import AdamW\n","\n","results = []\n","\n","for run_seed in range(10):\n","    print(\"\\n================ Run\", run_seed, \"================\")\n","\n","    np.random.seed(run_seed)\n","    torch.manual_seed(run_seed)\n","    random.seed(run_seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(run_seed)\n","\n","    perm = np.random.permutation(X.shape[0])\n","    features = X[perm].astype(np.float32)\n","    labels = y[perm]\n","\n","    alpha = 0.5\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    feats_dim = 180\n","    K = 2\n","\n","    W0 = create_adj(features, 0, alpha)\n","    node_feats, edge_index, _ = load_data(W0, features)\n","    data0 = Data(x=node_feats, edge_index=edge_index).to(device)\n","    A1 = torch.from_numpy(W0).float().to(device)\n","\n","    model = ARMA(feats_dim, 512, K, device, \"ELU\").to(device)\n","    optimizer = AdamW(model.parameters(), lr=0.0001, weight_decay=0.0001)\n","    scheduler = StepLR(optimizer, step_size=200, gamma=0.5)\n","\n","    num_epochs = 5000\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        optimizer.zero_grad()\n","\n","        S, logits = model(data0)\n","        unsup_loss = model.loss(A1, logits)\n","\n","        total_loss = unsup_loss\n","        total_loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","\n","        if epoch % 500 == 0:\n","            print(f\"Epoch {epoch} | Loss: {total_loss:.4f}\")\n","\n","    model.eval()\n","    with torch.no_grad():\n","        S, logits = model(data0)\n","        y_pred = torch.argmax(logits, dim=1).cpu().numpy()\n","        y_pred_proba = nnFn.softmax(logits, dim=1).cpu().numpy()\n","\n","    acc_score = accuracy_score(labels, y_pred)\n","    acc_score_inverted = accuracy_score(labels, 1 - y_pred)\n","\n","    if acc_score_inverted > acc_score:\n","        acc_score = acc_score_inverted\n","        y_pred = 1 - y_pred\n","\n","    prec_score = precision_score(labels, y_pred)\n","    rec_score = recall_score(labels, y_pred)\n","    f1 = f1_score(labels, y_pred)\n","    log_loss_value = log_loss(labels, y_pred_proba)\n","\n","    print(\"Accuracy:\", acc_score, \"Precision:\", prec_score, \"Recall:\", rec_score, \"F1:\", f1)\n","\n","    results.append({\n","        \"seed\": run_seed,\n","        \"accuracy\": acc_score,\n","        \"precision\": prec_score,\n","        \"recall\": rec_score,\n","        \"f1\": f1,\n","        \"log_loss\": log_loss_value\n","    })\n","\n","\n","accs = [r[\"accuracy\"] for r in results]\n","precisions = [r[\"precision\"] for r in results]\n","recalls = [r[\"recall\"] for r in results]\n","f1s = [r[\"f1\"] for r in results]\n","\n","print(\"\\n===== Final Results across 10 runs =====\")\n","print(\"Accuracy: mean=\", np.mean(accs), \"std=\", np.std(accs))\n","print(\"Precision: mean=\", np.mean(precisions), \"std=\", np.std(precisions))\n","print(\"Recall: mean=\", np.mean(recalls), \"std=\", np.std(recalls))\n","print(\"F1: mean=\", np.mean(f1s), \"std=\", np.std(f1s))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-Y5_OmBuv5m8","executionInfo":{"status":"ok","timestamp":1764155087725,"user_tz":-330,"elapsed":242315,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}},"outputId":"22b0630e-9d8c-4d86-d06c-09d8ca38936e"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Patients Shape: (98, 180)\n","Controls Shape: (48, 180)\n","\n","================ Run 0 ================\n","Epoch 0 | Loss: -0.2356\n","Epoch 500 | Loss: -0.5005\n","Epoch 1000 | Loss: -0.5007\n","Epoch 1500 | Loss: -0.5007\n","Epoch 2000 | Loss: -0.5008\n","Epoch 2500 | Loss: -0.5007\n","Epoch 3000 | Loss: -0.5006\n","Epoch 3500 | Loss: -0.5009\n","Epoch 4000 | Loss: -0.5006\n","Epoch 4500 | Loss: -0.5007\n","Accuracy: 0.6506849315068494 Precision: 0.821917808219178 Recall: 0.6122448979591837 F1: 0.7017543859649122\n","\n","================ Run 1 ================\n","Epoch 0 | Loss: -0.2359\n","Epoch 500 | Loss: -0.5003\n","Epoch 1000 | Loss: -0.5004\n","Epoch 1500 | Loss: -0.5004\n","Epoch 2000 | Loss: -0.5003\n","Epoch 2500 | Loss: -0.5004\n","Epoch 3000 | Loss: -0.5003\n","Epoch 3500 | Loss: -0.5004\n","Epoch 4000 | Loss: -0.5005\n","Epoch 4500 | Loss: -0.5004\n","Accuracy: 0.636986301369863 Precision: 0.8082191780821918 Recall: 0.6020408163265306 F1: 0.6900584795321637\n","\n","================ Run 2 ================\n","Epoch 0 | Loss: -0.2347\n","Epoch 500 | Loss: -0.4989\n","Epoch 1000 | Loss: -0.4997\n","Epoch 1500 | Loss: -0.4999\n","Epoch 2000 | Loss: -0.4999\n","Epoch 2500 | Loss: -0.4999\n","Epoch 3000 | Loss: -0.5000\n","Epoch 3500 | Loss: -0.5000\n","Epoch 4000 | Loss: -0.4999\n","Epoch 4500 | Loss: -0.5000\n","Accuracy: 0.5 Precision: 0.6712328767123288 Recall: 0.5 F1: 0.5730994152046783\n","\n","================ Run 3 ================\n","Epoch 0 | Loss: -0.2344\n","Epoch 500 | Loss: -0.5003\n","Epoch 1000 | Loss: -0.5005\n","Epoch 1500 | Loss: -0.5004\n","Epoch 2000 | Loss: -0.5005\n","Epoch 2500 | Loss: -0.5004\n","Epoch 3000 | Loss: -0.5004\n","Epoch 3500 | Loss: -0.5004\n","Epoch 4000 | Loss: -0.5003\n","Epoch 4500 | Loss: -0.5003\n","Accuracy: 0.5684931506849316 Precision: 0.7397260273972602 Recall: 0.5510204081632653 F1: 0.631578947368421\n","\n","================ Run 4 ================\n","Epoch 0 | Loss: -0.2387\n","Epoch 500 | Loss: -0.4998\n","Epoch 1000 | Loss: -0.4998\n","Epoch 1500 | Loss: -0.4999\n","Epoch 2000 | Loss: -0.4999\n","Epoch 2500 | Loss: -0.5000\n","Epoch 3000 | Loss: -0.5000\n","Epoch 3500 | Loss: -0.4998\n","Epoch 4000 | Loss: -0.4999\n","Epoch 4500 | Loss: -0.4999\n","Accuracy: 0.541095890410959 Precision: 0.7123287671232876 Recall: 0.5306122448979592 F1: 0.6081871345029239\n","\n","================ Run 5 ================\n","Epoch 0 | Loss: -0.2356\n","Epoch 500 | Loss: -0.5002\n","Epoch 1000 | Loss: -0.5004\n","Epoch 1500 | Loss: -0.5001\n","Epoch 2000 | Loss: -0.5004\n","Epoch 2500 | Loss: -0.5001\n","Epoch 3000 | Loss: -0.5004\n","Epoch 3500 | Loss: -0.5002\n","Epoch 4000 | Loss: -0.5004\n","Epoch 4500 | Loss: -0.5004\n","Accuracy: 0.5958904109589042 Precision: 0.7671232876712328 Recall: 0.5714285714285714 F1: 0.6549707602339181\n","\n","================ Run 6 ================\n","Epoch 0 | Loss: -0.2339\n","Epoch 500 | Loss: -0.5002\n","Epoch 1000 | Loss: -0.5003\n","Epoch 1500 | Loss: -0.5003\n","Epoch 2000 | Loss: -0.5004\n","Epoch 2500 | Loss: -0.5003\n","Epoch 3000 | Loss: -0.5003\n","Epoch 3500 | Loss: -0.5004\n","Epoch 4000 | Loss: -0.5003\n","Epoch 4500 | Loss: -0.5003\n","Accuracy: 0.6232876712328768 Precision: 0.7945205479452054 Recall: 0.5918367346938775 F1: 0.6783625730994152\n","\n","================ Run 7 ================\n","Epoch 0 | Loss: -0.2353\n","Epoch 500 | Loss: -0.5004\n","Epoch 1000 | Loss: -0.5006\n","Epoch 1500 | Loss: -0.5007\n","Epoch 2000 | Loss: -0.5006\n","Epoch 2500 | Loss: -0.5002\n","Epoch 3000 | Loss: -0.5005\n","Epoch 3500 | Loss: -0.5008\n","Epoch 4000 | Loss: -0.5007\n","Epoch 4500 | Loss: -0.5006\n","Accuracy: 0.6917808219178082 Precision: 0.863013698630137 Recall: 0.6428571428571429 F1: 0.7368421052631579\n","\n","================ Run 8 ================\n","Epoch 0 | Loss: -0.2353\n","Epoch 500 | Loss: -0.4996\n","Epoch 1000 | Loss: -0.5003\n","Epoch 1500 | Loss: -0.5003\n","Epoch 2000 | Loss: -0.5003\n","Epoch 2500 | Loss: -0.5003\n","Epoch 3000 | Loss: -0.5003\n","Epoch 3500 | Loss: -0.5004\n","Epoch 4000 | Loss: -0.5004\n","Epoch 4500 | Loss: -0.5004\n","Accuracy: 0.636986301369863 Precision: 0.8082191780821918 Recall: 0.6020408163265306 F1: 0.6900584795321637\n","\n","================ Run 9 ================\n","Epoch 0 | Loss: -0.2326\n","Epoch 500 | Loss: -0.5005\n","Epoch 1000 | Loss: -0.5005\n","Epoch 1500 | Loss: -0.5006\n","Epoch 2000 | Loss: -0.4998\n","Epoch 2500 | Loss: -0.5006\n","Epoch 3000 | Loss: -0.5006\n","Epoch 3500 | Loss: -0.5006\n","Epoch 4000 | Loss: -0.5006\n","Epoch 4500 | Loss: -0.5008\n","Accuracy: 0.636986301369863 Precision: 0.8082191780821918 Recall: 0.6020408163265306 F1: 0.6900584795321637\n","\n","===== Final Results across 10 runs =====\n","Accuracy: mean= 0.6082191780821917 std= 0.054261102247502344\n","Precision: mean= 0.7794520547945205 std= 0.05426110224750235\n","Recall: mean= 0.5806122448979592 std= 0.04041898432722115\n","F1: mean= 0.6654970760233918 std= 0.04632819256219499\n"]}]}]}
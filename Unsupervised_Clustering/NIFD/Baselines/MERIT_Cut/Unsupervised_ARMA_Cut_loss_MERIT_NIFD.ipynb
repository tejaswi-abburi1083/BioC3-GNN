{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"T5aSgvBjOxj8"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RCsIte6uO5cI"},"outputs":[],"source":["import torch, torch_geometric\n","print(\"Torch:\", torch.__version__)\n","print(\"CUDA available:\", torch.cuda.is_available())\n","print(\"PyG:\", torch_geometric.__version__)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x8mZcDISO8cx"},"outputs":[],"source":["!pip install -q torch_geometric\n","!pip install -q class_resolver\n","!pip3 install pymatting"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I_51Esj6O_Jo"},"outputs":[],"source":["import torch.optim as optim\n","from tqdm import tqdm\n","import numpy as np\n","import torch\n","# import util\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","from numpy import asarray\n","import tifffile as tiff\n","import torch.nn as nn\n","import torch.nn.functional as nnFn\n","import torch_geometric.nn as pyg_nn\n","from torch_geometric.data import Data\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss\n","from sklearn.manifold import TSNE\n","import random\n","from torch_geometric.nn import ARMAConv\n","import copy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SQr5l2viT-h4"},"outputs":[],"source":["fa_patients_path = \"/home/snu/Downloads/NIFD_Patients_FA_Histogram_Feature.npy\"\n","Patients_FA_array = np.load(fa_patients_path, allow_pickle=True)\n","\n","fa_controls_path = \"/home/snu/Downloads/NIFD_Control_FA_Histogram_Feature.npy\"\n","Controls_FA_array = np.load(fa_controls_path, allow_pickle=True)\n","\n","X = np.vstack([Controls_FA_array, Patients_FA_array])\n","y = np.hstack([\n","    np.zeros(Controls_FA_array.shape[0], dtype=np.int64),  # 0 = Control\n","    np.ones(Patients_FA_array.shape[0], dtype=np.int64)    # 1 = Patient\n","])\n","\n","np.random.seed(42)\n","perm = np.random.permutation(X.shape[0])\n","X = X[perm]\n","y = y[perm]"]},{"cell_type":"markdown","metadata":{"id":"qGTVC3dfl6gQ"},"source":["## 1 layer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xr_ZShMtTI0w"},"outputs":[],"source":["def sim(h1, h2, tau = 0.2):\n","    z1 = nnFn.normalize(h1, dim=-1, p=2)\n","    z2 = nnFn.normalize(h2, dim=-1, p=2)\n","    return torch.mm(z1, z2.t()) / tau\n","\n","def contrastive_loss_wo_cross_network(h1, h2, z):\n","    f = lambda x: torch.exp(x)\n","    intra_sim = f(sim(h1, h1))\n","    inter_sim = f(sim(h1, h2))\n","    return -torch.log(inter_sim.diag() /\n","                     (intra_sim.sum(dim=-1) + inter_sim.sum(dim=-1) - intra_sim.diag()))\n","\n","def contrastive_loss_wo_cross_view(h1, h2, z):\n","    f = lambda x: torch.exp(x)\n","    cross_sim = f(sim(h1, z))\n","    return -torch.log(cross_sim.diag() / cross_sim.sum(dim=-1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9u3KA8lBUaWX"},"outputs":[],"source":["class MLP(nn.Module):\n","    def __init__(self, inp_size, outp_size, hidden_size):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(inp_size, hidden_size),\n","            nn.BatchNorm1d(hidden_size),\n","            nn.PReLU(), # nn.ELU()\n","            nn.Dropout(0.3),\n","            nn.Linear(hidden_size, outp_size)\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YaY22OlFZkb1"},"outputs":[],"source":["class ARMAEncoder(torch.nn.Module):\n","    def __init__(self, input_dim, hidden_dim, device, activ=\"ELU\", num_stacks=1, num_layers=1):\n","        super(ARMAEncoder, self).__init__()\n","        self.device = device\n","        # Define all activation functions\n","        activations = {\n","            \"SELU\": nnFn.selu,\n","            \"SiLU\": nnFn.silu,\n","            \"GELU\": nnFn.gelu,\n","            \"ELU\": nnFn.elu,\n","            \"RELU\": nnFn.relu\n","        }\n","        # Get the activation function based on the input string\n","        self.act = activations.get(activ, nnFn.elu)\n","\n","        self.arma = ARMAConv(\n","            in_channels=input_dim,\n","            out_channels=hidden_dim,\n","            num_stacks=num_stacks,   # number of parallel stacks\n","            num_layers=num_layers,   # depth per stack\n","            act=self.act,               # nonlinearity inside ARMA\n","            shared_weights=True,     # weight sharing across layers\n","            dropout=0.25             # ARMA-internal dropout\n","        )\n","        self.batchnorm = nn.BatchNorm1d(hidden_dim)\n","        self.dropout = nn.Dropout(0.3)\n","        self.mlp = nn.Linear(hidden_dim, hidden_dim)\n","\n","    def forward(self, data):\n","        x, edge_index = data.x, data.edge_index\n","        x = self.arma(x, edge_index)\n","        x = self.dropout(x)\n","        x = self.batchnorm(x)\n","        logits = self.mlp(x)\n","        return logits"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ykP3A7hbVvGR"},"outputs":[],"source":["class EMA(): # Moving Average update\n","\n","    def __init__(self, beta):\n","        super().__init__()\n","        self.beta = beta\n","\n","    def update_average(self, old, new):\n","        # old: old model parameter\n","        # new: new model parameter\n","        if old is None:\n","            return new\n","        return old * self.beta + (1 - self.beta) * new\n","\n","def update_moving_average(ema_updater, ma_model, current_model):\n","    for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):\n","        old_weight, up_weight = ma_params.data, current_params.data\n","        ma_params.data = ema_updater.update_average(old_weight, up_weight)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FB6kC-IdmxN4"},"outputs":[],"source":["class ARMA(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_clusters, device, activ, moving_average_decay=0.5, cut=True):\n","        super(ARMA, self).__init__()\n","        self.device = device\n","        self.num_clusters = num_clusters\n","        self.cut = cut\n","        self.beta = 0.6\n","\n","        # Use ARMA encoder instead of GCN encoder\n","        self.online_encoder = ARMAEncoder(input_dim, hidden_dim, device, activ)\n","        self.target_encoder = copy.deepcopy(self.online_encoder)\n","\n","        activations = {\n","            \"SELU\": nnFn.selu,\n","            \"SiLU\": nnFn.silu,\n","            \"GELU\": nnFn.gelu,\n","            \"RELU\": nnFn.relu\n","        }\n","        self.act = activations.get(activ, nnFn.elu)\n","\n","        # Predictor head\n","        self.online_predictor = MLP(hidden_dim, num_clusters, hidden_dim)\n","\n","        # Loss selection\n","        self.loss = self.cut_loss if cut else self.modularity_loss\n","        self.target_ema_updater = EMA(moving_average_decay)\n","\n","    def reset_moving_average(self):\n","        del self.target_encoder\n","        self.target_encoder = None\n","\n","    def update_ma(self):\n","        assert self.target_encoder is not None, 'target encoder has not been created yet'\n","        update_moving_average(self.target_ema_updater, self.target_encoder, self.online_encoder)\n","\n","    def forward(self, data1, data2):\n","        x1 = self.online_encoder(data1)\n","        logits1 = self.online_predictor(x1)\n","        S1 = nnFn.softmax(logits1, dim=1)\n","\n","        x2 = self.online_encoder(data2)\n","        logits2 = self.online_predictor(x2)\n","        S2 = nnFn.softmax(logits2, dim=1)\n","\n","        with torch.no_grad():\n","            target_proj_one = self.target_encoder(data1).detach()\n","            target_proj_two = self.target_encoder(data2).detach()\n","\n","        l1 = self.beta * contrastive_loss_wo_cross_network(x1, x2, target_proj_two) + \\\n","             (1.0 - self.beta) * contrastive_loss_wo_cross_view(x1, x2, target_proj_two)\n","\n","        l2 = self.beta * contrastive_loss_wo_cross_network(x2, x1, target_proj_one) + \\\n","             (1.0 - self.beta) * contrastive_loss_wo_cross_view(x2, x1, target_proj_one)\n","\n","        return S1, S2, logits1, logits2, l1, l2\n","\n","    def modularity_loss(self, A, S):\n","        C = nnFn.softmax(S, dim=1)\n","        d = torch.sum(A, dim=1)\n","        m = torch.sum(A)\n","        B = A - torch.ger(d, d) / (2 * m)\n","\n","        I_S = torch.eye(self.num_clusters, device=self.device)\n","        # k = torch.norm(I_S)\n","        k = torch.tensor(self.num_clusters, device=self.device, dtype=torch.float32)\n","        n = S.shape[0]\n","\n","        modularity_term = (-1 / (2 * m)) * torch.trace(torch.mm(torch.mm(C.t(), B), C))\n","        collapse_reg_term = (torch.sqrt(k) / n) * torch.norm(torch.sum(C, dim=0), p='fro') - 1\n","\n","        return modularity_term + collapse_reg_term\n","\n","    def cut_loss(self, A, S):\n","        S = nnFn.softmax(S, dim=1)\n","        A_pool = torch.matmul(torch.matmul(A, S).t(), S)\n","        num = torch.trace(A_pool)\n","\n","        D = torch.diag(torch.sum(A, dim=-1))\n","        D_pooled = torch.matmul(torch.matmul(D, S).t(), S)\n","        den = torch.trace(D_pooled)\n","        mincut_loss = -(num / den)\n","\n","        St_S = torch.matmul(S.t(), S)\n","        I_S = torch.eye(self.num_clusters, device=self.device)\n","        ortho_loss = torch.norm(St_S / torch.norm(St_S) - I_S / torch.norm(I_S))\n","\n","        return mincut_loss + ortho_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JsS6_ScmXAv4"},"outputs":[],"source":["def create_adj(features, cut, alpha=1.0):\n","    \"\"\"Return a dense W0 matrix (only once), as you originally used for A1 / unsup loss.\n","       We still create the dense matrix once, but all augmentations below work with edge_index.\n","    \"\"\"\n","    F_norm = features / np.linalg.norm(features, axis=1, keepdims=True)\n","    W = np.dot(F_norm, F_norm.T)\n","\n","    if cut == 0:\n","        W = np.where(W >= alpha, 1, 0).astype(np.float32)\n","        W = (W / W.max()).astype(np.float32)\n","    else:\n","        W = (W * (W >= alpha)).astype(np.float32)\n","    return W"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4tweH4RNur0e"},"outputs":[],"source":["def edge_index_from_dense(W):\n","    \"\"\"Return edge_index as numpy array shape (2, E) and edge_weight vector.\"\"\"\n","    rows, cols = np.nonzero(W > 0)\n","    edge_index = np.vstack([rows, cols]).astype(np.int64)\n","    edge_weight = W[rows, cols].astype(np.float32)\n","    return edge_index, edge_weight"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1nP8oSs6ur5N"},"outputs":[],"source":["def build_adj_list(edge_index_np, num_nodes):\n","    \"\"\"Build adjacency list: list of neighbor arrays for each node (numpy).\"\"\"\n","    adj = [[] for _ in range(num_nodes)]\n","    src = edge_index_np[0]\n","    dst = edge_index_np[1]\n","    for s, d in zip(src, dst):\n","        adj[s].append(d)\n","    # convert to numpy arrays for speed\n","    adj = [np.array(neis, dtype=np.int64) if len(neis) > 0 else np.array([], dtype=np.int64) for neis in adj]\n","    return adj"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kVvIBHQsur9M"},"outputs":[],"source":["def aug_random_edge_edge_index(edge_index_np, drop_percent=0.2, seed=None):\n","    \"\"\"Randomly drop edges from edge_index. Returns new edge_index (2 x E') and edge_weight placeholder.\"\"\"\n","    rng = np.random.default_rng(seed)\n","    E = edge_index_np.shape[1]\n","    keep_mask = rng.random(E) >= drop_percent\n","    new_edge_index = edge_index_np[:, keep_mask]\n","    return new_edge_index"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kcGNq86RusAt"},"outputs":[],"source":["def aug_subgraph_edge_index(features_np, edge_index_np, adj_list, drop_percent=0.2, seed=None):\n","    \"\"\"\n","    Sample a subgraph by selecting s_node_num nodes via neighbor expansion (BFS-like),\n","    then return (sub_features, sub_edge_index) with node ids remapped to [0..s-1].\n","    \"\"\"\n","    rng = np.random.default_rng(seed)\n","    num_nodes = features_np.shape[0]\n","    s_node_num = int(num_nodes * (1 - drop_percent))\n","    if s_node_num < 1:\n","        s_node_num = 1\n","\n","    # choose a random center node\n","    center_node = int(rng.integers(0, num_nodes))\n","    sub_nodes = [center_node]\n","    front_idx = 0\n","\n","    # BFS-like expansion using adjacency list until we reach s_node_num\n","    while len(sub_nodes) < s_node_num and front_idx < len(sub_nodes):\n","        cur = sub_nodes[front_idx]\n","        neighbors = adj_list[cur]\n","        if neighbors.size > 0:\n","            # shuffle neighbors and try to add new ones\n","            nbrs_shuffled = neighbors.copy()\n","            rng.shuffle(nbrs_shuffled)\n","            for nb in nbrs_shuffled:\n","                if nb not in sub_nodes:\n","                    sub_nodes.append(int(nb))\n","                    if len(sub_nodes) >= s_node_num:\n","                        break\n","        front_idx += 1\n","        # if BFS stalls (no new neighbors), add random nodes\n","        if front_idx >= len(sub_nodes) and len(sub_nodes) < s_node_num:\n","            remaining = [n for n in range(num_nodes) if n not in sub_nodes]\n","            if not remaining:\n","                break\n","            add = int(rng.choice(remaining))\n","            sub_nodes.append(add)\n","\n","    sub_nodes = sorted(set(sub_nodes))\n","    node_map = {old: new for new, old in enumerate(sub_nodes)}\n","\n","    # induce edges that have both ends in sub_nodes\n","    src = edge_index_np[0]\n","    dst = edge_index_np[1]\n","    mask_src_in = np.isin(src, sub_nodes)\n","    mask_dst_in = np.isin(dst, sub_nodes)\n","    mask = mask_src_in & mask_dst_in\n","    sel_src = src[mask]\n","    sel_dst = dst[mask]\n","    # remap\n","    remapped_src = np.array([node_map[int(s)] for s in sel_src], dtype=np.int64)\n","    remapped_dst = np.array([node_map[int(d)] for d in sel_dst], dtype=np.int64)\n","    new_edge_index = np.vstack([remapped_src, remapped_dst])\n","    # sub features\n","    sub_features = features_np[sub_nodes, :].astype(np.float32)\n","    return sub_features, new_edge_index"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yr0l6W5WusGV"},"outputs":[],"source":["def load_data_from_edge_index(node_feats_np, edge_index_np, device):\n","    \"\"\"Return PyG Data with torch tensors. edge_index_np is (2, E) numpy.\"\"\"\n","    node_feats = torch.from_numpy(node_feats_np).float().to(device)\n","    edge_index = torch.from_numpy(edge_index_np.astype(np.int64)).long().to(device)\n","    return node_feats, edge_index"]},{"cell_type":"markdown","metadata":{"id":"MGmFFoeAQ4A_"},"source":["# Data Loading and preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C6ixOkliQ6y2","outputId":"f1347569-b54e-4842-f7dc-201b283ba158","executionInfo":{"status":"ok","timestamp":1766240993274,"user_tz":-330,"elapsed":6,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["(146, 180) float32\n"]}],"source":["features = X\n","#features = np.concatenate((Histogram_feature_CN_FA_array, Histogram_feature_MCI_FA_array), axis=0)\n","features = features.astype(np.float32)\n","print(features.shape, features.dtype)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DdEsfaCeRKdm"},"outputs":[],"source":["# Required Parameters\n","cut = 1  # Consider n-cut loss OR Modularity loss\n","alpha = 0.5 #0.7 # Edge creation Threshold\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","feats_dim = 180  # 20-bin\n","K = 2  # Number of clusters\n","epoch = [2500, 60, 100]  # Training epochs for different phases\n","\n","# Define all activation functions to test\n","define_activations = [\"SELU\", \"SiLU\", \"GELU\", \"ELU\", \"RELU\"]\n","activ = \"ELU\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YOS22Jo7spbj","outputId":"d5116f9d-ff5c-47f6-8c2a-3dfb578935ae","executionInfo":{"status":"ok","timestamp":1766240998660,"user_tz":-330,"elapsed":5,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["(146, 180)\n"]}],"source":["print(features.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tORSJ9G5saNu","outputId":"56d9e1e1-b353-4541-95a3-a367a121a85c","executionInfo":{"status":"ok","timestamp":1766241000024,"user_tz":-330,"elapsed":6,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["(2, 20520)\n"]}],"source":["F_norm = features / np.linalg.norm(features, axis=1, keepdims=True)\n","W = np.dot(F_norm, F_norm.T)\n","print(np.array(np.nonzero(W>0.6)).shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CTwFu2gkYqt3","outputId":"4ff6ab7f-32a1-460c-ec87-bcff0273edf2","executionInfo":{"status":"ok","timestamp":1766241000639,"user_tz":-330,"elapsed":16,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.99999994 0.8497284  0.90970075 ... 0.8536243  0.85516846 0.87134004]\n"," [0.8497284  0.9999999  0.89257383 ... 0.8011868  0.8645267  0.86090064]\n"," [0.90970075 0.89257383 1.0000001  ... 0.8730685  0.8966978  0.92472154]\n"," ...\n"," [0.8536243  0.8011868  0.8730685  ... 1.0000001  0.8663177  0.903542  ]\n"," [0.85516846 0.8645267  0.8966978  ... 0.8663177  1.         0.9069954 ]\n"," [0.87134004 0.86090064 0.92472154 ... 0.903542   0.9069954  1.0000004 ]]\n"]}],"source":["F_norm = features / np.linalg.norm(features, axis=1, keepdims=True)\n","W = np.dot(F_norm, F_norm.T)\n","print(W)"]},{"cell_type":"markdown","metadata":{"id":"G9kDUzF9i3Mg"},"source":["# Fully connected directed graph\n","# ONLY single directed edge between any two nodes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NS56NCJGib7Y"},"outputs":[],"source":["# num_nodes = F.shape[0]\n","# edge_index = torch.combinations(torch.arange(num_nodes), r=2).T\n","# def create_adjacency_matrix(edge_index, num_nodes):\n","#     adj = torch.zeros((num_nodes, num_nodes))\n","#     adj[edge_index[0], edge_index[1]] = 1\n","#     return adj\n","\n","# W = create_adjacency_matrix(edge_index, num_nodes).numpy()\n","# print(\"W Shape= \" , W.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ya4xuSn3i1BH","outputId":"467fedaf-570e-4b17-d6bc-007533462dd0","executionInfo":{"status":"ok","timestamp":1766244922030,"user_tz":-330,"elapsed":15,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Data0: Data(x=[146, 180], edge_index=[2, 21256])\n"]}],"source":["W0 = create_adj(features, cut, alpha)  # shape (N, N) dense\n","A1 = torch.from_numpy(W0).float().to(device)\n","\n","edge_index_np, edge_weight_np = edge_index_from_dense(W0)  # numpy edge_index (2, E)\n","num_nodes = features.shape[0]\n","adj_list = build_adj_list(edge_index_np, num_nodes)  # adjacency list for fast subgraph sampling\n","\n","# convert features to numpy (we'll slice them in augmentations)\n","features_np = features.copy()\n","\n","# Build initial Data object (full graph)\n","node_feats_full, edge_index_full = load_data_from_edge_index(features_np, edge_index_np, device)\n","data0 = Data(x=node_feats_full.to(device), edge_index=edge_index_full.to(device))\n","print(\"Data0:\", data0)"]},{"cell_type":"markdown","metadata":{"id":"PuJLS4CgR8-u"},"source":["# Model initialization"]},{"cell_type":"markdown","metadata":{"id":"QZov9rTSOzyV"},"source":["## Contrastive Loss"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XW57ZES1L1UW","outputId":"809ddad6-75a5-4217-d256-b645397656d8","executionInfo":{"status":"ok","timestamp":1766241043178,"user_tz":-330,"elapsed":36738,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0 | Total: 1.2387 | Unsup: -0.2348 | Cont: 4.9118\n","Epoch 100 | Total: 0.8268 | Unsup: -0.4453 | Cont: 4.2403\n","Epoch 200 | Total: 0.7430 | Unsup: -0.4553 | Cont: 3.9941\n","Epoch 300 | Total: 0.6849 | Unsup: -0.4725 | Cont: 3.8578\n","Epoch 400 | Total: 0.6799 | Unsup: -0.4666 | Cont: 3.8215\n","Epoch 500 | Total: 0.6440 | Unsup: -0.4849 | Cont: 3.7629\n","Epoch 600 | Total: 0.6520 | Unsup: -0.4686 | Cont: 3.7353\n","Epoch 700 | Total: 0.6380 | Unsup: -0.4806 | Cont: 3.7286\n","Epoch 800 | Total: 0.6446 | Unsup: -0.4731 | Cont: 3.7255\n","Epoch 900 | Total: 0.6219 | Unsup: -0.4811 | Cont: 3.6767\n","Epoch 1000 | Total: 0.6398 | Unsup: -0.4667 | Cont: 3.6887\n","Epoch 1100 | Total: 0.6486 | Unsup: -0.4752 | Cont: 3.7460\n","Epoch 1200 | Total: 0.6380 | Unsup: -0.4707 | Cont: 3.6958\n","Epoch 1300 | Total: 0.6169 | Unsup: -0.4806 | Cont: 3.6582\n","Epoch 1400 | Total: 0.6291 | Unsup: -0.4856 | Cont: 3.7155\n","Epoch 1500 | Total: 0.6285 | Unsup: -0.4752 | Cont: 3.6790\n","Epoch 1600 | Total: 0.6225 | Unsup: -0.4766 | Cont: 3.6634\n","Epoch 1700 | Total: 0.6337 | Unsup: -0.4818 | Cont: 3.7183\n","Epoch 1800 | Total: 0.6123 | Unsup: -0.4821 | Cont: 3.6481\n","Epoch 1900 | Total: 0.6323 | Unsup: -0.4768 | Cont: 3.6972\n","Epoch 2000 | Total: 0.6426 | Unsup: -0.4758 | Cont: 3.7281\n","Epoch 2100 | Total: 0.6124 | Unsup: -0.4881 | Cont: 3.6682\n","Epoch 2200 | Total: 0.6448 | Unsup: -0.4531 | Cont: 3.6596\n","Epoch 2300 | Total: 0.6087 | Unsup: -0.4881 | Cont: 3.6562\n","Epoch 2400 | Total: 0.6319 | Unsup: -0.4781 | Cont: 3.6999\n","Epoch 2500 | Total: 0.6388 | Unsup: -0.4612 | Cont: 3.6666\n","Epoch 2600 | Total: 0.6222 | Unsup: -0.4677 | Cont: 3.6329\n","Epoch 2700 | Total: 0.6082 | Unsup: -0.4738 | Cont: 3.6065\n","Epoch 2800 | Total: 0.6116 | Unsup: -0.4820 | Cont: 3.6451\n","Epoch 2900 | Total: 0.6272 | Unsup: -0.4691 | Cont: 3.6542\n","Epoch 3000 | Total: 0.5982 | Unsup: -0.4851 | Cont: 3.6111\n","Epoch 3100 | Total: 0.6698 | Unsup: -0.4439 | Cont: 3.7123\n","Epoch 3200 | Total: 0.6129 | Unsup: -0.4829 | Cont: 3.6527\n","Epoch 3300 | Total: 0.6481 | Unsup: -0.4625 | Cont: 3.7022\n","Epoch 3400 | Total: 0.6374 | Unsup: -0.4633 | Cont: 3.6689\n","Epoch 3500 | Total: 0.6355 | Unsup: -0.4684 | Cont: 3.6798\n","Epoch 3600 | Total: 0.6167 | Unsup: -0.4786 | Cont: 3.6507\n","Epoch 3700 | Total: 0.6131 | Unsup: -0.4724 | Cont: 3.6182\n","Epoch 3800 | Total: 0.6194 | Unsup: -0.4755 | Cont: 3.6497\n","Epoch 3900 | Total: 0.6471 | Unsup: -0.4804 | Cont: 3.7583\n","Epoch 4000 | Total: 0.6322 | Unsup: -0.4742 | Cont: 3.6879\n","Epoch 4100 | Total: 0.6081 | Unsup: -0.4782 | Cont: 3.6208\n","Epoch 4200 | Total: 0.6386 | Unsup: -0.4713 | Cont: 3.6998\n","Epoch 4300 | Total: 0.6383 | Unsup: -0.4617 | Cont: 3.6668\n","Epoch 4400 | Total: 0.6206 | Unsup: -0.4836 | Cont: 3.6808\n","Epoch 4500 | Total: 0.6168 | Unsup: -0.4743 | Cont: 3.6370\n","Epoch 4600 | Total: 0.6213 | Unsup: -0.4686 | Cont: 3.6328\n","Epoch 4700 | Total: 0.6184 | Unsup: -0.4824 | Cont: 3.6692\n","Epoch 4800 | Total: 0.6072 | Unsup: -0.4814 | Cont: 3.6288\n","Epoch 4900 | Total: 0.6161 | Unsup: -0.4746 | Cont: 3.6355\n"]}],"source":["from torch.optim.lr_scheduler import StepLR\n","from torch.optim import AdamW\n","import torch.nn as nn\n","\n","model = ARMA(feats_dim, 256, K, device, activ, cut=cut).to(device)\n","optimizer = AdamW(model.parameters(), lr=0.0001, weight_decay=0.0001)\n","scheduler = StepLR(optimizer, step_size=200, gamma=0.5)\n","criterion = nn.CrossEntropyLoss()\n","\n","num_epochs = 5000\n","lambda_contrastive = 0.3\n","np.random.seed(42)\n","random.seed(42)\n","torch.manual_seed(42)\n","for epoch in range(num_epochs):\n","    # --- Augmentations using edge_index or adjacency list (fast, sparse) ---\n","    # 1) Random edge drop on edge_index\n","    W_aug1_edge_index = aug_random_edge_edge_index(edge_index_np, drop_percent=0.2, seed=epoch)\n","\n","    # 2) Subgraph via adjacency list (returns sub_features and sub_edge_index)\n","    W_aug2_edge_index = aug_random_edge_edge_index(edge_index_np, drop_percent=0.2, seed=epoch + 999)\n","    features_aug2 = features_np.copy()\n","\n","    # 3) Feature augmentations (keep these as numpy operations)\n","    # Feature dropout (column-wise)\n","    rng = np.random.default_rng(epoch)\n","    mask = rng.random(features_np.shape) >= 0.2\n","    features_aug1 = (features_np * mask.astype(np.float32))\n","\n","    # Feature cell dropout (random cell zeroing)\n","    aug_feat2 = features_np.copy()\n","    num_nodes_local, feat_dim = aug_feat2.shape\n","    drop_feat_num = int(num_nodes_local * feat_dim * 0.2)\n","    # random positions to zero\n","    flat_idx = rng.choice(num_nodes_local * feat_dim, size=drop_feat_num, replace=False)\n","    rows = (flat_idx // feat_dim)\n","    cols = (flat_idx % feat_dim)\n","    aug_feat2[rows, cols] = 0.0\n","    features_aug2_feat = aug_feat2.astype(np.float32)\n","\n","    # --- Build PyG Data objects for the two views ---\n","    # view1: features_aug1 with W_aug1_edge_index\n","    node_feats1, edge_index1 = load_data_from_edge_index(features_aug1, W_aug1_edge_index, device)\n","    data1 = Data(x=node_feats1.to(device), edge_index=edge_index1.to(device))\n","\n","    # view2: features_aug2 (from subgraph) and its edge_index\n","    node_feats2, edge_index2 = load_data_from_edge_index(features_aug2, W_aug2_edge_index, device)\n","    data2 = Data(x=node_feats2.to(device), edge_index=edge_index2.to(device))\n","\n","    # --- Training step ---\n","    model.train()\n","    optimizer.zero_grad()\n","\n","    S1, S2, logits1, logits2, l1, l2 = model(data1, data2)\n","\n","    unsup_loss = model.loss(A1, logits1)\n","    cont_loss = ((l1 + l2) / 2).mean()\n","    total_loss = unsup_loss + lambda_contrastive * cont_loss\n","\n","    total_loss.backward()\n","    optimizer.step()\n","    scheduler.step()\n","    model.update_ma()\n","\n","    if epoch % 100 == 0:\n","        print(f\"Epoch {epoch} | Total: {total_loss.item():.4f} | Unsup: {unsup_loss.item():.4f} | Cont: {cont_loss.item():.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6ty1p11xXxaM","outputId":"584d54f2-34c5-4f2a-84ad-6b9258f28f14","executionInfo":{"status":"ok","timestamp":1766241043255,"user_tz":-330,"elapsed":47,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[0 0 0 0 0 0 1 1 1 0 0 0 0 1 1 1 1 1 0 1 0 0 0 1 0 1 1 1 1 0 0 0 1 1 0 0 1\n"," 1 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 1 1 0 0 1 1 0 1 0 1 1 0 0 1 1 0 1 0 1 1\n"," 0 0 1 1 0 1 0 1 0 0 1 1 1 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 1 0 1 0\n"," 1 0 0 0 1 1 1 0 1 0 1 1 0 1 0 0 0 1 1 0 1 1 0 1 1 0 1 1 0 1 1 1 0 1 0]\n","[0.9944542  0.99999356 0.9967524  0.9999403  0.99999344 0.9997589\n"," 0.6199211  1.         0.9999963  0.9999882  0.9386562  0.9929906\n"," 0.8662325  0.99999845 0.95755357 0.9804386  0.9798623  0.99998224\n"," 1.         0.9999999  0.9999914  0.96117043 0.9999913  0.99683654\n"," 0.9991399  0.9994276  1.         0.9999962  0.97822374 0.99960524\n"," 1.         0.995613   0.7224679  0.9998153  1.         0.9999962\n"," 0.74800366 1.         1.         1.         0.9820809  0.99894375\n"," 0.996505   1.         0.99999595 0.99997854 1.         0.99281114\n"," 0.9999995  0.999984   0.9999999  1.         0.9998834  0.9987552\n"," 0.99528056 0.9919016  0.9982274  0.9999989  0.9613275  1.\n"," 0.999204   0.9999931  0.72389364 0.9247491  0.99299675 0.9999436\n"," 0.99949527 0.9999989  0.9582058  0.6887555  1.         0.9999987\n"," 0.9992865  0.9999732  0.99884546 0.9823085  0.9985512  0.9999994\n"," 0.99840516 1.         0.6615955  0.9999057  0.9999999  0.82263255\n"," 0.9997589  0.9999999  0.99999464 0.9999219  0.9999945  0.60450125\n"," 0.99999964 0.9999758  0.9999876  0.9999993  0.9998479  0.99934477\n"," 0.94586915 1.         0.8705807  0.9999993  0.9432501  0.9999901\n"," 0.9999995  0.981928   0.85973924 0.9999981  0.9992428  0.99982685\n"," 0.9919756  0.9999989  0.9888489  1.         0.99998975 0.99999535\n"," 0.9999969  0.5124252  0.99932396 0.9825911  0.9978058  0.67513347\n"," 0.9983694  0.99997616 0.9665866  0.99999964 0.9997706  0.97202986\n"," 0.9616113  0.99988973 0.99876815 0.9999807  0.99997616 0.9999999\n"," 0.9713904  1.         0.95169294 0.99999774 0.9997075  0.9999951\n"," 0.999828   0.97871006 0.9999999  0.8705602  0.7686259  0.9994836\n"," 0.53598624 0.9972663 ]\n"]}],"source":["model.eval()\n","with torch.no_grad():\n","        S1, _, logits1,_,_,_ = model(data0, data0)\n","        y_pred = torch.argmax(logits1, dim=1).cpu().numpy()\n","        y_pred_proba = nnFn.softmax(logits1, dim=1).cpu().numpy()\n","        print(y_pred)\n","        print(y_pred_proba.max(axis=-1))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WoiV1peZa1kb","outputId":"b6d7b21b-476b-4157-ed24-bd76d9dc9eeb","executionInfo":{"status":"ok","timestamp":1766241043260,"user_tz":-330,"elapsed":4,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy Score: 0.678082191780822\n","Accuracy Score Inverted: 0.3219178082191781\n","Precision Score: 0.84\n","Recall Score: 0.6428571428571429\n","F1 Score: 0.7283236994219653\n","Log Loss: 2.2774661976189066\n"]}],"source":["acc_score = accuracy_score(y, y_pred)\n","acc_score_inverted = accuracy_score(y, 1 - y_pred)\n","\n","print(\"Accuracy Score:\", acc_score)\n","print(\"Accuracy Score Inverted:\", acc_score_inverted)\n","\n","if acc_score_inverted > acc_score:\n","    acc_score = acc_score_inverted\n","    y_pred = 1 - y_pred\n","\n","prec_score = precision_score(y, y_pred)\n","rec_score = recall_score(y, y_pred)\n","f1 = f1_score(y, y_pred)\n","log_loss_value = log_loss(y, y_pred_proba)\n","\n","print(\"Precision Score:\", prec_score)\n","print(\"Recall Score:\", rec_score)\n","print(\"F1 Score:\", f1)\n","print(\"Log Loss:\", log_loss_value)"]},{"cell_type":"code","source":["import numpy as np\n","import random\n","import torch\n","import torch.nn as nn\n","from torch.optim import AdamW\n","from torch.optim.lr_scheduler import StepLR\n","from sklearn.metrics import (\n","    accuracy_score, precision_score,\n","    recall_score, f1_score, log_loss\n",")\n","\n","NUM_RUNS = 10\n","num_epochs = 5000\n","lambda_contrastive = 0.01\n","\n","acc_list, prec_list, rec_list, f1_list, logloss_list = [], [], [], [], []\n","\n","for run in range(NUM_RUNS):\n","    print(f\"\\n===== Run {run + 1}/{NUM_RUNS} =====\")\n","\n","    # --------------------\n","    # Set seeds per run\n","    # --------------------\n","    seed = 42 + run\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","\n","    # --------------------\n","    # Model / optimizer\n","    # --------------------\n","    model = ARMA(feats_dim, 256, K, device, activ, cut=cut).to(device)\n","    optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n","    scheduler = StepLR(optimizer, step_size=200, gamma=0.5)\n","    criterion = nn.CrossEntropyLoss()\n","\n","    # --------------------\n","    # Training loop\n","    # --------------------\n","    for epoch in range(num_epochs):\n","        # --- Edge augmentations ---\n","        W_aug1_edge_index = aug_random_edge_edge_index(\n","            edge_index_np, drop_percent=0.2, seed=epoch\n","        )\n","        W_aug2_edge_index = aug_random_edge_edge_index(\n","            edge_index_np, drop_percent=0.2, seed=epoch + 999\n","        )\n","\n","        # --- Feature augmentations ---\n","        rng = np.random.default_rng(epoch)\n","\n","        mask = rng.random(features_np.shape) >= 0.2\n","        features_aug1 = features_np * mask.astype(np.float32)\n","\n","        aug_feat2 = features_np.copy()\n","        num_nodes_local, feat_dim = aug_feat2.shape\n","        drop_feat_num = int(num_nodes_local * feat_dim * 0.2)\n","        flat_idx = rng.choice(num_nodes_local * feat_dim, size=drop_feat_num, replace=False)\n","        rows = flat_idx // feat_dim\n","        cols = flat_idx % feat_dim\n","        aug_feat2[rows, cols] = 0.0\n","        features_aug2 = aug_feat2.astype(np.float32)\n","\n","        # --- PyG data ---\n","        node_feats1, edge_index1 = load_data_from_edge_index(\n","            features_aug1, W_aug1_edge_index, device\n","        )\n","        node_feats2, edge_index2 = load_data_from_edge_index(\n","            features_aug2, W_aug2_edge_index, device\n","        )\n","\n","        data1 = Data(x=node_feats1, edge_index=edge_index1)\n","        data2 = Data(x=node_feats2, edge_index=edge_index2)\n","\n","        # --- Optimization ---\n","        model.train()\n","        optimizer.zero_grad()\n","\n","        S1, S2, logits1, logits2, l1, l2 = model(data1, data2)\n","        unsup_loss = model.loss(A1, logits1)\n","        cont_loss = ((l1 + l2) / 2).mean()\n","        total_loss = unsup_loss + lambda_contrastive * cont_loss\n","\n","        total_loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","        model.update_ma()\n","\n","    # --------------------\n","    # Evaluation\n","    # --------------------\n","    model.eval()\n","    with torch.no_grad():\n","        _, _, logits, _, _, _ = model(data0, data0)\n","\n","        y_pred = torch.argmax(logits, dim=1).cpu().numpy()\n","        y_pred_proba = nnFn.softmax(logits, dim=1).cpu().numpy()\n","\n","    acc = accuracy_score(y, y_pred)\n","    acc_inv = accuracy_score(y, 1 - y_pred)\n","\n","    if acc_inv > acc:\n","        y_pred = 1 - y_pred\n","        acc = acc_inv\n","\n","    prec = precision_score(y, y_pred)\n","    rec = recall_score(y, y_pred)\n","    f1 = f1_score(y, y_pred)\n","    ll = log_loss(y, y_pred_proba)\n","\n","    acc_list.append(acc)\n","    prec_list.append(prec)\n","    rec_list.append(rec)\n","    f1_list.append(f1)\n","    logloss_list.append(ll)\n","\n","# --------------------\n","# Final statistics\n","# --------------------\n","def mean_std(x):\n","    return np.mean(x), np.std(x)\n","\n","print(\"\\n===== Final Results (Mean ± Std over 10 runs) =====\")\n","print(f\"Accuracy : {mean_std(acc_list)[0]:.4f} ± {mean_std(acc_list)[1]:.4f}\")\n","print(f\"Precision: {mean_std(prec_list)[0]:.4f} ± {mean_std(prec_list)[1]:.4f}\")\n","print(f\"Recall   : {mean_std(rec_list)[0]:.4f} ± {mean_std(rec_list)[1]:.4f}\")\n","print(f\"F1 Score : {mean_std(f1_list)[0]:.4f} ± {mean_std(f1_list)[1]:.4f}\")\n","print(f\"Log Loss : {mean_std(logloss_list)[0]:.4f} ± {mean_std(logloss_list)[1]:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FKho3ZqdLIYQ","executionInfo":{"status":"ok","timestamp":1766247113357,"user_tz":-330,"elapsed":460658,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}},"outputId":"9979746c-56ec-46ce-d03a-31710044d0b7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","===== Run 1/10 =====\n","\n","===== Run 2/10 =====\n","\n","===== Run 3/10 =====\n","\n","===== Run 4/10 =====\n","\n","===== Run 5/10 =====\n","\n","===== Run 6/10 =====\n","\n","===== Run 7/10 =====\n","\n","===== Run 8/10 =====\n","\n","===== Run 9/10 =====\n","\n","===== Run 10/10 =====\n","\n","===== Final Results (Mean ± Std over 10 runs) =====\n","Accuracy : 0.5644 ± 0.0561\n","Precision: 0.7396 ± 0.0494\n","Recall   : 0.5398 ± 0.0539\n","F1 Score : 0.6238 ± 0.0529\n","Log Loss : 5.0330 ± 1.0738\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as nnFn\n","from torch.optim.lr_scheduler import StepLR\n","from torch.optim import AdamW\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss\n","from torch_geometric.data import Data\n","import random\n","import scipy.sparse as sp\n","\n","# -------------------- Hyperparameters --------------------\n","num_runs = 10\n","num_epochs = 5000\n","lr = 1e-4\n","weight_decay = 1e-4\n","lambda_list = [0.001, 0.009, 0.1, 0.5, 1, 8]\n","base_seed = 42\n","\n","# -------------------- Initialize results storage --------------------\n","all_results = []\n","\n","# -------------------- Loop over different lambda values --------------------\n","for lam in lambda_list:\n","    print(f\"\\n================ LAMBDA = {lam} ================\\n\")\n","\n","    acc_scores = []\n","    prec_scores = []\n","    rec_scores = []\n","    f1_scores = []\n","    log_losses = []\n","\n","    for run in range(num_runs):\n","        print(f\"\\n--- Run {run+1}/{num_runs} ---\")\n","        torch.manual_seed(run)\n","        np.random.seed(run)\n","        random.seed(run)\n","\n","        # -------------------- Initialize model and optimizer --------------------\n","        model = ARMA(feats_dim, 256, K, device, activ, cut=cut).to(device)\n","        optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n","        scheduler = StepLR(optimizer, step_size=200, gamma=0.5)\n","\n","        # -------------------- Training loop --------------------\n","        for epoch in range(num_epochs):\n","\n","            # --- Augmentation for adjacency ---\n","            # Using aug_random_edge_edge_index which operates on edge_index_np\n","            W_aug1_edge_index = aug_random_edge_edge_index(edge_index_np, drop_percent=0.2, seed=epoch)\n","            # Using aug_subgraph_edge_index which returns (sub_features, sub_edge_index)\n","            features_subgraph_aug2, W_aug2_edge_index = aug_subgraph_edge_index(features_np, edge_index_np, adj_list, drop_percent=0.2, seed=epoch + 999)\n","\n","            # --- Augmentation for features ---\n","            features_aug1 = aug_feature_dropout(features_np, drop_percent=0.2, seed=epoch)\n","            features_aug2 = aug_feature_dropout_cell(features_np.copy(), drop_percent=0.2, seed=epoch + 1999)\n","\n","            # --- Load data for PyG ---\n","            node_feats1, edge_index1 = load_data_from_edge_index(features_aug1, W_aug1_edge_index, device)\n","            node_feats2, edge_index2 = load_data_from_edge_index(features_subgraph_aug2, W_aug2_edge_index, device)\n","\n","            # Ensure data is moved to device\n","            data1 = Data(x=node_feats1.to(device), edge_index=edge_index1.to(device))\n","            data2 = Data(x=node_feats2.to(device), edge_index=edge_index2.to(device))\n","\n","            # --- Training step ---\n","            model.train()\n","            optimizer.zero_grad()\n","\n","            S1, S2, logits1, logits2, l1, l2 = model(data1, data2)\n","\n","            unsup_loss = model.loss(A1, logits1)\n","            cont_loss = ((l1 + l2) / 2).mean()\n","            total_loss = unsup_loss + lam * cont_loss\n","\n","            total_loss.backward()\n","            optimizer.step()\n","            scheduler.step()\n","            model.update_ma()\n","\n","            if epoch % 500 == 0:\n","                print(f\"Epoch {epoch} | Total: {total_loss:.4f} | Unsup: {unsup_loss:.4f} | Cont: {cont_loss:.4f}\")\n","\n","        # -------------------- Evaluation --------------------\n","        model.eval()\n","        with torch.no_grad():\n","            # Ensure data0 is on the correct device for evaluation\n","            data0_on_device = data0.to(device)\n","            _, _, logits, _, _, _ = model(data0_on_device, data0_on_device)\n","\n","            y_pred = torch.argmax(logits, dim=1).cpu().numpy()\n","            y_pred_proba = nnFn.softmax(logits, dim=1).cpu().numpy()\n","\n","            acc_score = accuracy_score(y, y_pred)\n","            acc_score_inverted = accuracy_score(y, 1 - y_pred)\n","            if acc_score_inverted > acc_score:\n","                acc_score = acc_score_inverted\n","                y_pred = 1 - y_pred\n","\n","            print(f\"Run {run+1} Accuracy: {acc_score:.4f}\")\n","\n","            prec_score = precision_score(y, y_pred)\n","            rec_score = recall_score(y, y_pred)\n","            f1 = f1_score(y, y_pred)\n","            log_loss_value = log_loss(y, y_pred_proba)\n","\n","            acc_scores.append(acc_score)\n","            prec_scores.append(prec_score)\n","            rec_scores.append(rec_score)\n","            f1_scores.append(f1)\n","            log_losses.append(log_loss_value)\n","\n","    # -------------------- Store results for this lambda --------------------\n","    lambda_results = {\n","        \"lambda\": lam,\n","        \"accuracy\": (np.mean(acc_scores), np.std(acc_scores)),\n","        \"precision\": (np.mean(prec_scores), np.std(prec_scores)),\n","        \"recall\": (np.mean(rec_scores), np.std(rec_scores)),\n","        \"f1\": (np.mean(f1_scores), np.std(f1_scores)),\n","        \"log_loss\": (np.mean(log_losses), np.std(log_losses))\n","    }\n","    all_results.append(lambda_results)\n","\n","    print(f\"\\n--- RESULTS FOR LAMBDA = {lam} ---\")\n","    print(f\"Accuracy: {lambda_results['accuracy'][0]:.4f} \\u00B1 {lambda_results['accuracy'][1]:.4f}\")\n","    print(f\"Precision: {lambda_results['precision'][0]:.4f} \\u00B1 {lambda_results['precision'][1]:.4f}\")\n","    print(f\"Recall: {lambda_results['recall'][0]:.4f} \\u00B1 {lambda_results['recall'][1]:.4f}\")\n","    print(f\"F1 Score: {lambda_results['f1'][0]:.4f} \\u00B1 {lambda_results['f1'][1]:.4f}\")\n","    print(f\"Log Loss: {lambda_results['log_loss'][0]:.4f} \\u00B1 {lambda_results['log_loss'][1]:.4f}\")\n","\n","# -------------------- Final Summary --------------------\n","print(\"\\n================ FINAL SUMMARY FOR ALL LAMBDAS ================\\n\")\n","print(f\"{'Lambda':>8} | {'Accuracy':>18} | {'Precision':>18} | {'Recall':>18} | {'F1 Score':>18} | {'Log Loss':>18}\")\n","print(\"-\" * 108)\n","for res in all_results:\n","    print(f\"{res['lambda']:>8} | \"\n","          f\"{res['accuracy'][0]:.4f} \\u00B1 {res['accuracy'][1]:.4f} | \"\n","          f\"{res['precision'][0]:.4f} \\u00B1 {res['precision'][1]:.4f} | \"\n","          f\"{res['recall'][0]:.4f} \\u00B1 {res['recall'][1]:.4f} | \"\n","          f\"{res['f1'][0]:.4f} \\u00B1 {res['f1'][1]:.4f} | \"\n","          f\"{res['log_loss'][0]:.4f} \\u00B1 {res['log_loss'][1]:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":332},"id":"XMA49074ZUSG","executionInfo":{"status":"error","timestamp":1763981669096,"user_tz":-330,"elapsed":104,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}},"outputId":"c3b2183b-65b4-4a90-ee7e-bc1c91365a72"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","================ LAMBDA = 0.001 ================\n","\n","\n","--- Run 1/10 ---\n"]},{"output_type":"error","ename":"NameError","evalue":"name 'aug_random_edge' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[57], line 48\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# -------------------- Training loop --------------------\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     46\u001b[0m \n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# --- Augmentation for adjacency ---\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m     W_aug1 \u001b[38;5;241m=\u001b[39m \u001b[43maug_random_edge\u001b[49m(W0, drop_percent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, seed\u001b[38;5;241m=\u001b[39mepoch)  \u001b[38;5;66;03m# drop edges\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     W_aug2 \u001b[38;5;241m=\u001b[39m aug_subgraph(features, sp\u001b[38;5;241m.\u001b[39mcsr_matrix(W0), drop_percent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# subgraph\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# --- Augmentation for features ---\u001b[39;00m\n","\u001b[0;31mNameError\u001b[0m: name 'aug_random_edge' is not defined"]}]},{"cell_type":"code","metadata":{"id":"29387623"},"source":["import numpy as np\n","import torch\n","\n","def aug_feature_dropout(features_np, drop_percent=0.2, seed=None):\n","    rng = np.random.default_rng(seed)\n","    mask = rng.random(features_np.shape) >= drop_percent\n","    return (features_np * mask).astype(np.float32)\n","\n","def aug_feature_dropout_cell(features_np, drop_percent=0.2, seed=None):\n","    rng = np.random.default_rng(seed)\n","    aug_feat = features_np.copy()\n","    num_nodes_local, feat_dim = aug_feat.shape\n","    drop_feat_num = int(num_nodes_local * feat_dim * drop_percent)\n","    flat_idx = rng.choice(num_nodes_local * feat_dim, size=drop_feat_num, replace=False)\n","    rows = (flat_idx // feat_dim)\n","    cols = (flat_idx % feat_dim)\n","    aug_feat[rows, cols] = 0.0\n","    return aug_feat.astype(np.float32)"],"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.18"}},"nbformat":4,"nbformat_minor":0}
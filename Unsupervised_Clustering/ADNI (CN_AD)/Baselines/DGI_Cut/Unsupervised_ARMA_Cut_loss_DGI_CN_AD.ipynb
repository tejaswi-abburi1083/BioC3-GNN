{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1XLM31otwAb-lsrSJl8kC0VbSO-PNRcUK","timestamp":1748402375877},{"file_id":"18xxNJDLcJndTxx-laCpvq_2AdGc4rK2D","timestamp":1748063610059}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T5aSgvBjOxj8","executionInfo":{"status":"ok","timestamp":1758188767775,"user_tz":-330,"elapsed":27664,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}},"outputId":"18dbff11-d2ff-4d2d-8556-e6eaa3ec33d1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import os\n","import torch\n","os.environ['TORCH'] = torch.__version__\n","print(torch.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RCsIte6uO5cI","executionInfo":{"status":"ok","timestamp":1755781341224,"user_tz":-330,"elapsed":4443,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}},"outputId":"de6bbb94-37ca-4c44-ebbf-01a76c936af3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2.8.0+cu126\n"]}]},{"cell_type":"code","source":["!pip install sympy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O98Id85qI5tW","executionInfo":{"status":"ok","timestamp":1755781350160,"user_tz":-330,"elapsed":8938,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}},"outputId":"f9760880-34df-411d-86d3-11cf186d17db"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (1.13.3)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy) (1.3.0)\n"]}]},{"cell_type":"code","source":["!pip install -q torch_geometric\n","!pip install -q class_resolver\n","!pip3 install pymatting"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x8mZcDISO8cx","executionInfo":{"status":"ok","timestamp":1758188708342,"user_tz":-330,"elapsed":17195,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}},"outputId":"049c1c36-4ec1-4464-83fa-f4572c83d93a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pymatting\n","  Downloading pymatting-1.1.14-py3-none-any.whl.metadata (7.7 kB)\n","Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from pymatting) (2.0.2)\n","Requirement already satisfied: pillow>=5.2.0 in /usr/local/lib/python3.12/dist-packages (from pymatting) (11.3.0)\n","Requirement already satisfied: numba!=0.49.0 in /usr/local/lib/python3.12/dist-packages (from pymatting) (0.60.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from pymatting) (1.16.1)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba!=0.49.0->pymatting) (0.43.0)\n","Downloading pymatting-1.1.14-py3-none-any.whl (54 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.7/54.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pymatting\n","Successfully installed pymatting-1.1.14\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch_geometric.data import Data\n","from torch_geometric.nn import ARMAConv\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss"],"metadata":{"id":"YXFd7Zsz-3XQ","executionInfo":{"status":"ok","timestamp":1763980000892,"user_tz":-330,"elapsed":3,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["fa_feature_path = \"/home/snu/Downloads/Histogram_CN_FA_20bin_updated.npy\"\n","Histogram_feature_CN_FA_array = np.load(fa_feature_path, allow_pickle=True)\n","\n","# Load AD features\n","fa_feature_path = \"/home/snu/Downloads/Histogram_AD_FA_20bin_updated.npy\"\n","Histogram_feature_AD_FA_array = np.load(fa_feature_path, allow_pickle=True)\n","\n","# Combine features and labels\n","X = np.vstack([Histogram_feature_CN_FA_array, Histogram_feature_AD_FA_array])\n","y = np.hstack([\n","    np.zeros(Histogram_feature_CN_FA_array.shape[0], dtype=np.int64),\n","    np.ones(Histogram_feature_AD_FA_array.shape[0], dtype=np.int64)\n","])\n","np.random.seed(42)\n","perm = np.random.permutation(X.shape[0])\n","X = X[perm]\n","y = y[perm]\n","num_nodes, num_feats = X.shape\n","print(f\"Features: {X.shape}, Labels: {y.shape}\")"],"metadata":{"id":"JQ6KiAvW-5rn","executionInfo":{"status":"ok","timestamp":1763980001638,"user_tz":-330,"elapsed":11,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d5fe8825-50cd-4f41-cc20-879f0cfe9ab3"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Features: (223, 180), Labels: (223,)\n"]}]},{"cell_type":"code","source":["def create_adj(F, alpha=1):\n","    F_norm = F / np.linalg.norm(F, axis=1, keepdims=True)\n","    W = np.dot(F_norm, F_norm.T)\n","    W = (W >= alpha).astype(np.float32)\n","    return W"],"metadata":{"id":"MkKdnnzI-7zI","executionInfo":{"status":"ok","timestamp":1763980003012,"user_tz":-330,"elapsed":3,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def load_data(adj, node_feats):\n","    node_feats = torch.from_numpy(node_feats).float()\n","    edge_index = torch.from_numpy(np.array(np.nonzero(adj))).long()\n","    return node_feats, edge_index"],"metadata":{"id":"zQSZYHWPwTG2","executionInfo":{"status":"ok","timestamp":1763980004170,"user_tz":-330,"elapsed":2,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["features = X.astype(np.float32)\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","W0 = create_adj(features, alpha=0.8)\n","# W_asym = asymmetrize_random(W0, seed=42)\n","node_feats, edge_index = load_data(W0, features)\n","data = Data(x=node_feats, edge_index=edge_index).to(device)\n","A = torch.from_numpy(W0).to(device)\n","print(data)"],"metadata":{"id":"Zz160NfE--Tg","executionInfo":{"status":"ok","timestamp":1763980005429,"user_tz":-330,"elapsed":128,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c0fa3e54-2d0f-4772-dd1a-e50402ad5dfb"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Data(x=[223, 180], edge_index=[2, 41689])\n"]}]},{"cell_type":"code","source":["import torch.nn.functional as nnFn\n","\n","class ARMAEncoder(torch.nn.Module):\n","    def __init__(self, input_dim, hidden_dim, device, activ=\"ELU\", num_stacks=1, num_layers=1):\n","        super(ARMAEncoder, self).__init__()\n","        self.device = device\n","        # Define all activation functions\n","        activations = {\n","            \"SELU\": nnFn.selu,\n","            \"SiLU\": nnFn.silu,\n","            \"GELU\": nnFn.gelu,\n","            \"ELU\": nnFn.elu,\n","            \"RELU\": nnFn.relu\n","        }\n","        # Get the activation function based on the input string\n","        self.act = activations.get(activ, nnFn.elu)\n","\n","        self.arma = ARMAConv(\n","            in_channels=input_dim,\n","            out_channels=hidden_dim,\n","            num_stacks=num_stacks,   # number of parallel stacks\n","            num_layers=num_layers,   # depth per stack\n","            act=self.act,               # nonlinearity inside ARMA\n","            shared_weights=True,     # weight sharing across layers\n","            dropout=0.25             # ARMA-internal dropout\n","        )\n","        self.batchnorm = nn.BatchNorm1d(hidden_dim)\n","        self.dropout = nn.Dropout(0.3)\n","        self.mlp = nn.Linear(hidden_dim, hidden_dim)\n","\n","    def forward(self, data):\n","        x, edge_index = data.x, data.edge_index\n","        x = self.arma(x, edge_index)\n","        x = self.dropout(x)\n","        x = self.batchnorm(x)\n","        logits = self.mlp(x)\n","        return logits"],"metadata":{"id":"nhh413xH_Awp","executionInfo":{"status":"ok","timestamp":1763980006546,"user_tz":-330,"elapsed":3,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["class AvgReadout(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, seq, msk=None):\n","        if msk is None:\n","            return torch.mean(seq, 0)\n","        else:\n","            msk = torch.unsqueeze(msk, -1)\n","            return torch.sum(seq * msk, 0) / torch.sum(msk)"],"metadata":{"id":"t1CWUjjG48t_","executionInfo":{"status":"ok","timestamp":1763980007703,"user_tz":-330,"elapsed":7,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["class Discriminator(nn.Module):\n","    def __init__(self, n_h):\n","        super().__init__()\n","        self.f_k = nn.Bilinear(n_h, n_h, 1)\n","        nn.init.xavier_uniform_(self.f_k.weight.data)\n","        if self.f_k.bias is not None:\n","            self.f_k.bias.data.fill_(0.0)\n","\n","    def forward(self, c, h_pl, h_mi):\n","        c_x = torch.unsqueeze(c, 0).expand_as(h_pl)\n","        sc_1 = torch.squeeze(self.f_k(h_pl, c_x), 1)\n","        sc_2 = torch.squeeze(self.f_k(h_mi, c_x), 1)\n","        logits = torch.cat((sc_1, sc_2), 0)\n","        return logits"],"metadata":{"id":"IbNOUPjg42s9","executionInfo":{"status":"ok","timestamp":1763980008577,"user_tz":-330,"elapsed":2,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["class DGI(nn.Module):\n","    def __init__(self, n_in, n_h, dropout=0.25):\n","        super().__init__()\n","        self.arma1 = ARMAEncoder(n_in, n_h, device='cuda' if torch.cuda.is_available() else 'cpu', activ=nn.ELU())\n","        self.read = AvgReadout()\n","        self.sigm = nn.Sigmoid()\n","        self.disc = Discriminator(n_h)\n","\n","    def forward(self, seq1, seq2, edge_index):\n","        # Create Data objects for the ARMAEncoder\n","        data1 = Data(x=seq1, edge_index=edge_index)\n","        data2 = Data(x=seq2, edge_index=edge_index)\n","\n","        h_1 = self.arma1(data1)\n","        c = self.read(h_1)\n","        c = self.sigm(c)\n","        h_2 = self.arma1(data2)\n","        logits = self.disc(c, h_1, h_2)\n","        return logits, h_1"],"metadata":{"id":"7X7cwk2N8Al2","executionInfo":{"status":"ok","timestamp":1763980009987,"user_tz":-330,"elapsed":2,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["class DGI_with_classifier(DGI):\n","    def __init__(self, n_in, n_h, n_classes=2, cut=0, dropout=0.25):\n","        super().__init__(n_in, n_h, dropout=dropout)\n","        self.classifier = nn.Linear(n_h, n_classes)\n","        self.cut = cut\n","\n","    def get_embeddings(self, node_feats, edge_index):\n","        _, embeddings = self.forward(node_feats, node_feats, edge_index)\n","        return embeddings\n","\n","    def cut_loss(self, A, S):\n","        S = F.softmax(S, dim=1)\n","        A_pool = torch.matmul(torch.matmul(A, S).t(), S)\n","        num = torch.trace(A_pool)\n","        D = torch.diag(torch.sum(A, dim=-1))\n","        D_pooled = torch.matmul(torch.matmul(D, S).t(), S)\n","        den = torch.trace(D_pooled)\n","        mincut_loss = -(num / den)\n","        St_S = torch.matmul(S.t(), S)\n","        I_S = torch.eye(S.shape[1], device=A.device)\n","        ortho_loss = torch.norm(St_S / torch.norm(St_S) - I_S / torch.norm(I_S))\n","        return mincut_loss + ortho_loss\n","\n","    def modularity_loss(self, A, S):\n","        C = F.softmax(S, dim=1)\n","        d = torch.sum(A, dim=1)\n","        m = torch.sum(A)\n","        B = A - torch.ger(d, d) / (2 * m)\n","        I_S = torch.eye(C.shape[1], device=A.device)\n","        k = torch.norm(I_S)\n","        n = S.shape[0]\n","        modularity_term = (-1 / (2 * m)) * torch.trace(torch.mm(torch.mm(C.t(), B), C))\n","        collapse_reg_term = (torch.sqrt(k) / n) * torch.norm(torch.sum(C, dim=0), p='fro') - 1\n","        return modularity_term + collapse_reg_term\n","\n","    def Reg_loss(self, A, embeddings):\n","        logits = self.classifier(embeddings)\n","        if self.cut == 1:\n","            return self.cut_loss(A, logits)\n","        else:\n","            return self.modularity_loss(A, logits)"],"metadata":{"id":"yVuaMXM1C9T3","executionInfo":{"status":"ok","timestamp":1763980011444,"user_tz":-330,"elapsed":3,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["hidden_dim = 256\n","cut = 1\n","dropout = 0.25\n","model = DGI_with_classifier(features.shape[1], hidden_dim, n_classes=2, cut=cut, dropout=dropout).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay = 0.00001)\n","bce_loss = nn.BCEWithLogitsLoss()\n","\n","num_epochs = 10000\n","for epoch in range(num_epochs + 1):\n","    model.train()\n","    optimizer.zero_grad()\n","\n","    perm = torch.randperm(node_feats.size(0))\n","    corrupt_features = node_feats[perm]\n","\n","    logits, embeddings = model(node_feats.to(device), corrupt_features.to(device), edge_index.to(device))\n","\n","    lbl = torch.cat([\n","        torch.ones(node_feats.size(0)),\n","        torch.zeros(node_feats.size(0))\n","    ]).to(device)\n","\n","    dgi_loss = bce_loss(logits.squeeze(), lbl)\n","    reg_loss = model.Reg_loss(A, embeddings)\n","    loss = dgi_loss + 2 * reg_loss\n","\n","    if epoch % 500 == 0:\n","        print(f\"Epoch {epoch} | DGI Loss: {dgi_loss.item():.4f} | Reg Loss: {reg_loss.item():.4f} | Total: {loss.item():.4f}\")\n","\n","    loss.backward()\n","    optimizer.step()"],"metadata":{"id":"4ClSGTZaA_xt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763980188438,"user_tz":-330,"elapsed":169600,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}},"outputId":"8584b184-edb5-43a3-8161-a9d35058c58d"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0 | DGI Loss: 0.7101 | Reg Loss: -0.2362 | Total: 0.2377\n","Epoch 500 | DGI Loss: 0.6972 | Reg Loss: -0.5376 | Total: -0.3779\n","Epoch 1000 | DGI Loss: 0.6971 | Reg Loss: -0.5381 | Total: -0.3790\n","Epoch 1500 | DGI Loss: 0.6979 | Reg Loss: -0.5359 | Total: -0.3739\n","Epoch 2000 | DGI Loss: 0.6955 | Reg Loss: -0.5316 | Total: -0.3678\n","Epoch 2500 | DGI Loss: 0.6931 | Reg Loss: -0.5316 | Total: -0.3701\n","Epoch 3000 | DGI Loss: 0.6938 | Reg Loss: -0.5296 | Total: -0.3655\n","Epoch 3500 | DGI Loss: 0.6987 | Reg Loss: -0.5424 | Total: -0.3861\n","Epoch 4000 | DGI Loss: 0.6944 | Reg Loss: -0.5385 | Total: -0.3826\n","Epoch 4500 | DGI Loss: 0.6932 | Reg Loss: -0.5272 | Total: -0.3613\n","Epoch 5000 | DGI Loss: 0.7786 | Reg Loss: -0.5381 | Total: -0.2976\n","Epoch 5500 | DGI Loss: 0.6932 | Reg Loss: -0.5442 | Total: -0.3952\n","Epoch 6000 | DGI Loss: 0.7060 | Reg Loss: -0.5397 | Total: -0.3735\n","Epoch 6500 | DGI Loss: 0.6931 | Reg Loss: -0.5407 | Total: -0.3882\n","Epoch 7000 | DGI Loss: 0.6931 | Reg Loss: -0.5440 | Total: -0.3948\n","Epoch 7500 | DGI Loss: 0.6937 | Reg Loss: -0.5397 | Total: -0.3858\n","Epoch 8000 | DGI Loss: 0.6937 | Reg Loss: -0.5117 | Total: -0.3296\n","Epoch 8500 | DGI Loss: 0.6931 | Reg Loss: -0.5396 | Total: -0.3860\n","Epoch 9000 | DGI Loss: 0.6936 | Reg Loss: -0.5242 | Total: -0.3548\n","Epoch 9500 | DGI Loss: 0.6978 | Reg Loss: -0.5457 | Total: -0.3937\n","Epoch 10000 | DGI Loss: 0.6932 | Reg Loss: -0.5401 | Total: -0.3869\n"]}]},{"cell_type":"code","source":["model.eval()\n","with torch.no_grad():\n","    embeddings = model.get_embeddings(node_feats.to(device), edge_index.to(device))\n","    class_probabilities = F.softmax(model.classifier(embeddings), dim=1).cpu().numpy()\n","\n","y_pred = np.argmax(class_probabilities, axis=1)"],"metadata":{"id":"giN_kiZckBqV","executionInfo":{"status":"ok","timestamp":1763980188442,"user_tz":-330,"elapsed":1,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["acc_score = accuracy_score(y, y_pred)\n","acc_score_inverted = accuracy_score(y, 1 - y_pred)\n","prec_score = precision_score(y, y_pred)\n","rec_score = recall_score(y, y_pred)\n","f1 = f1_score(y, y_pred)\n","log_loss_value = log_loss(y, class_probabilities)\n","\n","print(\"Accuracy:\", acc_score)\n","print(\"Accuracy (inverted):\", acc_score_inverted)\n","print(\"Precision:\", prec_score)\n","print(\"Recall:\", rec_score)\n","print(\"F1:\", f1)\n","print(\"Log Loss:\", log_loss_value)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eVJYp9cnauDO","executionInfo":{"status":"ok","timestamp":1763980188510,"user_tz":-330,"elapsed":67,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}},"outputId":"1d2a0186-0e77-42d2-f081-e6b51b3de6b8"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.23318385650224216\n","Accuracy (inverted): 0.7668161434977578\n","Precision: 0.14782608695652175\n","Recall: 0.18888888888888888\n","F1: 0.16585365853658537\n","Log Loss: 11.352154625556347\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.optim import AdamW\n","from torch.optim.lr_scheduler import StepLR\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss\n","\n","hidden_dim   = 256\n","cut          = 1\n","dropout      = 0.25\n","num_runs     = 10\n","num_epochs   = 10000\n","lambda_list  = [0.09]\n","#lambda_list  = [2]\n","base_seed    = 42\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","node_feats = node_feats.to(device)\n","edge_index = edge_index.to(device)\n","A = A.to(device)\n","\n","if isinstance(y, torch.Tensor):\n","    y_np = y.detach().cpu().numpy().astype(int)\n","else:\n","    y_np = np.asarray(y).astype(int)\n","\n","N, feats_dim = node_feats.size(0), node_feats.size(1)\n","\n","all_results = []\n","bce_loss = nn.BCEWithLogitsLoss()\n","\n","for lam in lambda_list:\n","    print(f\"\\n================ LAMBDA = {lam} ================\\n\")\n","\n","    acc_scores, prec_scores, rec_scores, f1_scores, log_losses = [], [], [], [], []\n","\n","    for run in range(num_runs):\n","        print(f\"\\n--- Run {run+1}/{num_runs} ---\")\n","\n","        seed = base_seed + run\n","        torch.manual_seed(seed)\n","        np.random.seed(seed)\n","        if torch.cuda.is_available():\n","            torch.cuda.manual_seed_all(seed)\n","\n","\n","        model = DGI_with_classifier(feats_dim, hidden_dim, n_classes=2, cut=cut, dropout=dropout).to(device)\n","        optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.00001)\n","        scheduler = StepLR(optimizer, step_size=200, gamma=0.5)\n","\n","\n","        for epoch in range(num_epochs + 1):\n","            model.train()\n","            optimizer.zero_grad()\n","\n","            perm = torch.randperm(N, device=device)\n","            corrupt_features = node_feats[perm]\n","\n","            logits, embeddings = model(node_feats, corrupt_features, edge_index)\n","\n","            lbl = torch.cat([torch.ones(N, device=device), torch.zeros(N, device=device)])\n","            dgi_loss = bce_loss(logits.squeeze(), lbl)\n","            reg_loss = model.Reg_loss(A, embeddings)\n","\n","            loss = dgi_loss + lam * reg_loss\n","\n","            if epoch % 500 == 0:\n","                print(f\"Epoch {epoch:4d} | DGI: {dgi_loss.item():.4f} | Reg: {reg_loss.item():.4f} | \"\n","                      f\"λ*Reg: {(lam * reg_loss).item():.4f} | Total: {loss.item():.4f}\")\n","\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step()\n","\n","        model.eval()\n","        with torch.no_grad():\n","            emb = model.get_embeddings(node_feats, edge_index)\n","            logits_cls = model.classifier(emb)                   # [N, 2]\n","            class_probabilities = F.softmax(logits_cls, dim=1).cpu().numpy()\n","            y_pred = np.argmax(class_probabilities, axis=1)\n","\n","        acc  = accuracy_score(y_np, y_pred)\n","        acc_inv = accuracy_score(y_np, 1 - y_pred)\n","\n","        if acc_inv > acc:\n","            acc = acc_inv\n","            y_pred = 1 - y_pred\n","            class_probabilities = class_probabilities[:, ::-1]\n","\n","        prec = precision_score(y_np, y_pred, zero_division=0)\n","        rec  = recall_score(y_np, y_pred, zero_division=0)\n","        f1   = f1_score(y_np, y_pred, zero_division=0)\n","        ll   = log_loss(y_np, class_probabilities)\n","\n","        print(f\"Run {run+1} | Accuracy: {acc:.4f} | Precision: {prec:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | LogLoss: {ll:.4f}\")\n","\n","        acc_scores.append(acc)\n","        prec_scores.append(prec)\n","        rec_scores.append(rec)\n","        f1_scores.append(f1)\n","        log_losses.append(ll)\n","\n","    lambda_results = {\n","        \"lambda\": lam,\n","        \"accuracy\":  (float(np.mean(acc_scores)), float(np.std(acc_scores))),\n","        \"precision\": (float(np.mean(prec_scores)), float(np.std(prec_scores))),\n","        \"recall\":    (float(np.mean(rec_scores)), float(np.std(rec_scores))),\n","        \"f1\":        (float(np.mean(f1_scores)),  float(np.std(f1_scores))),\n","        \"log_loss\":  (float(np.mean(log_losses)), float(np.std(log_losses))),\n","    }\n","    all_results.append(lambda_results)\n","\n","    print(f\"\\n--- RESULTS FOR LAMBDA = {lam} ---\")\n","    print(f\"Accuracy : {lambda_results['accuracy'][0]:.4f} ± {lambda_results['accuracy'][1]:.4f}\")\n","    print(f\"Precision: {lambda_results['precision'][0]:.4f} ± {lambda_results['precision'][1]:.4f}\")\n","    print(f\"Recall   : {lambda_results['recall'][0]:.4f} ± {lambda_results['recall'][1]:.4f}\")\n","    print(f\"F1 Score : {lambda_results['f1'][0]:.4f} ± {lambda_results['f1'][1]:.4f}\")\n","    print(f\"Log Loss : {lambda_results['log_loss'][0]:.4f} ± {lambda_results['log_loss'][1]:.4f}\")\n","\n","print(\"\\n================ FINAL SUMMARY FOR ALL LAMBDAS ================\\n\")\n","print(f\"{'Lambda':>8} | {'Accuracy':>18} | {'Precision':>18} | {'Recall':>18} | {'F1 Score':>18} | {'Log Loss':>18}\")\n","print(\"-\" * 108)\n","for res in all_results:\n","    print(f\"{res['lambda']:>8} | \"\n","          f\"{res['accuracy'][0]:.4f} ± {res['accuracy'][1]:.4f} | \"\n","          f\"{res['precision'][0]:.4f} ± {res['precision'][1]:.4f} | \"\n","          f\"{res['recall'][0]:.4f} ± {res['recall'][1]:.4f} | \"\n","          f\"{res['f1'][0]:.4f} ± {res['f1'][1]:.4f} | \"\n","          f\"{res['log_loss'][0]:.4f} ± {res['log_loss'][1]:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VG5T4ClGjB8x","executionInfo":{"status":"ok","timestamp":1763981532020,"user_tz":-330,"elapsed":1343463,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}},"outputId":"41b36fe9-c9a8-4893-e472-aec7661cc9e9"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","================ LAMBDA = 0.09 ================\n","\n","\n","--- Run 1/10 ---\n","Epoch    0 | DGI: 0.7195 | Reg: -0.2362 | λ*Reg: -0.0213 | Total: 0.6983\n","Epoch  500 | DGI: 0.6931 | Reg: -0.5338 | λ*Reg: -0.0480 | Total: 0.6451\n","Epoch 1000 | DGI: 0.6931 | Reg: -0.5359 | λ*Reg: -0.0482 | Total: 0.6449\n","Epoch 1500 | DGI: 0.6931 | Reg: -0.5393 | λ*Reg: -0.0485 | Total: 0.6446\n","Epoch 2000 | DGI: 0.6931 | Reg: -0.5360 | λ*Reg: -0.0482 | Total: 0.6449\n","Epoch 2500 | DGI: 0.6931 | Reg: -0.5214 | λ*Reg: -0.0469 | Total: 0.6462\n","Epoch 3000 | DGI: 0.6931 | Reg: -0.5363 | λ*Reg: -0.0483 | Total: 0.6449\n","Epoch 3500 | DGI: 0.6931 | Reg: -0.5365 | λ*Reg: -0.0483 | Total: 0.6449\n","Epoch 4000 | DGI: 0.6931 | Reg: -0.5292 | λ*Reg: -0.0476 | Total: 0.6455\n","Epoch 4500 | DGI: 0.6931 | Reg: -0.5288 | λ*Reg: -0.0476 | Total: 0.6456\n","Epoch 5000 | DGI: 0.6931 | Reg: -0.5328 | λ*Reg: -0.0480 | Total: 0.6452\n","Epoch 5500 | DGI: 0.6931 | Reg: -0.5191 | λ*Reg: -0.0467 | Total: 0.6464\n","Epoch 6000 | DGI: 0.6931 | Reg: -0.5341 | λ*Reg: -0.0481 | Total: 0.6451\n","Epoch 6500 | DGI: 0.6931 | Reg: -0.5348 | λ*Reg: -0.0481 | Total: 0.6450\n","Epoch 7000 | DGI: 0.6931 | Reg: -0.5300 | λ*Reg: -0.0477 | Total: 0.6454\n","Epoch 7500 | DGI: 0.6931 | Reg: -0.5075 | λ*Reg: -0.0457 | Total: 0.6475\n","Epoch 8000 | DGI: 0.6931 | Reg: -0.5241 | λ*Reg: -0.0472 | Total: 0.6460\n","Epoch 8500 | DGI: 0.6931 | Reg: -0.5382 | λ*Reg: -0.0484 | Total: 0.6447\n","Epoch 9000 | DGI: 0.6931 | Reg: -0.5414 | λ*Reg: -0.0487 | Total: 0.6444\n","Epoch 9500 | DGI: 0.6931 | Reg: -0.5368 | λ*Reg: -0.0483 | Total: 0.6448\n","Epoch 10000 | DGI: 0.6931 | Reg: -0.5350 | λ*Reg: -0.0481 | Total: 0.6450\n","Run 1 | Accuracy: 0.7534 | Precision: 0.6606 | Recall: 0.8000 | F1: 0.7236 | LogLoss: 2.5271\n","\n","--- Run 2/10 ---\n","Epoch    0 | DGI: 0.7147 | Reg: -0.2351 | λ*Reg: -0.0212 | Total: 0.6935\n","Epoch  500 | DGI: 0.6931 | Reg: -0.5204 | λ*Reg: -0.0468 | Total: 0.6463\n","Epoch 1000 | DGI: 0.6931 | Reg: -0.5420 | λ*Reg: -0.0488 | Total: 0.6444\n","Epoch 1500 | DGI: 0.6931 | Reg: -0.5348 | λ*Reg: -0.0481 | Total: 0.6450\n","Epoch 2000 | DGI: 0.6931 | Reg: -0.5206 | λ*Reg: -0.0469 | Total: 0.6463\n","Epoch 2500 | DGI: 0.6931 | Reg: -0.5330 | λ*Reg: -0.0480 | Total: 0.6452\n","Epoch 3000 | DGI: 0.6931 | Reg: -0.5285 | λ*Reg: -0.0476 | Total: 0.6456\n","Epoch 3500 | DGI: 0.6931 | Reg: -0.5350 | λ*Reg: -0.0482 | Total: 0.6450\n","Epoch 4000 | DGI: 0.6931 | Reg: -0.5375 | λ*Reg: -0.0484 | Total: 0.6448\n","Epoch 4500 | DGI: 0.6931 | Reg: -0.5396 | λ*Reg: -0.0486 | Total: 0.6446\n","Epoch 5000 | DGI: 0.6931 | Reg: -0.5368 | λ*Reg: -0.0483 | Total: 0.6448\n","Epoch 5500 | DGI: 0.6931 | Reg: -0.5246 | λ*Reg: -0.0472 | Total: 0.6459\n","Epoch 6000 | DGI: 0.6931 | Reg: -0.5303 | λ*Reg: -0.0477 | Total: 0.6454\n","Epoch 6500 | DGI: 0.6931 | Reg: -0.5290 | λ*Reg: -0.0476 | Total: 0.6455\n","Epoch 7000 | DGI: 0.6931 | Reg: -0.5368 | λ*Reg: -0.0483 | Total: 0.6448\n","Epoch 7500 | DGI: 0.6931 | Reg: -0.5348 | λ*Reg: -0.0481 | Total: 0.6450\n","Epoch 8000 | DGI: 0.6931 | Reg: -0.5344 | λ*Reg: -0.0481 | Total: 0.6451\n","Epoch 8500 | DGI: 0.6931 | Reg: -0.5249 | λ*Reg: -0.0472 | Total: 0.6459\n","Epoch 9000 | DGI: 0.6931 | Reg: -0.5278 | λ*Reg: -0.0475 | Total: 0.6456\n","Epoch 9500 | DGI: 0.6931 | Reg: -0.5359 | λ*Reg: -0.0482 | Total: 0.6449\n","Epoch 10000 | DGI: 0.6931 | Reg: -0.5292 | λ*Reg: -0.0476 | Total: 0.6455\n","Run 2 | Accuracy: 0.7713 | Precision: 0.6822 | Recall: 0.8111 | F1: 0.7411 | LogLoss: 2.5551\n","\n","--- Run 3/10 ---\n","Epoch    0 | DGI: 0.7167 | Reg: -0.2370 | λ*Reg: -0.0213 | Total: 0.6954\n","Epoch  500 | DGI: 0.6931 | Reg: -0.5214 | λ*Reg: -0.0469 | Total: 0.6462\n","Epoch 1000 | DGI: 0.6931 | Reg: -0.5385 | λ*Reg: -0.0485 | Total: 0.6447\n","Epoch 1500 | DGI: 0.6931 | Reg: -0.5392 | λ*Reg: -0.0485 | Total: 0.6446\n","Epoch 2000 | DGI: 0.6931 | Reg: -0.5364 | λ*Reg: -0.0483 | Total: 0.6449\n","Epoch 2500 | DGI: 0.6931 | Reg: -0.5354 | λ*Reg: -0.0482 | Total: 0.6450\n","Epoch 3000 | DGI: 0.6931 | Reg: -0.5367 | λ*Reg: -0.0483 | Total: 0.6448\n","Epoch 3500 | DGI: 0.6931 | Reg: -0.5367 | λ*Reg: -0.0483 | Total: 0.6448\n","Epoch 4000 | DGI: 0.6931 | Reg: -0.5394 | λ*Reg: -0.0485 | Total: 0.6446\n","Epoch 4500 | DGI: 0.6931 | Reg: -0.5314 | λ*Reg: -0.0478 | Total: 0.6453\n","Epoch 5000 | DGI: 0.6931 | Reg: -0.5360 | λ*Reg: -0.0482 | Total: 0.6449\n","Epoch 5500 | DGI: 0.6931 | Reg: -0.5380 | λ*Reg: -0.0484 | Total: 0.6447\n","Epoch 6000 | DGI: 0.6931 | Reg: -0.5259 | λ*Reg: -0.0473 | Total: 0.6458\n","Epoch 6500 | DGI: 0.6931 | Reg: -0.5350 | λ*Reg: -0.0481 | Total: 0.6450\n","Epoch 7000 | DGI: 0.6931 | Reg: -0.5064 | λ*Reg: -0.0456 | Total: 0.6476\n","Epoch 7500 | DGI: 0.6931 | Reg: -0.5281 | λ*Reg: -0.0475 | Total: 0.6456\n","Epoch 8000 | DGI: 0.6931 | Reg: -0.5286 | λ*Reg: -0.0476 | Total: 0.6456\n","Epoch 8500 | DGI: 0.6931 | Reg: -0.5399 | λ*Reg: -0.0486 | Total: 0.6446\n","Epoch 9000 | DGI: 0.6931 | Reg: -0.5389 | λ*Reg: -0.0485 | Total: 0.6446\n","Epoch 9500 | DGI: 0.6931 | Reg: -0.5318 | λ*Reg: -0.0479 | Total: 0.6453\n","Epoch 10000 | DGI: 0.6931 | Reg: -0.5330 | λ*Reg: -0.0480 | Total: 0.6452\n","Run 3 | Accuracy: 0.7489 | Precision: 0.6545 | Recall: 0.8000 | F1: 0.7200 | LogLoss: 2.6886\n","\n","--- Run 4/10 ---\n","Epoch    0 | DGI: 0.7148 | Reg: -0.2363 | λ*Reg: -0.0213 | Total: 0.6935\n","Epoch  500 | DGI: 0.6931 | Reg: -0.5339 | λ*Reg: -0.0480 | Total: 0.6451\n","Epoch 1000 | DGI: 0.6931 | Reg: -0.5250 | λ*Reg: -0.0472 | Total: 0.6459\n","Epoch 1500 | DGI: 0.6931 | Reg: -0.5378 | λ*Reg: -0.0484 | Total: 0.6447\n","Epoch 2000 | DGI: 0.6931 | Reg: -0.5184 | λ*Reg: -0.0467 | Total: 0.6465\n","Epoch 2500 | DGI: 0.6931 | Reg: -0.5286 | λ*Reg: -0.0476 | Total: 0.6456\n","Epoch 3000 | DGI: 0.6931 | Reg: -0.5370 | λ*Reg: -0.0483 | Total: 0.6448\n","Epoch 3500 | DGI: 0.6931 | Reg: -0.5380 | λ*Reg: -0.0484 | Total: 0.6447\n","Epoch 4000 | DGI: 0.6931 | Reg: -0.5397 | λ*Reg: -0.0486 | Total: 0.6446\n","Epoch 4500 | DGI: 0.6931 | Reg: -0.5271 | λ*Reg: -0.0474 | Total: 0.6457\n","Epoch 5000 | DGI: 0.6931 | Reg: -0.5409 | λ*Reg: -0.0487 | Total: 0.6445\n","Epoch 5500 | DGI: 0.6931 | Reg: -0.5336 | λ*Reg: -0.0480 | Total: 0.6451\n","Epoch 6000 | DGI: 0.6931 | Reg: -0.5339 | λ*Reg: -0.0481 | Total: 0.6451\n","Epoch 6500 | DGI: 0.6931 | Reg: -0.5375 | λ*Reg: -0.0484 | Total: 0.6448\n","Epoch 7000 | DGI: 0.6931 | Reg: -0.5373 | λ*Reg: -0.0484 | Total: 0.6448\n","Epoch 7500 | DGI: 0.6931 | Reg: -0.5392 | λ*Reg: -0.0485 | Total: 0.6446\n","Epoch 8000 | DGI: 0.6931 | Reg: -0.5310 | λ*Reg: -0.0478 | Total: 0.6454\n","Epoch 8500 | DGI: 0.6931 | Reg: -0.5250 | λ*Reg: -0.0472 | Total: 0.6459\n","Epoch 9000 | DGI: 0.6931 | Reg: -0.5399 | λ*Reg: -0.0486 | Total: 0.6446\n","Epoch 9500 | DGI: 0.6931 | Reg: -0.5322 | λ*Reg: -0.0479 | Total: 0.6452\n","Epoch 10000 | DGI: 0.6931 | Reg: -0.5426 | λ*Reg: -0.0488 | Total: 0.6443\n","Run 4 | Accuracy: 0.7578 | Precision: 0.6636 | Recall: 0.8111 | F1: 0.7300 | LogLoss: 2.6223\n","\n","--- Run 5/10 ---\n","Epoch    0 | DGI: 0.7086 | Reg: -0.2388 | λ*Reg: -0.0215 | Total: 0.6871\n","Epoch  500 | DGI: 0.6931 | Reg: -0.5197 | λ*Reg: -0.0468 | Total: 0.6464\n","Epoch 1000 | DGI: 0.6931 | Reg: -0.5277 | λ*Reg: -0.0475 | Total: 0.6457\n","Epoch 1500 | DGI: 0.6931 | Reg: -0.5297 | λ*Reg: -0.0477 | Total: 0.6455\n","Epoch 2000 | DGI: 0.6931 | Reg: -0.5218 | λ*Reg: -0.0470 | Total: 0.6462\n","Epoch 2500 | DGI: 0.6931 | Reg: -0.5261 | λ*Reg: -0.0474 | Total: 0.6458\n","Epoch 3000 | DGI: 0.6931 | Reg: -0.5289 | λ*Reg: -0.0476 | Total: 0.6455\n","Epoch 3500 | DGI: 0.6931 | Reg: -0.5265 | λ*Reg: -0.0474 | Total: 0.6458\n","Epoch 4000 | DGI: 0.6931 | Reg: -0.5273 | λ*Reg: -0.0475 | Total: 0.6457\n","Epoch 4500 | DGI: 0.6931 | Reg: -0.5179 | λ*Reg: -0.0466 | Total: 0.6465\n","Epoch 5000 | DGI: 0.6931 | Reg: -0.5350 | λ*Reg: -0.0481 | Total: 0.6450\n","Epoch 5500 | DGI: 0.6931 | Reg: -0.5352 | λ*Reg: -0.0482 | Total: 0.6450\n","Epoch 6000 | DGI: 0.6931 | Reg: -0.5238 | λ*Reg: -0.0471 | Total: 0.6460\n","Epoch 6500 | DGI: 0.6931 | Reg: -0.5324 | λ*Reg: -0.0479 | Total: 0.6452\n","Epoch 7000 | DGI: 0.6931 | Reg: -0.5297 | λ*Reg: -0.0477 | Total: 0.6455\n","Epoch 7500 | DGI: 0.6931 | Reg: -0.5230 | λ*Reg: -0.0471 | Total: 0.6461\n","Epoch 8000 | DGI: 0.6931 | Reg: -0.5358 | λ*Reg: -0.0482 | Total: 0.6449\n","Epoch 8500 | DGI: 0.6931 | Reg: -0.5231 | λ*Reg: -0.0471 | Total: 0.6461\n","Epoch 9000 | DGI: 0.6931 | Reg: -0.5288 | λ*Reg: -0.0476 | Total: 0.6456\n","Epoch 9500 | DGI: 0.6931 | Reg: -0.5317 | λ*Reg: -0.0479 | Total: 0.6453\n","Epoch 10000 | DGI: 0.6931 | Reg: -0.5172 | λ*Reg: -0.0465 | Total: 0.6466\n","Run 5 | Accuracy: 0.7265 | Precision: 0.6306 | Recall: 0.7778 | F1: 0.6965 | LogLoss: 2.9918\n","\n","--- Run 6/10 ---\n","Epoch    0 | DGI: 0.7161 | Reg: -0.2349 | λ*Reg: -0.0211 | Total: 0.6949\n","Epoch  500 | DGI: 0.6931 | Reg: -0.5329 | λ*Reg: -0.0480 | Total: 0.6452\n","Epoch 1000 | DGI: 0.6931 | Reg: -0.5232 | λ*Reg: -0.0471 | Total: 0.6461\n","Epoch 1500 | DGI: 0.6931 | Reg: -0.5313 | λ*Reg: -0.0478 | Total: 0.6453\n","Epoch 2000 | DGI: 0.6931 | Reg: -0.5315 | λ*Reg: -0.0478 | Total: 0.6453\n","Epoch 2500 | DGI: 0.6931 | Reg: -0.5347 | λ*Reg: -0.0481 | Total: 0.6450\n","Epoch 3000 | DGI: 0.6931 | Reg: -0.5232 | λ*Reg: -0.0471 | Total: 0.6461\n","Epoch 3500 | DGI: 0.6931 | Reg: -0.5373 | λ*Reg: -0.0484 | Total: 0.6448\n","Epoch 4000 | DGI: 0.6931 | Reg: -0.5298 | λ*Reg: -0.0477 | Total: 0.6455\n","Epoch 4500 | DGI: 0.6931 | Reg: -0.5139 | λ*Reg: -0.0463 | Total: 0.6469\n","Epoch 5000 | DGI: 0.6931 | Reg: -0.5379 | λ*Reg: -0.0484 | Total: 0.6447\n","Epoch 5500 | DGI: 0.6931 | Reg: -0.5417 | λ*Reg: -0.0487 | Total: 0.6444\n","Epoch 6000 | DGI: 0.6931 | Reg: -0.5408 | λ*Reg: -0.0487 | Total: 0.6445\n","Epoch 6500 | DGI: 0.6931 | Reg: -0.5392 | λ*Reg: -0.0485 | Total: 0.6446\n","Epoch 7000 | DGI: 0.6931 | Reg: -0.5179 | λ*Reg: -0.0466 | Total: 0.6465\n","Epoch 7500 | DGI: 0.6931 | Reg: -0.5273 | λ*Reg: -0.0475 | Total: 0.6457\n","Epoch 8000 | DGI: 0.6931 | Reg: -0.5289 | λ*Reg: -0.0476 | Total: 0.6455\n","Epoch 8500 | DGI: 0.6931 | Reg: -0.5329 | λ*Reg: -0.0480 | Total: 0.6452\n","Epoch 9000 | DGI: 0.6931 | Reg: -0.5346 | λ*Reg: -0.0481 | Total: 0.6450\n","Epoch 9500 | DGI: 0.6931 | Reg: -0.5334 | λ*Reg: -0.0480 | Total: 0.6451\n","Epoch 10000 | DGI: 0.6931 | Reg: -0.5304 | λ*Reg: -0.0477 | Total: 0.6454\n","Run 6 | Accuracy: 0.7534 | Precision: 0.6636 | Recall: 0.7889 | F1: 0.7208 | LogLoss: 2.5953\n","\n","--- Run 7/10 ---\n","Epoch    0 | DGI: 0.7134 | Reg: -0.2342 | λ*Reg: -0.0211 | Total: 0.6923\n","Epoch  500 | DGI: 0.6931 | Reg: -0.5378 | λ*Reg: -0.0484 | Total: 0.6447\n","Epoch 1000 | DGI: 0.6931 | Reg: -0.5172 | λ*Reg: -0.0465 | Total: 0.6466\n","Epoch 1500 | DGI: 0.6931 | Reg: -0.5400 | λ*Reg: -0.0486 | Total: 0.6445\n","Epoch 2000 | DGI: 0.6931 | Reg: -0.5326 | λ*Reg: -0.0479 | Total: 0.6452\n","Epoch 2500 | DGI: 0.6931 | Reg: -0.5432 | λ*Reg: -0.0489 | Total: 0.6443\n","Epoch 3000 | DGI: 0.6931 | Reg: -0.5410 | λ*Reg: -0.0487 | Total: 0.6445\n","Epoch 3500 | DGI: 0.6931 | Reg: -0.5387 | λ*Reg: -0.0485 | Total: 0.6447\n","Epoch 4000 | DGI: 0.6931 | Reg: -0.5327 | λ*Reg: -0.0479 | Total: 0.6452\n","Epoch 4500 | DGI: 0.6931 | Reg: -0.5363 | λ*Reg: -0.0483 | Total: 0.6449\n","Epoch 5000 | DGI: 0.6931 | Reg: -0.5388 | λ*Reg: -0.0485 | Total: 0.6447\n","Epoch 5500 | DGI: 0.6931 | Reg: -0.5200 | λ*Reg: -0.0468 | Total: 0.6463\n","Epoch 6000 | DGI: 0.6931 | Reg: -0.5319 | λ*Reg: -0.0479 | Total: 0.6453\n","Epoch 6500 | DGI: 0.6931 | Reg: -0.5425 | λ*Reg: -0.0488 | Total: 0.6443\n","Epoch 7000 | DGI: 0.6931 | Reg: -0.5303 | λ*Reg: -0.0477 | Total: 0.6454\n","Epoch 7500 | DGI: 0.6931 | Reg: -0.5311 | λ*Reg: -0.0478 | Total: 0.6454\n","Epoch 8000 | DGI: 0.6931 | Reg: -0.5246 | λ*Reg: -0.0472 | Total: 0.6459\n","Epoch 8500 | DGI: 0.6931 | Reg: -0.5348 | λ*Reg: -0.0481 | Total: 0.6450\n","Epoch 9000 | DGI: 0.6931 | Reg: -0.5174 | λ*Reg: -0.0466 | Total: 0.6466\n","Epoch 9500 | DGI: 0.6931 | Reg: -0.5400 | λ*Reg: -0.0486 | Total: 0.6446\n","Epoch 10000 | DGI: 0.6931 | Reg: -0.5361 | λ*Reg: -0.0482 | Total: 0.6449\n","Run 7 | Accuracy: 0.7489 | Precision: 0.6545 | Recall: 0.8000 | F1: 0.7200 | LogLoss: 2.6839\n","\n","--- Run 8/10 ---\n","Epoch    0 | DGI: 0.7133 | Reg: -0.2366 | λ*Reg: -0.0213 | Total: 0.6920\n","Epoch  500 | DGI: 0.6931 | Reg: -0.5085 | λ*Reg: -0.0458 | Total: 0.6474\n","Epoch 1000 | DGI: 0.6931 | Reg: -0.5212 | λ*Reg: -0.0469 | Total: 0.6462\n","Epoch 1500 | DGI: 0.6931 | Reg: -0.5289 | λ*Reg: -0.0476 | Total: 0.6455\n","Epoch 2000 | DGI: 0.6931 | Reg: -0.5159 | λ*Reg: -0.0464 | Total: 0.6467\n","Epoch 2500 | DGI: 0.6931 | Reg: -0.5271 | λ*Reg: -0.0474 | Total: 0.6457\n","Epoch 3000 | DGI: 0.6931 | Reg: -0.5312 | λ*Reg: -0.0478 | Total: 0.6453\n","Epoch 3500 | DGI: 0.6931 | Reg: -0.5367 | λ*Reg: -0.0483 | Total: 0.6448\n","Epoch 4000 | DGI: 0.6931 | Reg: -0.5279 | λ*Reg: -0.0475 | Total: 0.6456\n","Epoch 4500 | DGI: 0.6931 | Reg: -0.5210 | λ*Reg: -0.0469 | Total: 0.6463\n","Epoch 5000 | DGI: 0.6931 | Reg: -0.5330 | λ*Reg: -0.0480 | Total: 0.6452\n","Epoch 5500 | DGI: 0.6931 | Reg: -0.5357 | λ*Reg: -0.0482 | Total: 0.6449\n","Epoch 6000 | DGI: 0.6931 | Reg: -0.5386 | λ*Reg: -0.0485 | Total: 0.6447\n","Epoch 6500 | DGI: 0.6931 | Reg: -0.5374 | λ*Reg: -0.0484 | Total: 0.6448\n","Epoch 7000 | DGI: 0.6931 | Reg: -0.5361 | λ*Reg: -0.0482 | Total: 0.6449\n","Epoch 7500 | DGI: 0.6931 | Reg: -0.5355 | λ*Reg: -0.0482 | Total: 0.6450\n","Epoch 8000 | DGI: 0.6931 | Reg: -0.5314 | λ*Reg: -0.0478 | Total: 0.6453\n","Epoch 8500 | DGI: 0.6931 | Reg: -0.5280 | λ*Reg: -0.0475 | Total: 0.6456\n","Epoch 9000 | DGI: 0.6931 | Reg: -0.5313 | λ*Reg: -0.0478 | Total: 0.6453\n","Epoch 9500 | DGI: 0.6931 | Reg: -0.5380 | λ*Reg: -0.0484 | Total: 0.6447\n","Epoch 10000 | DGI: 0.6931 | Reg: -0.5221 | λ*Reg: -0.0470 | Total: 0.6462\n","Run 8 | Accuracy: 0.7623 | Precision: 0.6697 | Recall: 0.8111 | F1: 0.7337 | LogLoss: 2.5278\n","\n","--- Run 9/10 ---\n","Epoch    0 | DGI: 0.7129 | Reg: -0.2336 | λ*Reg: -0.0210 | Total: 0.6919\n","Epoch  500 | DGI: 0.6931 | Reg: -0.5366 | λ*Reg: -0.0483 | Total: 0.6449\n","Epoch 1000 | DGI: 0.6931 | Reg: -0.5310 | λ*Reg: -0.0478 | Total: 0.6454\n","Epoch 1500 | DGI: 0.6931 | Reg: -0.5392 | λ*Reg: -0.0485 | Total: 0.6446\n","Epoch 2000 | DGI: 0.6931 | Reg: -0.5392 | λ*Reg: -0.0485 | Total: 0.6446\n","Epoch 2500 | DGI: 0.6931 | Reg: -0.5187 | λ*Reg: -0.0467 | Total: 0.6465\n","Epoch 3000 | DGI: 0.6931 | Reg: -0.5128 | λ*Reg: -0.0462 | Total: 0.6470\n","Epoch 3500 | DGI: 0.6931 | Reg: -0.5293 | λ*Reg: -0.0476 | Total: 0.6455\n","Epoch 4000 | DGI: 0.6931 | Reg: -0.5251 | λ*Reg: -0.0473 | Total: 0.6459\n","Epoch 4500 | DGI: 0.6931 | Reg: -0.5396 | λ*Reg: -0.0486 | Total: 0.6446\n","Epoch 5000 | DGI: 0.6931 | Reg: -0.5314 | λ*Reg: -0.0478 | Total: 0.6453\n","Epoch 5500 | DGI: 0.6931 | Reg: -0.5336 | λ*Reg: -0.0480 | Total: 0.6451\n","Epoch 6000 | DGI: 0.6931 | Reg: -0.5311 | λ*Reg: -0.0478 | Total: 0.6454\n","Epoch 6500 | DGI: 0.6931 | Reg: -0.5382 | λ*Reg: -0.0484 | Total: 0.6447\n","Epoch 7000 | DGI: 0.6931 | Reg: -0.5387 | λ*Reg: -0.0485 | Total: 0.6447\n","Epoch 7500 | DGI: 0.6931 | Reg: -0.5352 | λ*Reg: -0.0482 | Total: 0.6450\n","Epoch 8000 | DGI: 0.6931 | Reg: -0.5267 | λ*Reg: -0.0474 | Total: 0.6457\n","Epoch 8500 | DGI: 0.6931 | Reg: -0.5221 | λ*Reg: -0.0470 | Total: 0.6462\n","Epoch 9000 | DGI: 0.6931 | Reg: -0.5347 | λ*Reg: -0.0481 | Total: 0.6450\n","Epoch 9500 | DGI: 0.6931 | Reg: -0.5382 | λ*Reg: -0.0484 | Total: 0.6447\n","Epoch 10000 | DGI: 0.6931 | Reg: -0.5399 | λ*Reg: -0.0486 | Total: 0.6446\n","Run 9 | Accuracy: 0.7668 | Precision: 0.6759 | Recall: 0.8111 | F1: 0.7374 | LogLoss: 2.5983\n","\n","--- Run 10/10 ---\n","Epoch    0 | DGI: 0.7085 | Reg: -0.2381 | λ*Reg: -0.0214 | Total: 0.6871\n","Epoch  500 | DGI: 0.6931 | Reg: -0.5241 | λ*Reg: -0.0472 | Total: 0.6460\n","Epoch 1000 | DGI: 0.6931 | Reg: -0.5423 | λ*Reg: -0.0488 | Total: 0.6443\n","Epoch 1500 | DGI: 0.6931 | Reg: -0.5394 | λ*Reg: -0.0485 | Total: 0.6446\n","Epoch 2000 | DGI: 0.6931 | Reg: -0.5350 | λ*Reg: -0.0481 | Total: 0.6450\n","Epoch 2500 | DGI: 0.6931 | Reg: -0.5385 | λ*Reg: -0.0485 | Total: 0.6447\n","Epoch 3000 | DGI: 0.6931 | Reg: -0.5348 | λ*Reg: -0.0481 | Total: 0.6450\n","Epoch 3500 | DGI: 0.6931 | Reg: -0.5406 | λ*Reg: -0.0487 | Total: 0.6445\n","Epoch 4000 | DGI: 0.6931 | Reg: -0.5391 | λ*Reg: -0.0485 | Total: 0.6446\n","Epoch 4500 | DGI: 0.6931 | Reg: -0.5408 | λ*Reg: -0.0487 | Total: 0.6445\n","Epoch 5000 | DGI: 0.6931 | Reg: -0.5393 | λ*Reg: -0.0485 | Total: 0.6446\n","Epoch 5500 | DGI: 0.6931 | Reg: -0.5394 | λ*Reg: -0.0485 | Total: 0.6446\n","Epoch 6000 | DGI: 0.6931 | Reg: -0.5356 | λ*Reg: -0.0482 | Total: 0.6449\n","Epoch 6500 | DGI: 0.6931 | Reg: -0.5355 | λ*Reg: -0.0482 | Total: 0.6450\n","Epoch 7000 | DGI: 0.6931 | Reg: -0.5412 | λ*Reg: -0.0487 | Total: 0.6444\n","Epoch 7500 | DGI: 0.6931 | Reg: -0.5387 | λ*Reg: -0.0485 | Total: 0.6447\n","Epoch 8000 | DGI: 0.6931 | Reg: -0.5318 | λ*Reg: -0.0479 | Total: 0.6453\n","Epoch 8500 | DGI: 0.6931 | Reg: -0.5386 | λ*Reg: -0.0485 | Total: 0.6447\n","Epoch 9000 | DGI: 0.6931 | Reg: -0.5377 | λ*Reg: -0.0484 | Total: 0.6448\n","Epoch 9500 | DGI: 0.6931 | Reg: -0.5280 | λ*Reg: -0.0475 | Total: 0.6456\n","Epoch 10000 | DGI: 0.6931 | Reg: -0.5336 | λ*Reg: -0.0480 | Total: 0.6451\n","Run 10 | Accuracy: 0.7489 | Precision: 0.6545 | Recall: 0.8000 | F1: 0.7200 | LogLoss: 2.5552\n","\n","--- RESULTS FOR LAMBDA = 0.09 ---\n","Accuracy : 0.7538 ± 0.0118\n","Precision: 0.6610 ± 0.0135\n","Recall   : 0.8011 ± 0.0105\n","F1 Score : 0.7243 ± 0.0119\n","Log Loss : 2.6345 ± 0.1310\n","\n","================ FINAL SUMMARY FOR ALL LAMBDAS ================\n","\n","  Lambda |           Accuracy |          Precision |             Recall |           F1 Score |           Log Loss\n","------------------------------------------------------------------------------------------------------------\n","    0.09 | 0.7538 ± 0.0118 | 0.6610 ± 0.0135 | 0.8011 ± 0.0105 | 0.7243 ± 0.0119 | 2.6345 ± 0.1310\n"]}]}]}
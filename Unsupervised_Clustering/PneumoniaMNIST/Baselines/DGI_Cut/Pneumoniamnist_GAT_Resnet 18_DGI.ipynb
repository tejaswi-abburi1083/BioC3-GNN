{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OnwYAezSObrB","executionInfo":{"status":"ok","timestamp":1766930081536,"user_tz":-330,"elapsed":26664,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}},"outputId":"e0213be2-f64a-41b0-e356-8bfab291ee3c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install -q torch_geometric\n","!pip install -q class_resolver\n","!pip3 install pymatting"],"metadata":{"id":"75l8Tlo5OcIO","outputId":"aa7778b1-bfd6-4d16-fab1-0257594730d7","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.8/1.3 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","execution_count":27,"metadata":{"id":"YXFd7Zsz-3XQ","executionInfo":{"status":"ok","timestamp":1766931232495,"user_tz":-330,"elapsed":7,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch_geometric.data import Data\n","from torch_geometric.nn import GATConv\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss\n","from torch.utils.data import TensorDataset, DataLoader, Subset\n","import random\n","from torchvision import models"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"JQ6KiAvW-5rn","executionInfo":{"status":"ok","timestamp":1766931236205,"user_tz":-330,"elapsed":2559,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"outputs":[],"source":["#data = np.load('pneumoniamnist_224.npz', allow_pickle=True)\n","data = np.load('/content/drive/MyDrive/TejaswiAbburi_va797/Dataset/Medmnist_data/pneumoniamnist_224.npz', allow_pickle=True)\n","# Combine train, val, and test sets\n","all_images = np.concatenate([data['train_images'], data['val_images'], data['test_images']], axis=0)\n","all_labels = np.concatenate([data['train_labels'], data['val_labels'], data['test_labels']], axis=0).squeeze()"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"33SSqbZiNsmM","outputId":"78768d97-f135-4cb5-bb4c-ad86595c6128","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1766931238040,"user_tz":-330,"elapsed":1834,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([5856, 3, 224, 224]) torch.Size([5856])\n"]}],"source":["images = all_images.astype(np.float32) / 255.0\n","images = np.repeat(images[:, None, :, :], 3, axis=1)  # Convert to 3 channels (N, 3, 224, 224)\n","\n","# Convert to torch tensors\n","X = torch.tensor(images)\n","y = torch.tensor(all_labels).long()\n","print(X.shape, y.shape)"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"_vBev8exNsmO","executionInfo":{"status":"ok","timestamp":1766931238285,"user_tz":-330,"elapsed":239,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"outputs":[],"source":["dataset = TensorDataset(X, y)\n","class0_indices = [i for i in range(len(y)) if y[i] == 0]\n","class1_indices = [i for i in range(len(y)) if y[i] == 1]\n","\n","random.seed(42)\n","sampled_class0 = random.sample(class0_indices, min(2000, len(class0_indices)))\n","sampled_class1 = random.sample(class1_indices, min(2000, len(class1_indices)))\n","\n","combined_indices = sampled_class0 + sampled_class1\n","random.shuffle(combined_indices)\n","\n","# Final subset\n","final_dataset = Subset(dataset, combined_indices)\n","final_loader = DataLoader(final_dataset, batch_size=64, shuffle=False)\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\""]},{"cell_type":"code","execution_count":46,"metadata":{"id":"zgvIH9HqNsmQ","outputId":"46767f68-1279-4221-af96-aaa39349eb33","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1766937844195,"user_tz":-330,"elapsed":1510,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["Feature shape: (3583, 512)\n","Label shape: (3583,)\n"]}],"source":["resnet = models.resnet18(pretrained=True)\n","resnet.fc = nn.Identity()  # Remove final classification layer\n","resnet = resnet.cuda() if torch.cuda.is_available() else resnet\n","resnet.eval()\n","resnet_feats = []\n","y_list = []\n","\n","with torch.no_grad():\n","    for imgs, labels in final_loader:\n","        imgs = imgs.cuda() if torch.cuda.is_available() else imgs\n","        features = resnet(imgs)\n","        resnet_feats.append(features.cpu())\n","        y_list.extend(labels.cpu().tolist())\n","F = torch.cat(resnet_feats, dim=0).numpy().astype(np.float32)\n","y_labels = np.array(y_list).astype(np.float32)\n","\n","print(\"Feature shape:\", F.shape)\n","print(\"Label shape:\", y_labels.shape)"]},{"cell_type":"code","execution_count":47,"metadata":{"id":"MkKdnnzI-7zI","executionInfo":{"status":"ok","timestamp":1766937844198,"user_tz":-330,"elapsed":1,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"outputs":[],"source":["def create_adj(F, alpha=1):\n","    F_norm = F / np.linalg.norm(F, axis=1, keepdims=True)\n","    W = np.dot(F_norm, F_norm.T)\n","    W = (W >= alpha).astype(np.float32)\n","    return W"]},{"cell_type":"code","execution_count":48,"metadata":{"id":"Qk8neEfFl7kX","executionInfo":{"status":"ok","timestamp":1766937844880,"user_tz":-330,"elapsed":9,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"outputs":[],"source":["def asymmetrize_random(adj_matrix, seed=None):\n","    \"\"\"\n","    Randomly orient each undirected edge from a symmetric adjacency matrix.\n","    \"\"\"\n","    adj = np.array(adj_matrix, dtype=np.float32)\n","    n = adj.shape[0]\n","    asym = np.zeros((n, n), dtype=np.float32)\n","    rng = np.random.default_rng(seed)\n","\n","    for i in range(n):\n","        for j in range(i + 1, n):\n","            if adj[i, j]:\n","                if rng.random() < 0.5:\n","                    asym[i, j] = adj[i, j]\n","                else:\n","                    asym[j, i] = adj[i, j]\n","\n","    return asym"]},{"cell_type":"code","source":["def load_data(adj, node_feats):\n","    node_feats = torch.from_numpy(node_feats).float()\n","    edge_index = torch.from_numpy(np.array(np.nonzero(adj))).long()\n","    return node_feats, edge_index"],"metadata":{"id":"f0DKxjBCP0Ed","executionInfo":{"status":"ok","timestamp":1766937845782,"user_tz":-330,"elapsed":7,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","execution_count":50,"metadata":{"id":"ewvzRu9zNsmT","outputId":"f06c39ee-9264-4f67-9dcd-e00b1962ff9e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1766937847002,"user_tz":-330,"elapsed":156,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Data(x=[3583, 512], edge_index=[2, 1642249])\n"]}],"source":["features = F # Use ResNet embeddings\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","W0 = create_adj(features, alpha=0.9)\n","# W_asym = asymmetrize_random(W0, seed=42)\n","node_feats, edge_index = load_data(W0, features)\n","data = Data(x=node_feats, edge_index=edge_index).to(device)\n","A = torch.from_numpy(W0).to(device)\n","print(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zz160NfE--Tg"},"outputs":[],"source":["# features = F # Use ResNet embeddings\n","\n","# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","# W0 = create_adj(features, alpha=0.9)\n","# W_asym = asymmetrize_random(W0, seed=42)\n","# node_feats, edge_index = load_data(W_asym, features) # Use ResNet embeddings here as well\n","# data = Data(x=node_feats, edge_index=edge_index).to(device)\n","# A = torch.from_numpy(W_asym).to(device)\n","# print(data)"]},{"cell_type":"code","execution_count":51,"metadata":{"id":"nhh413xH_Awp","executionInfo":{"status":"ok","timestamp":1766937848047,"user_tz":-330,"elapsed":4,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"outputs":[],"source":["class GATEncoder(torch.nn.Module):\n","    def __init__(self, input_dim, hidden_dim, heads, device, activ):\n","        super(GATEncoder, self).__init__()\n","        self.device = device\n","        self.gat1 = GATConv(input_dim, hidden_dim, heads=heads)\n","        self.batchnorm = nn.BatchNorm1d(hidden_dim * heads)\n","        self.dropout = nn.Dropout(0.25)\n","        self.mlp = nn.Linear(hidden_dim * heads, hidden_dim * heads)\n","\n","    def forward(self, data):\n","        x, edge_index = data.x, data.edge_index\n","        x = self.gat1(x, edge_index)\n","        x = self.dropout(x)\n","        x = self.batchnorm(x)\n","        logits = self.mlp(x)\n","        return logits"]},{"cell_type":"code","execution_count":52,"metadata":{"id":"t1CWUjjG48t_","executionInfo":{"status":"ok","timestamp":1766937849155,"user_tz":-330,"elapsed":25,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"outputs":[],"source":["class AvgReadout(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, seq, msk=None):\n","        if msk is None:\n","            return torch.mean(seq, 0)\n","        else:\n","            msk = torch.unsqueeze(msk, -1)\n","            return torch.sum(seq * msk, 0) / torch.sum(msk)"]},{"cell_type":"code","execution_count":53,"metadata":{"id":"IbNOUPjg42s9","executionInfo":{"status":"ok","timestamp":1766937850626,"user_tz":-330,"elapsed":2,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"outputs":[],"source":["class Discriminator(nn.Module):\n","    def __init__(self, n_h):\n","        super().__init__()\n","        self.f_k = nn.Bilinear(n_h, n_h, 1)\n","        nn.init.xavier_uniform_(self.f_k.weight.data)\n","        if self.f_k.bias is not None:\n","            self.f_k.bias.data.fill_(0.0)\n","\n","    def forward(self, c, h_pl, h_mi):\n","        c_x = torch.unsqueeze(c, 0).expand_as(h_pl)\n","        sc_1 = torch.squeeze(self.f_k(h_pl, c_x), 1)\n","        sc_2 = torch.squeeze(self.f_k(h_mi, c_x), 1)\n","        logits = torch.cat((sc_1, sc_2), 0)\n","        return logits"]},{"cell_type":"code","execution_count":54,"metadata":{"id":"7X7cwk2N8Al2","executionInfo":{"status":"ok","timestamp":1766937851902,"user_tz":-330,"elapsed":2,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"outputs":[],"source":["class DGI(nn.Module):\n","    def __init__(self, n_in, n_h, heads, dropout=0.25):\n","        super().__init__()\n","        self.gat1 = GATEncoder(n_in, n_h, heads=heads, device='cuda' if torch.cuda.is_available() else 'cpu', activ=nn.ELU())\n","        self.read = AvgReadout()\n","        self.sigm = nn.Sigmoid()\n","        self.disc = Discriminator(n_h * heads)\n","\n","    def forward(self, seq1, seq2, edge_index):\n","        # Create Data objects for the GATEncoder\n","        data1 = Data(x=seq1, edge_index=edge_index)\n","        data2 = Data(x=seq2, edge_index=edge_index)\n","\n","        h_1 = self.gat1(data1)\n","        c = self.read(h_1)\n","        c = self.sigm(c)\n","        h_2 = self.gat1(data2)\n","        logits = self.disc(c, h_1, h_2)\n","        return logits, h_1"]},{"cell_type":"code","execution_count":55,"metadata":{"id":"yVuaMXM1C9T3","executionInfo":{"status":"ok","timestamp":1766937853209,"user_tz":-330,"elapsed":14,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","\n","class DGI_with_classifier(DGI):\n","    def __init__(self, n_in, n_h, heads, n_classes=2, cut=0, dropout=0.25):\n","        super().__init__(n_in, n_h, heads, dropout=dropout)\n","        self.classifier = nn.Linear(n_h * heads, n_classes)\n","        self.cut = cut\n","\n","\n","    def get_embeddings(self, node_feats, edge_index):\n","        _, embeddings = self.forward(node_feats, node_feats, edge_index)\n","        return embeddings\n","\n","    def cut_loss(self, A, S):\n","        # ensure tensor\n","        if isinstance(A, np.ndarray):\n","            A = torch.from_numpy(A).float().to(S.device)\n","\n","        S = F.softmax(S, dim=1)   # cluster assignment\n","        A_pool = torch.matmul(torch.matmul(A, S).t(), S)\n","        num = torch.trace(A_pool)\n","\n","        D = torch.diag(torch.sum(A, dim=-1))\n","        D_pooled = torch.matmul(torch.matmul(D, S).t(), S)\n","        den = torch.trace(D_pooled)\n","\n","        mincut_loss = -(num / den)\n","\n","        St_S = torch.matmul(S.t(), S)\n","        I_S = torch.eye(S.shape[1], device=A.device)\n","        ortho_loss = torch.norm(St_S / torch.norm(St_S) - I_S / torch.norm(I_S))\n","\n","        return mincut_loss + ortho_loss\n","\n","    def modularity_loss(self, A, S):\n","        # ensure tensor\n","        if isinstance(A, np.ndarray):\n","            A = torch.from_numpy(A).float().to(S.device)\n","\n","        C = F.softmax(S, dim=1)   # cluster assignment\n","        d = torch.sum(A, dim=1)\n","        m = torch.sum(A)\n","\n","        B = A - torch.outer(d, d) / (2 * m)\n","\n","        I_S = torch.eye(C.shape[1], device=A.device)\n","        k = torch.norm(I_S)\n","        n = S.shape[0]\n","\n","        modularity_term = (-1 / (2 * m)) * torch.trace(torch.mm(torch.mm(C.t(), B), C))\n","        collapse_reg_term = (torch.sqrt(k) / n) * torch.norm(torch.sum(C, dim=0), p='fro') - 1\n","\n","        return modularity_term + collapse_reg_term\n","\n","    def Reg_loss(self, A, embeddings):\n","        # classifier output used as soft cluster assignment\n","        logits = self.classifier(embeddings)\n","\n","        if self.cut == 1:\n","            return self.cut_loss(A, logits)\n","        else:\n","            return self.modularity_loss(A, logits)"]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4ClSGTZaA_xt","outputId":"c740bd08-9d95-4a83-aaec-c2e140f29f2d","executionInfo":{"status":"ok","timestamp":1766931789846,"user_tz":-330,"elapsed":541978,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0 | DGI Loss: 0.7157 | Reg Loss: -0.2466 | Total: 0.6911\n","Epoch 500 | DGI Loss: 0.6935 | Reg Loss: -0.8921 | Total: 0.6043\n","Epoch 1000 | DGI Loss: 0.6963 | Reg Loss: -0.8960 | Total: 0.6067\n","Epoch 1500 | DGI Loss: 0.6934 | Reg Loss: -0.9098 | Total: 0.6024\n","Epoch 2000 | DGI Loss: 0.6932 | Reg Loss: -0.9117 | Total: 0.6020\n","Epoch 2500 | DGI Loss: 0.6931 | Reg Loss: -0.9125 | Total: 0.6019\n","Epoch 3000 | DGI Loss: 0.6932 | Reg Loss: -0.9143 | Total: 0.6017\n","Epoch 3500 | DGI Loss: 0.6939 | Reg Loss: -0.9119 | Total: 0.6027\n","Epoch 4000 | DGI Loss: 0.6932 | Reg Loss: -0.9077 | Total: 0.6024\n","Epoch 4500 | DGI Loss: 0.6931 | Reg Loss: -0.9139 | Total: 0.6018\n","Epoch 5000 | DGI Loss: 0.6934 | Reg Loss: -0.9142 | Total: 0.6020\n"]}],"source":["hidden_dim = 256\n","cut = 1\n","dropout = 0.25\n","heads = 2\n","# make sure adjacency is a tensor on GPU\n","if isinstance(A, np.ndarray):\n","    A = torch.from_numpy(A).float().to(device)\n","else:\n","    A = A.float().to(device)\n","\n","model = DGI_with_classifier(features.shape[1], hidden_dim, heads=heads, n_classes=2, cut=cut, dropout=dropout).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n","bce_loss = nn.BCEWithLogitsLoss()\n","\n","num_epochs = 5000\n","for epoch in range(num_epochs + 1):\n","    model.train()\n","    optimizer.zero_grad()\n","\n","    perm = torch.randperm(node_feats.size(0))\n","    corrupt_features = node_feats[perm]\n","\n","    logits, embeddings = model(node_feats.to(device), corrupt_features.to(device), edge_index.to(device))\n","\n","    lbl = torch.cat([\n","        torch.ones(node_feats.size(0)),\n","        torch.zeros(node_feats.size(0))\n","    ]).to(device)\n","\n","    dgi_loss = bce_loss(logits.squeeze(), lbl)\n","    reg_loss = model.Reg_loss(A, embeddings)\n","    loss = dgi_loss + 0.1 * reg_loss\n","\n","    if epoch % 500 == 0:\n","        print(f\"Epoch {epoch} | DGI Loss: {dgi_loss.item():.4f} | Reg Loss: {reg_loss.item():.4f} | Total: {loss.item():.4f}\")\n","\n","    loss.backward()\n","    optimizer.step()\n"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"giN_kiZckBqV","executionInfo":{"status":"ok","timestamp":1766931789898,"user_tz":-330,"elapsed":49,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"outputs":[],"source":["model.eval()\n","with torch.no_grad():\n","    embeddings = model.get_embeddings(node_feats.to(device), edge_index.to(device))\n","    class_probabilities = F.softmax(model.classifier(embeddings), dim=1).cpu().numpy()\n","\n","y_pred = np.argmax(class_probabilities, axis=1)"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eVJYp9cnauDO","outputId":"9f58c54f-ad7a-4c60-d6f2-d503c08455af","executionInfo":{"status":"ok","timestamp":1766931789924,"user_tz":-330,"elapsed":22,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.914596706670388\n","Accuracy (inverted): 0.08540329332961205\n","Precision: 0.9539121114683816\n","Recall: 0.89\n","F1: 0.9208484221417486\n","Log Loss: 0.6940230976995007\n"]}],"source":["# Extract the true labels for the subset used for prediction\n","y_subset = y[combined_indices].cpu().numpy()\n","\n","acc_score = accuracy_score(y_subset, y_pred)\n","acc_score_inverted = accuracy_score(y_subset, 1 - y_pred)\n","prec_score = precision_score(y_subset, y_pred)\n","rec_score = recall_score(y_subset, y_pred)\n","f1 = f1_score(y_subset, y_pred)\n","log_loss_value = log_loss(y_subset, class_probabilities)\n","\n","print(\"Accuracy:\", acc_score)\n","print(\"Accuracy (inverted):\", acc_score_inverted)\n","print(\"Precision:\", prec_score)\n","print(\"Recall:\", rec_score)\n","print(\"F1:\", f1)\n","print(\"Log Loss:\", log_loss_value)"]},{"cell_type":"code","source":["import numpy as np\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss\n","import torch.nn.functional as F\n","\n","NUM_RUNS = 10\n","\n","acc_list = []\n","acc_inv_list = []\n","prec_list = []\n","rec_list = []\n","f1_list = []\n","logloss_list = []\n","\n","for run in range(NUM_RUNS):\n","    print(f\"\\n===== RUN {run+1}/{NUM_RUNS} =====\")\n","\n","    hidden_dim = 256\n","    cut = 1\n","    dropout = 0.25\n","\n","\n","    # make sure adjacency is tensor on GPU\n","    if isinstance(A, np.ndarray):\n","        A_gpu = torch.from_numpy(A).float().to(device)\n","    else:\n","        A_gpu = A.float().to(device)\n","\n","    model = DGI_with_classifier(features.shape[1], hidden_dim, heads=heads, n_classes=2, cut=cut, dropout=dropout).to(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n","    bce_loss = nn.BCEWithLogitsLoss()\n","\n","    num_epochs = 5000\n","    for epoch in range(num_epochs + 1):\n","        model.train()\n","        optimizer.zero_grad()\n","\n","        perm = torch.randperm(node_feats.size(0))\n","        corrupt_features = node_feats[perm]\n","\n","        logits, embeddings = model(\n","            node_feats.to(device),\n","            corrupt_features.to(device),\n","            edge_index.to(device)\n","        )\n","\n","        lbl = torch.cat([\n","            torch.ones(node_feats.size(0)),\n","            torch.zeros(node_feats.size(0))\n","        ]).to(device)\n","\n","        dgi_loss = bce_loss(logits.squeeze(), lbl)\n","        reg_loss = model.Reg_loss(A_gpu, embeddings)\n","        loss = dgi_loss + 0.1 * reg_loss\n","\n","        if epoch % 500 == 0:\n","            print(f\"Epoch {epoch} | DGI Loss: {dgi_loss.item():.4f} \"\n","                  f\"| Reg Loss: {reg_loss.item():.4f} | Total: {loss.item():.4f}\")\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","    # ---------- Evaluation ----------\n","    model.eval()\n","    with torch.no_grad():\n","        embeddings = model.get_embeddings(node_feats.to(device), edge_index.to(device))\n","        class_probabilities = F.softmax(model.classifier(embeddings), dim=1).cpu().numpy()\n","\n","    y_pred = np.argmax(class_probabilities, axis=1)\n","    y_subset = y[combined_indices].cpu().numpy()\n","\n","    acc = accuracy_score(y_subset, y_pred)\n","    acc_inv = accuracy_score(y_subset, 1 - y_pred)\n","    prec = precision_score(y_subset, y_pred)\n","    rec = recall_score(y_subset, y_pred)\n","    f1 = f1_score(y_subset, y_pred)\n","    ll = log_loss(y_subset, class_probabilities)\n","\n","    acc_list.append(acc)\n","    acc_inv_list.append(acc_inv)\n","    prec_list.append(prec)\n","    rec_list.append(rec)\n","    f1_list.append(f1)\n","    logloss_list.append(ll)\n","\n","# -------- mean ± std --------\n","def mean_std(a):\n","    return np.mean(a), np.std(a)\n","\n","print(\"\\n===== FINAL RESULTS OVER 10 RUNS =====\")\n","m, s = mean_std(acc_list);     print(f\"Accuracy: {m:.4f} ± {s:.4f}\")\n","m, s = mean_std(acc_inv_list); print(f\"Accuracy (Inverted): {m:.4f} ± {s:.4f}\")\n","m, s = mean_std(prec_list);    print(f\"Precision: {m:.4f} ± {s:.4f}\")\n","m, s = mean_std(rec_list);     print(f\"Recall: {m:.4f} ± {s:.4f}\")\n","m, s = mean_std(f1_list);      print(f\"F1: {m:.4f} ± {s:.4f}\")\n","m, s = mean_std(logloss_list); print(f\"Log Loss: {m:.4f} ± {s:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dplfmsymR1Ln","executionInfo":{"status":"ok","timestamp":1766937670144,"user_tz":-330,"elapsed":5407037,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}},"outputId":"dea8cdf4-c392-4d4b-fd3d-cdd0934881c4"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","===== RUN 1/10 =====\n","Epoch 0 | DGI Loss: 0.7126 | Reg Loss: -0.2422 | Total: 0.6884\n","Epoch 500 | DGI Loss: 0.6932 | Reg Loss: -0.9025 | Total: 0.6030\n","Epoch 1000 | DGI Loss: 0.6933 | Reg Loss: -0.9095 | Total: 0.6023\n","Epoch 1500 | DGI Loss: 0.6940 | Reg Loss: -0.9134 | Total: 0.6027\n","Epoch 2000 | DGI Loss: 0.6960 | Reg Loss: -0.8947 | Total: 0.6065\n","Epoch 2500 | DGI Loss: 0.6931 | Reg Loss: -0.9114 | Total: 0.6020\n","Epoch 3000 | DGI Loss: 0.6931 | Reg Loss: -0.9125 | Total: 0.6019\n","Epoch 3500 | DGI Loss: 0.6945 | Reg Loss: -0.9111 | Total: 0.6034\n","Epoch 4000 | DGI Loss: 0.9653 | Reg Loss: -0.7613 | Total: 0.8892\n","Epoch 4500 | DGI Loss: 0.6932 | Reg Loss: -0.9058 | Total: 0.6026\n","Epoch 5000 | DGI Loss: 0.6931 | Reg Loss: -0.9119 | Total: 0.6020\n","\n","===== RUN 2/10 =====\n","Epoch 0 | DGI Loss: 0.7116 | Reg Loss: -0.2410 | Total: 0.6875\n","Epoch 500 | DGI Loss: 0.6954 | Reg Loss: -0.8763 | Total: 0.6078\n","Epoch 1000 | DGI Loss: 0.6932 | Reg Loss: -0.9041 | Total: 0.6028\n","Epoch 1500 | DGI Loss: 0.6961 | Reg Loss: -0.9016 | Total: 0.6060\n","Epoch 2000 | DGI Loss: 0.6940 | Reg Loss: -0.9089 | Total: 0.6031\n","Epoch 2500 | DGI Loss: 0.6941 | Reg Loss: -0.9025 | Total: 0.6039\n","Epoch 3000 | DGI Loss: 0.6966 | Reg Loss: -0.9105 | Total: 0.6056\n","Epoch 3500 | DGI Loss: 0.6931 | Reg Loss: -0.9107 | Total: 0.6021\n","Epoch 4000 | DGI Loss: 0.6931 | Reg Loss: -0.9125 | Total: 0.6019\n","Epoch 4500 | DGI Loss: 0.6934 | Reg Loss: -0.9134 | Total: 0.6020\n","Epoch 5000 | DGI Loss: 3.9355 | Reg Loss: -0.7384 | Total: 3.8616\n","\n","===== RUN 3/10 =====\n","Epoch 0 | DGI Loss: 0.7127 | Reg Loss: -0.2436 | Total: 0.6884\n","Epoch 500 | DGI Loss: 0.6932 | Reg Loss: -0.8971 | Total: 0.6035\n","Epoch 1000 | DGI Loss: 0.6933 | Reg Loss: -0.9035 | Total: 0.6029\n","Epoch 1500 | DGI Loss: 0.6938 | Reg Loss: -0.8994 | Total: 0.6039\n","Epoch 2000 | DGI Loss: 0.6977 | Reg Loss: -0.9040 | Total: 0.6073\n","Epoch 2500 | DGI Loss: 0.6964 | Reg Loss: -0.8931 | Total: 0.6071\n","Epoch 3000 | DGI Loss: 0.6939 | Reg Loss: -0.9049 | Total: 0.6034\n","Epoch 3500 | DGI Loss: 0.6932 | Reg Loss: -0.9102 | Total: 0.6021\n","Epoch 4000 | DGI Loss: 0.6933 | Reg Loss: -0.9116 | Total: 0.6021\n","Epoch 4500 | DGI Loss: 0.6931 | Reg Loss: -0.9099 | Total: 0.6022\n","Epoch 5000 | DGI Loss: 0.6931 | Reg Loss: -0.9135 | Total: 0.6018\n","\n","===== RUN 4/10 =====\n","Epoch 0 | DGI Loss: 0.7193 | Reg Loss: -0.2377 | Total: 0.6955\n","Epoch 500 | DGI Loss: 0.6932 | Reg Loss: -0.8980 | Total: 0.6034\n","Epoch 1000 | DGI Loss: 0.7360 | Reg Loss: -0.8977 | Total: 0.6462\n","Epoch 1500 | DGI Loss: 0.6936 | Reg Loss: -0.9078 | Total: 0.6028\n","Epoch 2000 | DGI Loss: 0.6932 | Reg Loss: -0.9046 | Total: 0.6027\n","Epoch 2500 | DGI Loss: 0.6935 | Reg Loss: -0.9070 | Total: 0.6027\n","Epoch 3000 | DGI Loss: 0.7488 | Reg Loss: -0.8897 | Total: 0.6598\n","Epoch 3500 | DGI Loss: 0.6952 | Reg Loss: -0.9095 | Total: 0.6043\n","Epoch 4000 | DGI Loss: 0.6934 | Reg Loss: -0.9118 | Total: 0.6022\n","Epoch 4500 | DGI Loss: 0.6931 | Reg Loss: -0.9098 | Total: 0.6022\n","Epoch 5000 | DGI Loss: 0.6931 | Reg Loss: -0.9132 | Total: 0.6018\n","\n","===== RUN 5/10 =====\n","Epoch 0 | DGI Loss: 0.7121 | Reg Loss: -0.2442 | Total: 0.6877\n","Epoch 500 | DGI Loss: 0.6938 | Reg Loss: -0.8866 | Total: 0.6052\n","Epoch 1000 | DGI Loss: 0.6941 | Reg Loss: -0.8971 | Total: 0.6044\n","Epoch 1500 | DGI Loss: 0.6932 | Reg Loss: -0.9072 | Total: 0.6024\n","Epoch 2000 | DGI Loss: 0.6931 | Reg Loss: -0.9109 | Total: 0.6021\n","Epoch 2500 | DGI Loss: 0.6933 | Reg Loss: -0.9088 | Total: 0.6024\n","Epoch 3000 | DGI Loss: 0.6932 | Reg Loss: -0.9126 | Total: 0.6020\n","Epoch 3500 | DGI Loss: 0.6932 | Reg Loss: -0.9066 | Total: 0.6025\n","Epoch 4000 | DGI Loss: 0.6931 | Reg Loss: -0.9118 | Total: 0.6020\n","Epoch 4500 | DGI Loss: 0.6931 | Reg Loss: -0.9132 | Total: 0.6018\n","Epoch 5000 | DGI Loss: 0.6932 | Reg Loss: -0.9137 | Total: 0.6018\n","\n","===== RUN 6/10 =====\n","Epoch 0 | DGI Loss: 0.7116 | Reg Loss: -0.2495 | Total: 0.6866\n","Epoch 500 | DGI Loss: 0.6997 | Reg Loss: -0.8876 | Total: 0.6110\n","Epoch 1000 | DGI Loss: 0.6932 | Reg Loss: -0.9093 | Total: 0.6022\n","Epoch 1500 | DGI Loss: 0.6932 | Reg Loss: -0.9119 | Total: 0.6020\n","Epoch 2000 | DGI Loss: 0.6935 | Reg Loss: -0.8965 | Total: 0.6039\n","Epoch 2500 | DGI Loss: 0.6931 | Reg Loss: -0.9087 | Total: 0.6023\n","Epoch 3000 | DGI Loss: 0.6932 | Reg Loss: -0.9130 | Total: 0.6019\n","Epoch 3500 | DGI Loss: 0.6934 | Reg Loss: -0.9085 | Total: 0.6025\n","Epoch 4000 | DGI Loss: 0.6931 | Reg Loss: -0.9146 | Total: 0.6017\n","Epoch 4500 | DGI Loss: 0.6931 | Reg Loss: -0.9155 | Total: 0.6016\n","Epoch 5000 | DGI Loss: 0.6952 | Reg Loss: -0.9144 | Total: 0.6037\n","\n","===== RUN 7/10 =====\n","Epoch 0 | DGI Loss: 0.7088 | Reg Loss: -0.2471 | Total: 0.6841\n","Epoch 500 | DGI Loss: 0.6932 | Reg Loss: -0.8980 | Total: 0.6034\n","Epoch 1000 | DGI Loss: 0.6933 | Reg Loss: -0.9054 | Total: 0.6028\n","Epoch 1500 | DGI Loss: 0.6951 | Reg Loss: -0.9066 | Total: 0.6044\n","Epoch 2000 | DGI Loss: 0.6932 | Reg Loss: -0.9048 | Total: 0.6027\n","Epoch 2500 | DGI Loss: 0.6932 | Reg Loss: -0.9100 | Total: 0.6022\n","Epoch 3000 | DGI Loss: 0.6939 | Reg Loss: -0.9133 | Total: 0.6026\n","Epoch 3500 | DGI Loss: 0.6934 | Reg Loss: -0.9142 | Total: 0.6020\n","Epoch 4000 | DGI Loss: 0.6933 | Reg Loss: -0.9153 | Total: 0.6018\n","Epoch 4500 | DGI Loss: 0.6935 | Reg Loss: -0.8924 | Total: 0.6043\n","Epoch 5000 | DGI Loss: 0.6934 | Reg Loss: -0.9004 | Total: 0.6033\n","\n","===== RUN 8/10 =====\n","Epoch 0 | DGI Loss: 0.7166 | Reg Loss: -0.2394 | Total: 0.6926\n","Epoch 500 | DGI Loss: 0.6940 | Reg Loss: -0.8912 | Total: 0.6049\n","Epoch 1000 | DGI Loss: 0.6938 | Reg Loss: -0.9038 | Total: 0.6034\n","Epoch 1500 | DGI Loss: 0.7048 | Reg Loss: -0.8965 | Total: 0.6152\n","Epoch 2000 | DGI Loss: 0.6940 | Reg Loss: -0.9107 | Total: 0.6029\n","Epoch 2500 | DGI Loss: 0.7124 | Reg Loss: -0.9135 | Total: 0.6211\n","Epoch 3000 | DGI Loss: 0.6933 | Reg Loss: -0.9066 | Total: 0.6027\n","Epoch 3500 | DGI Loss: 0.6931 | Reg Loss: -0.9112 | Total: 0.6020\n","Epoch 4000 | DGI Loss: 0.6932 | Reg Loss: -0.9123 | Total: 0.6020\n","Epoch 4500 | DGI Loss: 0.6937 | Reg Loss: -0.9025 | Total: 0.6035\n","Epoch 5000 | DGI Loss: 0.6934 | Reg Loss: -0.9030 | Total: 0.6031\n","\n","===== RUN 9/10 =====\n","Epoch 0 | DGI Loss: 0.7115 | Reg Loss: -0.2491 | Total: 0.6866\n","Epoch 500 | DGI Loss: 0.6932 | Reg Loss: -0.8952 | Total: 0.6037\n","Epoch 1000 | DGI Loss: 0.6957 | Reg Loss: -0.8945 | Total: 0.6062\n","Epoch 1500 | DGI Loss: 0.6932 | Reg Loss: -0.9046 | Total: 0.6027\n","Epoch 2000 | DGI Loss: 0.6932 | Reg Loss: -0.9091 | Total: 0.6023\n","Epoch 2500 | DGI Loss: 0.6931 | Reg Loss: -0.9086 | Total: 0.6023\n","Epoch 3000 | DGI Loss: 0.6932 | Reg Loss: -0.9116 | Total: 0.6020\n","Epoch 3500 | DGI Loss: 0.6932 | Reg Loss: -0.9117 | Total: 0.6021\n","Epoch 4000 | DGI Loss: 0.6932 | Reg Loss: -0.9039 | Total: 0.6029\n","Epoch 4500 | DGI Loss: 0.6931 | Reg Loss: -0.9107 | Total: 0.6021\n","Epoch 5000 | DGI Loss: 0.6932 | Reg Loss: -0.9052 | Total: 0.6027\n","\n","===== RUN 10/10 =====\n","Epoch 0 | DGI Loss: 0.7088 | Reg Loss: -0.2657 | Total: 0.6823\n","Epoch 500 | DGI Loss: 0.6960 | Reg Loss: -0.8717 | Total: 0.6089\n","Epoch 1000 | DGI Loss: 0.6933 | Reg Loss: -0.9047 | Total: 0.6028\n","Epoch 1500 | DGI Loss: 0.6990 | Reg Loss: -0.8950 | Total: 0.6095\n","Epoch 2000 | DGI Loss: 0.6933 | Reg Loss: -0.9093 | Total: 0.6023\n","Epoch 2500 | DGI Loss: 0.6932 | Reg Loss: -0.9125 | Total: 0.6020\n","Epoch 3000 | DGI Loss: 0.6931 | Reg Loss: -0.9035 | Total: 0.6028\n","Epoch 3500 | DGI Loss: 0.6983 | Reg Loss: -0.9100 | Total: 0.6073\n","Epoch 4000 | DGI Loss: 0.6931 | Reg Loss: -0.9130 | Total: 0.6018\n","Epoch 4500 | DGI Loss: 0.6931 | Reg Loss: -0.9100 | Total: 0.6021\n","Epoch 5000 | DGI Loss: 0.6931 | Reg Loss: -0.9139 | Total: 0.6018\n","\n","===== FINAL RESULTS OVER 10 RUNS =====\n","Accuracy: 0.5072 ± 0.4037\n","Accuracy (Inverted): 0.4928 ± 0.4037\n","Precision: 0.5638 ± 0.3973\n","Recall: 0.5151 ± 0.3620\n","F1: 0.5383 ± 0.3788\n","Log Loss: 6.3089 ± 5.7424\n"]}]},{"cell_type":"code","execution_count":56,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u2TNZi1qljP0","outputId":"885249a8-0015-404b-c359-1ff4360a26dd","executionInfo":{"status":"ok","timestamp":1766942790239,"user_tz":-330,"elapsed":2169733,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","================ LAMBDA = 0.1 ================\n","\n","\n","--- Run 1/10 ---\n","Epoch    0 | DGI: 0.7175 | Reg: -0.2362 | λ*Reg: -0.0236 | Total: 0.6939\n","Epoch  500 | DGI: 0.6931 | Reg: -0.8987 | λ*Reg: -0.0899 | Total: 0.6033\n","Epoch 1000 | DGI: 0.6931 | Reg: -0.9055 | λ*Reg: -0.0906 | Total: 0.6026\n","Epoch 1500 | DGI: 0.6931 | Reg: -0.9054 | λ*Reg: -0.0905 | Total: 0.6026\n","Epoch 2000 | DGI: 0.6931 | Reg: -0.9063 | λ*Reg: -0.0906 | Total: 0.6025\n","Epoch 2500 | DGI: 0.6931 | Reg: -0.9070 | λ*Reg: -0.0907 | Total: 0.6024\n","Epoch 3000 | DGI: 0.6931 | Reg: -0.9060 | λ*Reg: -0.0906 | Total: 0.6025\n","Epoch 3500 | DGI: 0.6931 | Reg: -0.9070 | λ*Reg: -0.0907 | Total: 0.6024\n","Epoch 4000 | DGI: 0.6931 | Reg: -0.9062 | λ*Reg: -0.0906 | Total: 0.6025\n","Epoch 4500 | DGI: 0.6931 | Reg: -0.9064 | λ*Reg: -0.0906 | Total: 0.6025\n","Epoch 5000 | DGI: 0.6931 | Reg: -0.9072 | λ*Reg: -0.0907 | Total: 0.6024\n","Run 1 | Accuracy: 0.9129 | Precision: 0.9499 | Recall: 0.8910 | F1: 0.9195 | LogLoss: 0.6641\n","\n","--- Run 2/10 ---\n","Epoch    0 | DGI: 0.7099 | Reg: -0.2494 | λ*Reg: -0.0249 | Total: 0.6849\n","Epoch  500 | DGI: 0.6931 | Reg: -0.8941 | λ*Reg: -0.0894 | Total: 0.6037\n","Epoch 1000 | DGI: 0.6931 | Reg: -0.9033 | λ*Reg: -0.0903 | Total: 0.6028\n","Epoch 1500 | DGI: 0.6931 | Reg: -0.9044 | λ*Reg: -0.0904 | Total: 0.6027\n","Epoch 2000 | DGI: 0.6931 | Reg: -0.9054 | λ*Reg: -0.0905 | Total: 0.6026\n","Epoch 2500 | DGI: 0.6931 | Reg: -0.9039 | λ*Reg: -0.0904 | Total: 0.6028\n","Epoch 3000 | DGI: 0.6931 | Reg: -0.9056 | λ*Reg: -0.0906 | Total: 0.6026\n","Epoch 3500 | DGI: 0.6931 | Reg: -0.9040 | λ*Reg: -0.0904 | Total: 0.6027\n","Epoch 4000 | DGI: 0.6931 | Reg: -0.9052 | λ*Reg: -0.0905 | Total: 0.6026\n","Epoch 4500 | DGI: 0.6931 | Reg: -0.9044 | λ*Reg: -0.0904 | Total: 0.6027\n","Epoch 5000 | DGI: 0.6931 | Reg: -0.9051 | λ*Reg: -0.0905 | Total: 0.6026\n","Run 2 | Accuracy: 0.9057 | Precision: 0.9596 | Recall: 0.8675 | F1: 0.9112 | LogLoss: 0.7744\n","\n","--- Run 3/10 ---\n","Epoch    0 | DGI: 0.7130 | Reg: -0.2389 | λ*Reg: -0.0239 | Total: 0.6891\n","Epoch  500 | DGI: 0.6931 | Reg: -0.9038 | λ*Reg: -0.0904 | Total: 0.6028\n","Epoch 1000 | DGI: 0.6931 | Reg: -0.9060 | λ*Reg: -0.0906 | Total: 0.6025\n","Epoch 1500 | DGI: 0.6931 | Reg: -0.9071 | λ*Reg: -0.0907 | Total: 0.6024\n","Epoch 2000 | DGI: 0.6931 | Reg: -0.9072 | λ*Reg: -0.0907 | Total: 0.6024\n","Epoch 2500 | DGI: 0.6931 | Reg: -0.9074 | λ*Reg: -0.0907 | Total: 0.6024\n","Epoch 3000 | DGI: 0.6931 | Reg: -0.9072 | λ*Reg: -0.0907 | Total: 0.6024\n","Epoch 3500 | DGI: 0.6931 | Reg: -0.9078 | λ*Reg: -0.0908 | Total: 0.6024\n","Epoch 4000 | DGI: 0.6931 | Reg: -0.9076 | λ*Reg: -0.0908 | Total: 0.6024\n","Epoch 4500 | DGI: 0.6931 | Reg: -0.9076 | λ*Reg: -0.0908 | Total: 0.6024\n","Epoch 5000 | DGI: 0.6931 | Reg: -0.9079 | λ*Reg: -0.0908 | Total: 0.6024\n","Run 3 | Accuracy: 0.9121 | Precision: 0.9557 | Recall: 0.8835 | F1: 0.9182 | LogLoss: 0.7601\n","\n","--- Run 4/10 ---\n","Epoch    0 | DGI: 0.7143 | Reg: -0.2384 | λ*Reg: -0.0238 | Total: 0.6905\n","Epoch  500 | DGI: 0.6931 | Reg: -0.9040 | λ*Reg: -0.0904 | Total: 0.6028\n","Epoch 1000 | DGI: 0.6931 | Reg: -0.9066 | λ*Reg: -0.0907 | Total: 0.6025\n","Epoch 1500 | DGI: 0.6931 | Reg: -0.9085 | λ*Reg: -0.0909 | Total: 0.6023\n","Epoch 2000 | DGI: 0.6931 | Reg: -0.9075 | λ*Reg: -0.0908 | Total: 0.6024\n","Epoch 2500 | DGI: 0.6931 | Reg: -0.9068 | λ*Reg: -0.0907 | Total: 0.6025\n","Epoch 3000 | DGI: 0.6931 | Reg: -0.9081 | λ*Reg: -0.0908 | Total: 0.6023\n","Epoch 3500 | DGI: 0.6931 | Reg: -0.9088 | λ*Reg: -0.0909 | Total: 0.6023\n","Epoch 4000 | DGI: 0.6931 | Reg: -0.9087 | λ*Reg: -0.0909 | Total: 0.6023\n","Epoch 4500 | DGI: 0.6931 | Reg: -0.9076 | λ*Reg: -0.0908 | Total: 0.6024\n","Epoch 5000 | DGI: 0.6931 | Reg: -0.9067 | λ*Reg: -0.0907 | Total: 0.6025\n","Run 4 | Accuracy: 0.9118 | Precision: 0.9537 | Recall: 0.8850 | F1: 0.9180 | LogLoss: 0.7856\n","\n","--- Run 5/10 ---\n","Epoch    0 | DGI: 0.7118 | Reg: -0.2353 | λ*Reg: -0.0235 | Total: 0.6883\n","Epoch  500 | DGI: 0.6931 | Reg: -0.8962 | λ*Reg: -0.0896 | Total: 0.6035\n","Epoch 1000 | DGI: 0.6931 | Reg: -0.9030 | λ*Reg: -0.0903 | Total: 0.6028\n","Epoch 1500 | DGI: 0.6931 | Reg: -0.9048 | λ*Reg: -0.0905 | Total: 0.6027\n","Epoch 2000 | DGI: 0.6931 | Reg: -0.9053 | λ*Reg: -0.0905 | Total: 0.6026\n","Epoch 2500 | DGI: 0.6931 | Reg: -0.9054 | λ*Reg: -0.0905 | Total: 0.6026\n","Epoch 3000 | DGI: 0.6931 | Reg: -0.9037 | λ*Reg: -0.0904 | Total: 0.6028\n","Epoch 3500 | DGI: 0.6931 | Reg: -0.9037 | λ*Reg: -0.0904 | Total: 0.6028\n","Epoch 4000 | DGI: 0.6931 | Reg: -0.9050 | λ*Reg: -0.0905 | Total: 0.6027\n","Epoch 4500 | DGI: 0.6931 | Reg: -0.9058 | λ*Reg: -0.0906 | Total: 0.6026\n","Epoch 5000 | DGI: 0.6931 | Reg: -0.9045 | λ*Reg: -0.0905 | Total: 0.6027\n","Run 5 | Accuracy: 0.9112 | Precision: 0.9636 | Recall: 0.8740 | F1: 0.9166 | LogLoss: 0.7166\n","\n","--- Run 6/10 ---\n","Epoch    0 | DGI: 0.7106 | Reg: -0.2426 | λ*Reg: -0.0243 | Total: 0.6863\n","Epoch  500 | DGI: 0.6931 | Reg: -0.8982 | λ*Reg: -0.0898 | Total: 0.6033\n","Epoch 1000 | DGI: 0.6931 | Reg: -0.9061 | λ*Reg: -0.0906 | Total: 0.6025\n","Epoch 1500 | DGI: 0.6931 | Reg: -0.9064 | λ*Reg: -0.0906 | Total: 0.6025\n","Epoch 2000 | DGI: 0.6931 | Reg: -0.9052 | λ*Reg: -0.0905 | Total: 0.6026\n","Epoch 2500 | DGI: 0.6931 | Reg: -0.9053 | λ*Reg: -0.0905 | Total: 0.6026\n","Epoch 3000 | DGI: 0.6931 | Reg: -0.9067 | λ*Reg: -0.0907 | Total: 0.6025\n","Epoch 3500 | DGI: 0.6931 | Reg: -0.9062 | λ*Reg: -0.0906 | Total: 0.6025\n","Epoch 4000 | DGI: 0.6931 | Reg: -0.9064 | λ*Reg: -0.0906 | Total: 0.6025\n","Epoch 4500 | DGI: 0.6931 | Reg: -0.9051 | λ*Reg: -0.0905 | Total: 0.6026\n","Epoch 5000 | DGI: 0.6931 | Reg: -0.9054 | λ*Reg: -0.0905 | Total: 0.6026\n","Run 6 | Accuracy: 0.9043 | Precision: 0.9631 | Recall: 0.8615 | F1: 0.9095 | LogLoss: 0.8312\n","\n","--- Run 7/10 ---\n","Epoch    0 | DGI: 0.7139 | Reg: -0.2390 | λ*Reg: -0.0239 | Total: 0.6900\n","Epoch  500 | DGI: 0.6931 | Reg: -0.9024 | λ*Reg: -0.0902 | Total: 0.6029\n","Epoch 1000 | DGI: 0.6931 | Reg: -0.9067 | λ*Reg: -0.0907 | Total: 0.6025\n","Epoch 1500 | DGI: 0.6931 | Reg: -0.9077 | λ*Reg: -0.0908 | Total: 0.6024\n","Epoch 2000 | DGI: 0.6931 | Reg: -0.9084 | λ*Reg: -0.0908 | Total: 0.6023\n","Epoch 2500 | DGI: 0.6931 | Reg: -0.9074 | λ*Reg: -0.0907 | Total: 0.6024\n","Epoch 3000 | DGI: 0.6931 | Reg: -0.9068 | λ*Reg: -0.0907 | Total: 0.6025\n","Epoch 3500 | DGI: 0.6931 | Reg: -0.9065 | λ*Reg: -0.0906 | Total: 0.6025\n","Epoch 4000 | DGI: 0.6931 | Reg: -0.9082 | λ*Reg: -0.0908 | Total: 0.6023\n","Epoch 4500 | DGI: 0.6931 | Reg: -0.9072 | λ*Reg: -0.0907 | Total: 0.6024\n","Epoch 5000 | DGI: 0.6931 | Reg: -0.9073 | λ*Reg: -0.0907 | Total: 0.6024\n","Run 7 | Accuracy: 0.9118 | Precision: 0.9464 | Recall: 0.8925 | F1: 0.9187 | LogLoss: 0.7362\n","\n","--- Run 8/10 ---\n","Epoch    0 | DGI: 0.7214 | Reg: -0.2513 | λ*Reg: -0.0251 | Total: 0.6962\n","Epoch  500 | DGI: 0.6931 | Reg: -0.8960 | λ*Reg: -0.0896 | Total: 0.6035\n","Epoch 1000 | DGI: 0.6931 | Reg: -0.9045 | λ*Reg: -0.0904 | Total: 0.6027\n","Epoch 1500 | DGI: 0.6931 | Reg: -0.9057 | λ*Reg: -0.0906 | Total: 0.6026\n","Epoch 2000 | DGI: 0.6931 | Reg: -0.9063 | λ*Reg: -0.0906 | Total: 0.6025\n","Epoch 2500 | DGI: 0.6931 | Reg: -0.9055 | λ*Reg: -0.0905 | Total: 0.6026\n","Epoch 3000 | DGI: 0.6931 | Reg: -0.9062 | λ*Reg: -0.0906 | Total: 0.6025\n","Epoch 3500 | DGI: 0.6931 | Reg: -0.9057 | λ*Reg: -0.0906 | Total: 0.6026\n","Epoch 4000 | DGI: 0.6931 | Reg: -0.9069 | λ*Reg: -0.0907 | Total: 0.6025\n","Epoch 4500 | DGI: 0.6931 | Reg: -0.9061 | λ*Reg: -0.0906 | Total: 0.6025\n","Epoch 5000 | DGI: 0.6931 | Reg: -0.9059 | λ*Reg: -0.0906 | Total: 0.6026\n","Run 8 | Accuracy: 0.9138 | Precision: 0.9500 | Recall: 0.8925 | F1: 0.9203 | LogLoss: 0.6753\n","\n","--- Run 9/10 ---\n","Epoch    0 | DGI: 0.7133 | Reg: -0.2334 | λ*Reg: -0.0233 | Total: 0.6899\n","Epoch  500 | DGI: 0.6931 | Reg: -0.8975 | λ*Reg: -0.0897 | Total: 0.6034\n","Epoch 1000 | DGI: 0.6931 | Reg: -0.9055 | λ*Reg: -0.0905 | Total: 0.6026\n","Epoch 1500 | DGI: 0.6931 | Reg: -0.9061 | λ*Reg: -0.0906 | Total: 0.6025\n","Epoch 2000 | DGI: 0.6931 | Reg: -0.9073 | λ*Reg: -0.0907 | Total: 0.6024\n","Epoch 2500 | DGI: 0.6931 | Reg: -0.9056 | λ*Reg: -0.0906 | Total: 0.6026\n","Epoch 3000 | DGI: 0.6931 | Reg: -0.9061 | λ*Reg: -0.0906 | Total: 0.6025\n","Epoch 3500 | DGI: 0.6931 | Reg: -0.9064 | λ*Reg: -0.0906 | Total: 0.6025\n","Epoch 4000 | DGI: 0.6931 | Reg: -0.9063 | λ*Reg: -0.0906 | Total: 0.6025\n","Epoch 4500 | DGI: 0.6931 | Reg: -0.9062 | λ*Reg: -0.0906 | Total: 0.6025\n","Epoch 5000 | DGI: 0.6931 | Reg: -0.9062 | λ*Reg: -0.0906 | Total: 0.6025\n","Run 9 | Accuracy: 0.9132 | Precision: 0.9490 | Recall: 0.8925 | F1: 0.9199 | LogLoss: 0.5967\n","\n","--- Run 10/10 ---\n","Epoch    0 | DGI: 0.7116 | Reg: -0.2842 | λ*Reg: -0.0284 | Total: 0.6832\n","Epoch  500 | DGI: 0.6931 | Reg: -0.8992 | λ*Reg: -0.0899 | Total: 0.6032\n","Epoch 1000 | DGI: 0.6931 | Reg: -0.9055 | λ*Reg: -0.0905 | Total: 0.6026\n","Epoch 1500 | DGI: 0.6931 | Reg: -0.9061 | λ*Reg: -0.0906 | Total: 0.6025\n","Epoch 2000 | DGI: 0.6931 | Reg: -0.9064 | λ*Reg: -0.0906 | Total: 0.6025\n","Epoch 2500 | DGI: 0.6931 | Reg: -0.9076 | λ*Reg: -0.0908 | Total: 0.6024\n","Epoch 3000 | DGI: 0.6931 | Reg: -0.9065 | λ*Reg: -0.0906 | Total: 0.6025\n","Epoch 3500 | DGI: 0.6931 | Reg: -0.9066 | λ*Reg: -0.0907 | Total: 0.6025\n","Epoch 4000 | DGI: 0.6931 | Reg: -0.9060 | λ*Reg: -0.0906 | Total: 0.6025\n","Epoch 4500 | DGI: 0.6931 | Reg: -0.9070 | λ*Reg: -0.0907 | Total: 0.6024\n","Epoch 5000 | DGI: 0.6931 | Reg: -0.9068 | λ*Reg: -0.0907 | Total: 0.6025\n","Run 10 | Accuracy: 0.9121 | Precision: 0.9522 | Recall: 0.8870 | F1: 0.9185 | LogLoss: 0.6703\n","\n","--- RESULTS FOR LAMBDA = 0.1 ---\n","Accuracy : 0.9109 ± 0.0031\n","Precision: 0.9543 ± 0.0057\n","Recall   : 0.8827 ± 0.0107\n","F1 Score : 0.9170 ± 0.0035\n","Log Loss : 0.7210 ± 0.0665\n","\n","================ FINAL SUMMARY FOR ALL LAMBDAS ================\n","\n","  Lambda |           Accuracy |          Precision |             Recall |           F1 Score |           Log Loss\n","------------------------------------------------------------------------------------------------------------\n","     0.1 | 0.9109 ± 0.0031 | 0.9543 ± 0.0057 | 0.8827 ± 0.0107 | 0.9170 ± 0.0035 | 0.7210 ± 0.0665\n"]}],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.optim import AdamW\n","from torch.optim.lr_scheduler import StepLR\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss\n","\n","hidden_dim   = 256\n","cut          = 1\n","dropout      = 0.25\n","num_runs     = 10\n","heads = 2\n","num_epochs   = 5000\n","lambda_list  = [0.1]\n","base_seed    = 42\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","node_feats = node_feats.to(device)\n","edge_index = edge_index.to(device)\n","A = A.to(device)\n","\n","# Use the y_subset created earlier\n","y_subset_np = y_subset.astype(int)\n","\n","N, feats_dim = node_feats.size(0), node_feats.size(1)\n","\n","all_results = []\n","bce_loss = nn.BCEWithLogitsLoss()\n","\n","for lam in lambda_list:\n","    print(f\"\\n================ LAMBDA = {lam} ================\\n\")\n","\n","    acc_scores, prec_scores, rec_scores, f1_scores, log_losses = [], [], [], [], []\n","\n","    for run in range(num_runs):\n","        print(f\"\\n--- Run {run+1}/{num_runs} ---\")\n","\n","        seed = base_seed + run\n","        torch.manual_seed(seed)\n","        np.random.seed(seed)\n","        if torch.cuda.is_available():\n","            torch.cuda.manual_seed_all(seed)\n","\n","\n","        model = DGI_with_classifier(features.shape[1], hidden_dim, heads=heads, n_classes=2, cut=cut, dropout=dropout).to(device)\n","        optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.00001)\n","        scheduler = StepLR(optimizer, step_size=200, gamma=0.5)\n","\n","\n","        for epoch in range(num_epochs + 1):\n","            model.train()\n","            optimizer.zero_grad()\n","\n","            perm = torch.randperm(N, device=device)\n","            corrupt_features = node_feats[perm]\n","\n","            logits, embeddings = model(node_feats, corrupt_features, edge_index)\n","\n","            lbl = torch.cat([torch.ones(N, device=device), torch.zeros(N, device=device)])\n","            dgi_loss = bce_loss(logits.squeeze(), lbl)\n","            reg_loss = model.Reg_loss(A, embeddings)\n","\n","            loss = dgi_loss + lam * reg_loss\n","\n","            if epoch % 500 == 0:\n","                print(f\"Epoch {epoch:4d} | DGI: {dgi_loss.item():.4f} | Reg: {reg_loss.item():.4f} | \"\n","                      f\"λ*Reg: {(lam * reg_loss).item():.4f} | Total: {loss.item():.4f}\")\n","\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step()\n","\n","        model.eval()\n","        with torch.no_grad():\n","            emb = model.get_embeddings(node_feats, edge_index)\n","            logits_cls = model.classifier(emb)                   # [N, 2]\n","            class_probabilities = F.softmax(logits_cls, dim=1).cpu().numpy()\n","            y_pred = np.argmax(class_probabilities, axis=1)\n","\n","        acc  = accuracy_score(y_subset_np, y_pred)\n","        acc_inv = accuracy_score(y_subset_np, 1 - y_pred)\n","\n","        if acc_inv > acc:\n","            acc = acc_inv\n","            y_pred = 1 - y_pred\n","            class_probabilities = class_probabilities[:, ::-1]\n","\n","        prec = precision_score(y_subset_np, y_pred, zero_division=0)\n","        rec  = recall_score(y_subset_np, y_pred, zero_division=0)\n","        f1   = f1_score(y_subset_np, y_pred, zero_division=0)\n","        ll   = log_loss(y_subset_np, class_probabilities)\n","\n","        print(f\"Run {run+1} | Accuracy: {acc:.4f} | Precision: {prec:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | LogLoss: {ll:.4f}\")\n","\n","        acc_scores.append(acc)\n","        prec_scores.append(prec)\n","        rec_scores.append(rec)\n","        f1_scores.append(f1)\n","        log_losses.append(ll)\n","\n","    lambda_results = {\n","        \"lambda\": lam,\n","        \"accuracy\":  (float(np.mean(acc_scores)), float(np.std(acc_scores))),\n","        \"precision\": (float(np.mean(prec_scores)), float(np.std(prec_scores))),\n","        \"recall\":    (float(np.mean(rec_scores)), float(np.std(rec_scores))),\n","        \"f1\":        (float(np.mean(f1_scores)),  float(np.std(f1_scores))),\n","        \"log_loss\":  (float(np.mean(log_losses)), float(np.std(log_losses))),\n","    }\n","    all_results.append(lambda_results)\n","\n","    print(f\"\\n--- RESULTS FOR LAMBDA = {lam} ---\")\n","    print(f\"Accuracy : {lambda_results['accuracy'][0]:.4f} ± {lambda_results['accuracy'][1]:.4f}\")\n","    print(f\"Precision: {lambda_results['precision'][0]:.4f} ± {lambda_results['precision'][1]:.4f}\")\n","    print(f\"Recall   : {lambda_results['recall'][0]:.4f} ± {lambda_results['recall'][1]:.4f}\")\n","    print(f\"F1 Score : {lambda_results['f1'][0]:.4f} ± {lambda_results['f1'][1]:.4f}\")\n","    print(f\"Log Loss : {lambda_results['log_loss'][0]:.4f} ± {lambda_results['log_loss'][1]:.4f}\")\n","\n","print(\"\\n================ FINAL SUMMARY FOR ALL LAMBDAS ================\\n\")\n","print(f\"{'Lambda':>8} | {'Accuracy':>18} | {'Precision':>18} | {'Recall':>18} | {'F1 Score':>18} | {'Log Loss':>18}\")\n","print(\"-\" * 108)\n","for res in all_results:\n","    print(f\"{res['lambda']:>8} | \"\n","          f\"{res['accuracy'][0]:.4f} ± {res['accuracy'][1]:.4f} | \"\n","          f\"{res['precision'][0]:.4f} ± {res['precision'][1]:.4f} | \"\n","          f\"{res['recall'][0]:.4f} ± {res['recall'][1]:.4f} | \"\n","          f\"{res['f1'][0]:.4f} ± {res['f1'][1]:.4f} | \"\n","          f\"{res['log_loss'][0]:.4f} ± {res['log_loss'][1]:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YoZr8SupDBbh","outputId":"bf0c8e2c-ccdf-4256-dbcc-71538cba1dae"},"outputs":[{"name":"stdout","output_type":"stream","text":["Adjusted Rand Score: -0.001379769830930187\n","Normalized Mutual Information Score: 7.97612442553983e-05\n","Clustering Accuracy (mapped): 0.5333333333333333\n"]}],"source":["from sklearn.cluster import KMeans\n","from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n","from scipy.optimize import linear_sum_assignment\n","\n","def cluster_acc(y_true, y_pred):\n","    \"\"\"\n","    Calculate clustering accuracy. Assigns predicted clusters to true labels\n","    to maximize accuracy using the Jonker-Volgenant algorithm (linear_sum_assignment).\n","    \"\"\"\n","    y_true = y_true.astype(np.int64)\n","    assert y_pred.size == y_true.size\n","    D = max(y_pred.max(), y_true.max()) + 1\n","    w = np.zeros((D, D), dtype=np.int64)\n","    for i in range(y_pred.size):\n","        w[y_pred[i], y_true[i]] += 1\n","    row_ind, col_ind = linear_sum_assignment(-w)\n","    return w[row_ind, col_ind].sum() / y_pred.size\n","\n","\n","model.eval()\n","with torch.no_grad():\n","    embeddings = model.get_embeddings(node_feats.to(device), edge_index.to(device))\n","    embeddings = embeddings.cpu().numpy()\n","\n","kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n","kmeans.fit(embeddings)\n","y_pred_kmeans = kmeans.labels_\n","\n","ari_score = adjusted_rand_score(y, y_pred_kmeans)\n","nmi_score = normalized_mutual_info_score(y, y_pred_kmeans)\n","\n","print(\"Adjusted Rand Score:\", ari_score)\n","print(\"Normalized Mutual Information Score:\", nmi_score)\n","\n","acc_kmeans = cluster_acc(y, y_pred_kmeans)\n","print(\"Clustering Accuracy (mapped):\", acc_kmeans)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"adGSi9JiEEqO"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wPfsaXB6_EZp"},"outputs":[],"source":["# class DGI(nn.Module):\n","#     def __init__(self, input_dim, hidden_dim,output_dim, cut=0):\n","#         super().__init__()\n","#         self.encoder = GCNEncoder(input_dim, hidden_dim)\n","#         self.readout = nn.Linear(hidden_dim, output_dim)\n","#         self.cut = cut\n","#         self.output_dim = output_dim\n","\n","#     def forward(self, x, edge_index, corrupt_x, adj=None):\n","#         h = self.encoder(x, edge_index)\n","#         h_corrupt = self.encoder(corrupt_x, edge_index)\n","\n","#         # Summary vector\n","#         s = torch.sigmoid(h.mean(dim=0))\n","\n","#         # Positive & negative scores\n","#         pos = torch.matmul(h, s)\n","#         neg = torch.matmul(h_corrupt, s)\n","\n","#         # DGI loss\n","#         dgi_loss = -torch.log(torch.sigmoid(pos - neg) + 1e-8).mean()\n","\n","#         reg_loss = 0\n","#         if adj is not None:\n","#             A = torch.as_tensor(adj, dtype=torch.float32, device=x.device)\n","#             D = torch.diag(A.sum(dim=1))\n","\n","#             if self.cut == 1:  # Cut loss\n","#                 L = D - A\n","#                 p = self.readout(h)\n","#                 C = F.softmax(p, dim=1)\n","#                 reg_loss = torch.trace(C.T @ L @ C) / (torch.trace(C.T @ D @ C) + 1e-8)\n","\n","#             else:  # Modularity loss\n","#                 m = torch.sum(A)\n","#                 B = A - torch.outer(D.diag(), D.diag()) / (2 * m)\n","#                 p = self.readout(h)\n","#                 C = F.softmax(p, dim=1)\n","#                 k = torch.tensor(self.output_dim, dtype=torch.float32, device=x.device)\n","#                 n = C.shape[0]\n","#                 reg_loss = (-1 / (2 * m)) * torch.trace(torch.mm(torch.mm(C.t(), B), C))\n","#                 reg_loss += (torch.sqrt(k) / n) * torch.norm(torch.sum(C, dim=0), p='fro') - 1\n","\n","\n","#         return h, dgi_loss, reg_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xzy_llWw_UGN","outputId":"db4fe94b-e693-45af-a7b5-2f4b9bf7342c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0 | DGI Loss: 0.6922 | Reg Loss: -0.1250 | Total Loss: 0.5672\n","Epoch 500 | DGI Loss: 0.2924 | Reg Loss: -0.1250 | Total Loss: 0.1674\n","Epoch 1000 | DGI Loss: 0.2514 | Reg Loss: -0.1250 | Total Loss: 0.1264\n","Epoch 1500 | DGI Loss: 0.2425 | Reg Loss: -0.1250 | Total Loss: 0.1175\n","Epoch 2000 | DGI Loss: 0.2083 | Reg Loss: -0.1250 | Total Loss: 0.0833\n","Epoch 2500 | DGI Loss: 0.2106 | Reg Loss: -0.1250 | Total Loss: 0.0856\n","Epoch 3000 | DGI Loss: 0.1714 | Reg Loss: -0.1246 | Total Loss: 0.0468\n","Epoch 3500 | DGI Loss: 0.1480 | Reg Loss: -0.1256 | Total Loss: 0.0224\n","Epoch 4000 | DGI Loss: 0.1510 | Reg Loss: -0.1278 | Total Loss: 0.0232\n","Epoch 4500 | DGI Loss: 0.1389 | Reg Loss: -0.1328 | Total Loss: 0.0060\n","Epoch 5000 | DGI Loss: 0.1076 | Reg Loss: -0.1373 | Total Loss: -0.0297\n","Epoch 5500 | DGI Loss: 0.1220 | Reg Loss: -0.1399 | Total Loss: -0.0180\n","Epoch 6000 | DGI Loss: 0.0915 | Reg Loss: -0.1414 | Total Loss: -0.0499\n","Epoch 6500 | DGI Loss: 0.0746 | Reg Loss: -0.1428 | Total Loss: -0.0681\n","Epoch 7000 | DGI Loss: 0.0863 | Reg Loss: -0.1426 | Total Loss: -0.0563\n"]}],"source":["# hidden_dim = 256\n","# output_dim = 2\n","# cut = 0\n","# model = DGI(features.shape[1], hidden_dim, output_dim, cut=cut).to(device)\n","# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","# num_epochs = 7000\n","# for epoch in range(num_epochs+1):\n","#     model.train()\n","#     optimizer.zero_grad()\n","\n","#     perm = torch.randperm(features.shape[0])\n","#     corrupt_features = node_feats[perm]\n","\n","#     _, dgi_loss, reg_loss = model(\n","#         node_feats.to(device),\n","#         edge_index.to(device),\n","#         corrupt_features.to(device),\n","#         adj=torch.tensor(W0).to(device)\n","#     )\n","\n","#     loss = dgi_loss + reg_loss\n","#     loss.backward()\n","#     optimizer.step()\n","\n","#     if epoch % 500 == 0:\n","#         print(f\"Epoch {epoch} | DGI Loss: {dgi_loss.item():.4f} | Reg Loss: {reg_loss.item():.4f} | Total Loss: {loss.item():.4f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c3sTP63kBAhr"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wSW7Zhsy_cg6"},"outputs":[],"source":["# model.eval()\n","# with torch.no_grad():\n","#     embeddings, _, _ = model(\n","#         node_feats.to(device),\n","#         edge_index.to(device),\n","#         node_feats.to(device),\n","#         adj=torch.tensor(W0).to(device)\n","#     )\n","\n","# embeddings = embeddings.cpu().numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qCN-YL-LXrE5","outputId":"29244971-8ffd-42c0-90df-34d6ed34164a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Adjusted Rand Score: 0.00017775231989074027\n","Normalized Mutual Information Score: 0.00035768106511865666\n","Clustering Accuracy (mapped): 0.54\n"]}],"source":["# from sklearn.cluster import KMeans\n","# from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n","\n","# # Use KMeans clustering\n","# kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)\n","# kmeans.fit(embeddings)\n","# y_pred_kmeans = kmeans.labels_\n","\n","# # Evaluate clustering performance\n","# ari_score = adjusted_rand_score(y, y_pred_kmeans)\n","# nmi_score = normalized_mutual_info_score(y, y_pred_kmeans)\n","\n","# print(\"Adjusted Rand Score:\", ari_score)\n","# print(\"Normalized Mutual Information Score:\", nmi_score)\n","\n","# # Note: K-Means is an unsupervised algorithm, so traditional classification metrics like accuracy, precision, recall, and F1 are not directly applicable without mapping clusters to classes.\n","# # However, we can calculate accuracy by mapping the cluster labels to the true labels in the way that maximizes accuracy.\n","# # This is not a standard evaluation for clustering but can give an idea of how well the clusters separate the classes.\n","# from scipy.optimize import linear_sum_assignment\n","# def cluster_acc(y_true, y_pred):\n","#     \"\"\"\n","#     Calculate clustering accuracy. Assigns predicted clusters to true labels\n","#     to maximize accuracy using the Jonker-Volgenant algorithm (linear_sum_assignment).\n","#     \"\"\"\n","#     y_true = y_true.astype(np.int64)\n","#     assert y_pred.size == y_true.size\n","#     D = max(y_pred.max(), y_true.max()) + 1\n","#     w = np.zeros((D, D), dtype=np.int64)\n","#     for i in range(y_pred.size):\n","#         w[y_pred[i], y_true[i]] += 1\n","#     row_ind, col_ind = linear_sum_assignment(-w)\n","#     return w[row_ind, col_ind].sum() / y_pred.size\n","\n","# acc_kmeans = cluster_acc(y, y_pred_kmeans)\n","# print(\"Clustering Accuracy (mapped):\", acc_kmeans)"]},{"cell_type":"markdown","metadata":{"id":"WL8xpgMME8sS"},"source":["1- GCN"]},{"cell_type":"markdown","metadata":{"id":"rfZdJWjqBSU5"},"source":["Accuracy: 0.7466666666666667\n","Precision: 0.7263681592039801\n","Recall: 0.874251497005988\n","F1: 0.7934782608695652\n","Log Loss: 0.5786983582841999"]},{"cell_type":"markdown","metadata":{"id":"yWmKFw9cE77L"},"source":["Accuracy: 0.7533333333333333\n","Precision: 0.7360406091370558\n","Recall: 0.8682634730538922\n","F1: 0.7967032967032966\n","Log Loss: 0.5772490248961657"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.18"}},"nbformat":4,"nbformat_minor":0}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP82/m8zlznWdEiEEdt3P9z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import torch.optim as optim\n","from tqdm import tqdm\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as nnFn\n","from torch_geometric.data import Data\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss\n","import random\n","from torch_geometric.nn import GATConv"],"metadata":{"id":"HjHExb6bb9my","executionInfo":{"status":"ok","timestamp":1763212876664,"user_tz":-330,"elapsed":1928,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"731175a9-0776-4527-e76d-ff383af45e26"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["/home/snu/anaconda3/envs/torch_env/lib/python3.10/site-packages/torch_geometric/typing.py:68: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /home/snu/anaconda3/envs/torch_env/lib/python3.10/site-packages/libpyg.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb\n","  warnings.warn(f\"An issue occurred while importing 'pyg-lib'. \"\n","/home/snu/anaconda3/envs/torch_env/lib/python3.10/site-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /home/snu/anaconda3/envs/torch_env/lib/python3.10/site-packages/torch_scatter/_version_cuda.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb\n","  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n","/home/snu/anaconda3/envs/torch_env/lib/python3.10/site-packages/torch_geometric/typing.py:97: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: /home/snu/anaconda3/envs/torch_env/lib/python3.10/site-packages/torch_cluster/_version_cuda.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb\n","  warnings.warn(f\"An issue occurred while importing 'torch-cluster'. \"\n","/home/snu/anaconda3/envs/torch_env/lib/python3.10/site-packages/torch_geometric/typing.py:113: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: /home/snu/anaconda3/envs/torch_env/lib/python3.10/site-packages/torch_spline_conv/_version_cuda.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb\n","  warnings.warn(\n","/home/snu/anaconda3/envs/torch_env/lib/python3.10/site-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /home/snu/anaconda3/envs/torch_env/lib/python3.10/site-packages/torch_sparse/_version_cuda.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb\n","  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n","/home/snu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}]},{"cell_type":"code","source":["fa_feature_path = \"/home/snu/Downloads/Histogram_CN_FA_20bin_updated.npy\"\n","Histogram_feature_CN_FA_array = np.load(fa_feature_path, allow_pickle=True)\n","\n","# Load MCI features\n","fa_feature_path = \"/home/snu/Downloads/Histogram_MCI_FA_20bin_updated.npy\"\n","Histogram_feature_MCI_FA_array = np.load(fa_feature_path, allow_pickle=True)\n","\n","# Combine features and labels\n","X = np.vstack([Histogram_feature_CN_FA_array, Histogram_feature_MCI_FA_array])\n","y = np.hstack([\n","    np.zeros(Histogram_feature_CN_FA_array.shape[0], dtype=np.int64),\n","    np.ones(Histogram_feature_MCI_FA_array.shape[0], dtype=np.int64)\n","])\n","\n","num_nodes, num_feats = X.shape\n","print(f\"Features: {X.shape}, Labels: {y.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"muCGb51gb_MI","executionInfo":{"status":"ok","timestamp":1763212876709,"user_tz":-330,"elapsed":35,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}},"outputId":"fd34d7ec-31c9-49b5-e97a-3821ea509f0a"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Features: (300, 180), Labels: (300,)\n"]}]},{"cell_type":"code","source":["class MLP(nn.Module):\n","    def __init__(self, inp_size, outp_size, hidden_size):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(inp_size, hidden_size),\n","            nn.BatchNorm1d(hidden_size),\n","            nn.PReLU(),\n","            nn.Dropout(0.3),\n","            nn.Linear(hidden_size, outp_size)\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)"],"metadata":{"id":"0uj7oEjqcB2Y","executionInfo":{"status":"ok","timestamp":1763212877807,"user_tz":-330,"elapsed":7,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["class GATEncoder(torch.nn.Module):\n","    def __init__(self, input_dim, hidden_dim, device, activ, heads=4):\n","        super(GATEncoder, self).__init__()\n","        self.device = device\n","        self.gat1 = GATConv(input_dim, hidden_dim // heads, heads=heads)\n","        self.batchnorm = nn.BatchNorm1d(hidden_dim)\n","        self.dropout = nn.Dropout(0.3)\n","        self.mlp = nn.Linear(hidden_dim, hidden_dim)\n","\n","    def forward(self, data):\n","        x, edge_index = data.x, data.edge_index\n","        x = self.gat1(x, edge_index)\n","        x = self.dropout(x)\n","        x = self.batchnorm(x)\n","        logits = self.mlp(x)\n","        return logits"],"metadata":{"id":"3jqpIprJcovi","executionInfo":{"status":"ok","timestamp":1763212879023,"user_tz":-330,"elapsed":7,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class GAT(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_clusters, device, activ):\n","        super(GAT, self).__init__()\n","        self.device = device\n","        self.num_clusters = num_clusters\n","        self.cut = 0   # always modularity\n","\n","        self.online_encoder = GATEncoder(input_dim, hidden_dim, device, activ)\n","\n","        activations = {\n","            \"SELU\": nnFn.selu,\n","            \"SiLU\": nnFn.silu,\n","            \"GELU\": nnFn.gelu,\n","            \"RELU\": nnFn.relu\n","        }\n","        self.act = activations.get(activ, nnFn.elu)\n","\n","        self.online_predictor = MLP(hidden_dim, num_clusters, hidden_dim)\n","\n","        # only modularity loss\n","        self.loss = self.modularity_loss\n","\n","    def forward(self, data):\n","        x = self.online_encoder(data)\n","        logits = self.online_predictor(x)\n","        S = nnFn.softmax(logits, dim=1)\n","\n","        return S, logits\n","\n","    def modularity_loss(self, A, S):\n","        C = nnFn.softmax(S, dim=1)\n","        d = torch.sum(A, dim=1)\n","        m = torch.sum(A)\n","        B = A - torch.ger(d, d) / (2 * m)\n","\n","        I_S = torch.eye(self.num_clusters, device=self.device)\n","        k = torch.norm(I_S)\n","        n = S.shape[0]\n","\n","        modularity_term = (-1 / (2 * m)) * torch.trace(torch.mm(torch.mm(C.t(), B), C))\n","        collapse_reg_term = (torch.sqrt(k) / n) * torch.norm(torch.sum(C, dim=0), p='fro') - 1\n","\n","        return modularity_term + collapse_reg_term"],"metadata":{"id":"zL6vx-Oncr-K","executionInfo":{"status":"ok","timestamp":1763212881429,"user_tz":-330,"elapsed":3,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def create_adj(F, cut, alpha=1):\n","    F_norm = F / np.linalg.norm(F, axis=1, keepdims=True)\n","    W = np.dot(F_norm, F_norm.T)\n","    if cut == 0:\n","        W = np.where(W >= alpha, 1, 0).astype(np.float32)\n","        W = (W / W.max()).astype(np.float32)\n","    else:\n","        W = W - (W.max() / alpha)\n","    return W\n","\n","def load_data(adj, node_feats):\n","    node_feats = torch.from_numpy(node_feats)\n","    edge_index = torch.from_numpy(np.array(np.nonzero((adj > 0))))\n","    row, col = edge_index\n","    edge_weight = torch.from_numpy(adj[row, col])\n","    return node_feats, edge_index, edge_weight"],"metadata":{"id":"SDBPp49wcvFh","executionInfo":{"status":"ok","timestamp":1763212884271,"user_tz":-330,"elapsed":6,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["features = X.astype(np.float32)\n","print(features.shape, features.dtype)\n","\n","cut = 0\n","alpha = 0.92\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","feats_dim = 180\n","K = 2\n","\n","W0 = create_adj(features, 0, alpha)\n","node_feats, edge_index, _ = load_data(W0, features)\n","data0 = Data(x=node_feats, edge_index=edge_index).to(device)\n","A1 = torch.from_numpy(W0).float().to(device)\n","print(data0)"],"metadata":{"id":"ze8ee3-Mu4_N","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763212887795,"user_tz":-330,"elapsed":153,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}},"outputId":"b9b22f57-646a-4c35-ad90-e0682b139b41"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["(300, 180) float32\n","Data(x=[300, 180], edge_index=[2, 13604])\n"]}]},{"cell_type":"code","source":["from torch.optim.lr_scheduler import StepLR\n","from torch.optim import AdamW\n","\n","model = GAT(feats_dim, 256, K, device, \"ELU\").to(device)\n","optimizer = AdamW(model.parameters(), lr=0.0001, weight_decay=0.0001)\n","scheduler = StepLR(optimizer, step_size=200, gamma=0.5)\n","\n","num_epochs = 5000\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    optimizer.zero_grad()\n","\n","    S, logits = model(data0)\n","    unsup_loss = model.loss(A1, logits)\n","\n","    total_loss = unsup_loss\n","    total_loss.backward()\n","    optimizer.step()\n","    scheduler.step()\n","\n","    if epoch % 100 == 0:\n","        print(f\"Epoch {epoch} | Loss: {total_loss:.4f}\")"],"metadata":{"id":"Kq-UdCTxu5bc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763212935239,"user_tz":-330,"elapsed":45264,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}},"outputId":"acec7305-4eb0-4ad3-9e05-0f244104f1d5"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0 | Loss: -0.2835\n","Epoch 100 | Loss: -0.4476\n","Epoch 200 | Loss: -0.4513\n","Epoch 300 | Loss: -0.4523\n","Epoch 400 | Loss: -0.4528\n","Epoch 500 | Loss: -0.4530\n","Epoch 600 | Loss: -0.4533\n","Epoch 700 | Loss: -0.4536\n","Epoch 800 | Loss: -0.4536\n","Epoch 900 | Loss: -0.4535\n","Epoch 1000 | Loss: -0.4537\n","Epoch 1100 | Loss: -0.4536\n","Epoch 1200 | Loss: -0.4537\n","Epoch 1300 | Loss: -0.4537\n","Epoch 1400 | Loss: -0.4537\n","Epoch 1500 | Loss: -0.4538\n","Epoch 1600 | Loss: -0.4537\n","Epoch 1700 | Loss: -0.4538\n","Epoch 1800 | Loss: -0.4538\n","Epoch 1900 | Loss: -0.4538\n","Epoch 2000 | Loss: -0.4538\n","Epoch 2100 | Loss: -0.4540\n","Epoch 2200 | Loss: -0.4538\n","Epoch 2300 | Loss: -0.4538\n","Epoch 2400 | Loss: -0.4539\n","Epoch 2500 | Loss: -0.4541\n","Epoch 2600 | Loss: -0.4538\n","Epoch 2700 | Loss: -0.4539\n","Epoch 2800 | Loss: -0.4539\n","Epoch 2900 | Loss: -0.4537\n","Epoch 3000 | Loss: -0.4537\n","Epoch 3100 | Loss: -0.4540\n","Epoch 3200 | Loss: -0.4537\n","Epoch 3300 | Loss: -0.4538\n","Epoch 3400 | Loss: -0.4538\n","Epoch 3500 | Loss: -0.4537\n","Epoch 3600 | Loss: -0.4539\n","Epoch 3700 | Loss: -0.4538\n","Epoch 3800 | Loss: -0.4537\n","Epoch 3900 | Loss: -0.4537\n","Epoch 4000 | Loss: -0.4535\n","Epoch 4100 | Loss: -0.4539\n","Epoch 4200 | Loss: -0.4538\n","Epoch 4300 | Loss: -0.4538\n","Epoch 4400 | Loss: -0.4539\n","Epoch 4500 | Loss: -0.4538\n","Epoch 4600 | Loss: -0.4539\n","Epoch 4700 | Loss: -0.4538\n","Epoch 4800 | Loss: -0.4536\n","Epoch 4900 | Loss: -0.4540\n"]}]},{"cell_type":"code","source":["model.eval()\n","with torch.no_grad():\n","    S, logits = model(data0)\n","    y_pred = torch.argmax(logits, dim=1).cpu().numpy()\n","    y_pred_proba = nnFn.softmax(logits, dim=1).cpu().numpy()\n","\n","\n","acc_score = accuracy_score(y, y_pred)\n","acc_score_inverted = accuracy_score(y, 1 - y_pred)\n","\n","if acc_score_inverted > acc_score:\n","    acc_score = acc_score_inverted\n","    y_pred = 1 - y_pred\n","\n","prec_score = precision_score(y, y_pred)\n","rec_score = recall_score(y, y_pred)\n","f1 = f1_score(y, y_pred)\n","log_loss_value = log_loss(y, y_pred_proba)\n","\n","print(\"Accuracy:\", acc_score)\n","print(\"Precision:\", prec_score)\n","print(\"Recall:\", rec_score)\n","print(\"F1:\", f1)\n","print(\"Log Loss:\", log_loss_value)"],"metadata":{"id":"v9Ck7wvKvEr0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763212956273,"user_tz":-330,"elapsed":24,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}},"outputId":"dddccf72-d8db-4405-9923-9382126b2236"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.7566666666666667\n","Precision: 0.8133333333333334\n","Recall: 0.7305389221556886\n","F1: 0.7697160883280757\n","Log Loss: 7.075821996600855\n"]}]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5S5tfbniZ6sj","executionInfo":{"status":"ok","timestamp":1763213332772,"user_tz":-330,"elapsed":372235,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}},"outputId":"52c0ccae-7d86-47e6-ca50-41dd6f522444"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","================ Run 0 ================\n","Epoch 0 | Loss: -0.2840\n","Epoch 1000 | Loss: -0.4535\n","Epoch 2000 | Loss: -0.4535\n","Epoch 3000 | Loss: -0.4536\n","Epoch 4000 | Loss: -0.4536\n","Accuracy: 0.7566666666666667 Precision: 0.8133333333333334 Recall: 0.7305389221556886 F1: 0.7697160883280757\n","\n","================ Run 1 ================\n","Epoch 0 | Loss: -0.2836\n","Epoch 1000 | Loss: -0.4538\n","Epoch 2000 | Loss: -0.4539\n","Epoch 3000 | Loss: -0.4537\n","Epoch 4000 | Loss: -0.4539\n","Accuracy: 0.7533333333333333 Precision: 0.8079470198675497 Recall: 0.7305389221556886 F1: 0.7672955974842768\n","\n","================ Run 2 ================\n","Epoch 0 | Loss: -0.2835\n","Epoch 1000 | Loss: -0.4536\n","Epoch 2000 | Loss: -0.4538\n","Epoch 3000 | Loss: -0.4536\n","Epoch 4000 | Loss: -0.4537\n","Accuracy: 0.7633333333333333 Precision: 0.8243243243243243 Recall: 0.7305389221556886 F1: 0.7746031746031746\n","\n","================ Run 3 ================\n","Epoch 0 | Loss: -0.2835\n","Epoch 1000 | Loss: -0.4535\n","Epoch 2000 | Loss: -0.4538\n","Epoch 3000 | Loss: -0.4536\n","Epoch 4000 | Loss: -0.4536\n","Accuracy: 0.7533333333333333 Precision: 0.8079470198675497 Recall: 0.7305389221556886 F1: 0.7672955974842768\n","\n","================ Run 4 ================\n","Epoch 0 | Loss: -0.2817\n","Epoch 1000 | Loss: -0.4536\n","Epoch 2000 | Loss: -0.4535\n","Epoch 3000 | Loss: -0.4537\n","Epoch 4000 | Loss: -0.4538\n","Accuracy: 0.7566666666666667 Precision: 0.8133333333333334 Recall: 0.7305389221556886 F1: 0.7697160883280757\n","\n","================ Run 5 ================\n","Epoch 0 | Loss: -0.2867\n","Epoch 1000 | Loss: -0.4536\n","Epoch 2000 | Loss: -0.4538\n","Epoch 3000 | Loss: -0.4539\n","Epoch 4000 | Loss: -0.4538\n","Accuracy: 0.76 Precision: 0.8187919463087249 Recall: 0.7305389221556886 F1: 0.7721518987341772\n","\n","================ Run 6 ================\n","Epoch 0 | Loss: -0.2846\n","Epoch 1000 | Loss: -0.4534\n","Epoch 2000 | Loss: -0.4538\n","Epoch 3000 | Loss: -0.4539\n","Epoch 4000 | Loss: -0.4536\n","Accuracy: 0.77 Precision: 0.8266666666666667 Recall: 0.7425149700598802 F1: 0.7823343848580442\n","\n","================ Run 7 ================\n","Epoch 0 | Loss: -0.2848\n","Epoch 1000 | Loss: -0.4536\n","Epoch 2000 | Loss: -0.4539\n","Epoch 3000 | Loss: -0.4537\n","Epoch 4000 | Loss: -0.4540\n","Accuracy: 0.7633333333333333 Precision: 0.8243243243243243 Recall: 0.7305389221556886 F1: 0.7746031746031746\n","\n","================ Run 8 ================\n","Epoch 0 | Loss: -0.2782\n","Epoch 1000 | Loss: -0.4534\n","Epoch 2000 | Loss: -0.4539\n","Epoch 3000 | Loss: -0.4536\n","Epoch 4000 | Loss: -0.4537\n","Accuracy: 0.7466666666666667 Precision: 0.7973856209150327 Recall: 0.7305389221556886 F1: 0.7625\n","\n","================ Run 9 ================\n","Epoch 0 | Loss: -0.2817\n","Epoch 1000 | Loss: -0.4533\n","Epoch 2000 | Loss: -0.4538\n","Epoch 3000 | Loss: -0.4539\n","Epoch 4000 | Loss: -0.4538\n","Accuracy: 0.7433333333333333 Precision: 0.7960526315789473 Recall: 0.7245508982035929 F1: 0.7586206896551724\n","\n","===== Final Results across 10 runs =====\n","Accuracy: mean= 0.7566666666666666 std= 0.0076011695006609255\n","Precision: mean= 0.8130106220519785 std= 0.010291324033447145\n","Recall: mean= 0.7311377245508982 std= 0.004191616766467051\n","F1: mean= 0.7698836694078448 std= 0.006320904399762162\n"]}],"source":["from torch.optim.lr_scheduler import StepLR\n","from torch.optim import AdamW\n","\n","results = []\n","\n","for run_seed in range(10):\n","    print(\"\\n================ Run\", run_seed, \"================\")\n","\n","    # Set seeds for reproducibility\n","    np.random.seed(run_seed)\n","    torch.manual_seed(run_seed)\n","    random.seed(run_seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(run_seed)\n","\n","    # Shuffle features and labels\n","    perm = np.random.permutation(X.shape[0])\n","    features = X[perm].astype(np.float32)\n","    labels = y[perm]\n","\n","    cut = 0\n","    alpha = 0.92\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    feats_dim = 180\n","    K = 2\n","\n","    W0 = create_adj(features, cut, alpha)\n","    node_feats, edge_index, _ = load_data(W0, features)\n","    data0 = Data(x=node_feats, edge_index=edge_index).to(device)\n","    A1 = torch.from_numpy(W0).float().to(device)\n","\n","    model = GAT(feats_dim, 256, K, device, \"ELU\").to(device)\n","    optimizer = AdamW(model.parameters(), lr=0.0001, weight_decay=0.0001)\n","    scheduler = StepLR(optimizer, step_size=200, gamma=0.5)\n","\n","    num_epochs = 5000\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        optimizer.zero_grad()\n","\n","\n","        S, logits = model(data0)\n","        unsup_loss = model.loss(A1, logits)\n","\n","        unsup_loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","\n","        if epoch % 1000 == 0:\n","            print(f\"Epoch {epoch} | Loss: {unsup_loss:.4f}\")\n","\n","    model.eval()\n","    with torch.no_grad():\n","        S, logits = model(data0)\n","        y_pred = torch.argmax(logits, dim=1).cpu().numpy()\n","        y_pred_proba = nnFn.softmax(logits, dim=1).cpu().numpy()\n","\n","    acc_score = accuracy_score(labels, y_pred)\n","    acc_score_inverted = accuracy_score(labels, 1 - y_pred)\n","\n","    if acc_score_inverted > acc_score:\n","        acc_score = acc_score_inverted\n","        y_pred = 1 - y_pred\n","\n","    prec_score = precision_score(labels, y_pred)\n","    rec_score = recall_score(labels, y_pred)\n","    f1 = f1_score(labels, y_pred)\n","    log_loss_value = log_loss(labels, y_pred_proba)\n","\n","    print(\"Accuracy:\", acc_score, \"Precision:\", prec_score, \"Recall:\", rec_score, \"F1:\", f1)\n","\n","    results.append({\n","        \"seed\": run_seed,\n","        \"accuracy\": acc_score,\n","        \"precision\": prec_score,\n","        \"recall\": rec_score,\n","        \"f1\": f1,\n","        \"log_loss\": log_loss_value\n","    })\n","\n","accs = [r[\"accuracy\"] for r in results]\n","precisions = [r[\"precision\"] for r in results]\n","recalls = [r[\"recall\"] for r in results]\n","f1s = [r[\"f1\"] for r in results]\n","\n","print(\"\\n===== Final Results across 10 runs =====\")\n","print(\"Accuracy: mean=\", np.mean(accs), \"std=\", np.std(accs))\n","print(\"Precision: mean=\", np.mean(precisions), \"std=\", np.std(precisions))\n","print(\"Recall: mean=\", np.mean(recalls), \"std=\", np.std(recalls))\n","print(\"F1: mean=\", np.mean(f1s), \"std=\", np.std(f1s))"]},{"cell_type":"code","source":["from torch.optim.lr_scheduler import StepLR\n","from torch.optim import AdamW\n","\n","results = []\n","\n","for run_seed in range(10):\n","    print(\"\\n================ Run\", run_seed, \"================\")\n","\n","    # Set seeds for reproducibility\n","    np.random.seed(run_seed)\n","    torch.manual_seed(run_seed)\n","    random.seed(run_seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(run_seed)\n","\n","    # Shuffle features and labels\n","    perm = np.random.permutation(X.shape[0])\n","    features = X[perm].astype(np.float32)\n","    labels = y[perm]\n","\n","    cut = 0\n","    alpha = 0.92\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","    feats_dim = 180\n","    K = 2\n","\n","    W0 = create_adj(features, cut, alpha)\n","    node_feats, edge_index, _ = load_data(W0, features)\n","    data0 = Data(x=node_feats, edge_index=edge_index).to(device)\n","    A1 = torch.from_numpy(W0).float().to(device)\n","\n","    model = GAT(feats_dim, 256, K, device, \"SELU\").to(device)\n","    optimizer = AdamW(model.parameters(), lr=0.0001, weight_decay=0.0001)\n","    scheduler = StepLR(optimizer, step_size=200, gamma=0.5)\n","\n","    num_epochs = 5000\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        optimizer.zero_grad()\n","\n","\n","        S, logits = model(data0)\n","        unsup_loss = model.loss(A1, logits)\n","\n","        unsup_loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","\n","        if epoch % 1000 == 0:\n","            print(f\"Epoch {epoch} | Loss: {unsup_loss:.4f}\")\n","\n","    model.eval()\n","    with torch.no_grad():\n","        S, logits = model(data0)\n","        y_pred = torch.argmax(logits, dim=1).cpu().numpy()\n","        y_pred_proba = nnFn.softmax(logits, dim=1).cpu().numpy()\n","\n","    acc_score = accuracy_score(labels, y_pred)\n","    acc_score_inverted = accuracy_score(labels, 1 - y_pred)\n","\n","    if acc_score_inverted > acc_score:\n","        acc_score = acc_score_inverted\n","        y_pred = 1 - y_pred\n","\n","    prec_score = precision_score(labels, y_pred)\n","    rec_score = recall_score(labels, y_pred)\n","    f1 = f1_score(labels, y_pred)\n","    log_loss_value = log_loss(labels, y_pred_proba)\n","\n","    print(\"Accuracy:\", acc_score, \"Precision:\", prec_score, \"Recall:\", rec_score, \"F1:\", f1)\n","\n","    results.append({\n","        \"seed\": run_seed,\n","        \"accuracy\": acc_score,\n","        \"precision\": prec_score,\n","        \"recall\": rec_score,\n","        \"f1\": f1,\n","        \"log_loss\": log_loss_value\n","    })\n","\n","accs = [r[\"accuracy\"] for r in results]\n","precisions = [r[\"precision\"] for r in results]\n","recalls = [r[\"recall\"] for r in results]\n","f1s = [r[\"f1\"] for r in results]\n","\n","print(\"\\n===== Final Results across 10 runs =====\")\n","print(\"Accuracy: mean=\", np.mean(accs), \"std=\", np.std(accs))\n","print(\"Precision: mean=\", np.mean(precisions), \"std=\", np.std(precisions))\n","print(\"Recall: mean=\", np.mean(recalls), \"std=\", np.std(recalls))\n","print(\"F1: mean=\", np.mean(f1s), \"std=\", np.std(f1s))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2vtcE8X_tPy1","executionInfo":{"status":"ok","timestamp":1763213679332,"user_tz":-330,"elapsed":295508,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}},"outputId":"bc7eb2bd-f2b9-42fd-ef3d-6b36ead9f75b"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","================ Run 0 ================\n","Epoch 0 | Loss: -0.2840\n","Epoch 1000 | Loss: -0.4535\n","Epoch 2000 | Loss: -0.4535\n","Epoch 3000 | Loss: -0.4536\n","Epoch 4000 | Loss: -0.4536\n","Accuracy: 0.7566666666666667 Precision: 0.8133333333333334 Recall: 0.7305389221556886 F1: 0.7697160883280757\n","\n","================ Run 1 ================\n","Epoch 0 | Loss: -0.2836\n","Epoch 1000 | Loss: -0.4538\n","Epoch 2000 | Loss: -0.4539\n","Epoch 3000 | Loss: -0.4537\n","Epoch 4000 | Loss: -0.4539\n","Accuracy: 0.7533333333333333 Precision: 0.8079470198675497 Recall: 0.7305389221556886 F1: 0.7672955974842768\n","\n","================ Run 2 ================\n","Epoch 0 | Loss: -0.2835\n","Epoch 1000 | Loss: -0.4536\n","Epoch 2000 | Loss: -0.4538\n","Epoch 3000 | Loss: -0.4536\n","Epoch 4000 | Loss: -0.4537\n","Accuracy: 0.7633333333333333 Precision: 0.8243243243243243 Recall: 0.7305389221556886 F1: 0.7746031746031746\n","\n","================ Run 3 ================\n","Epoch 0 | Loss: -0.2835\n","Epoch 1000 | Loss: -0.4535\n","Epoch 2000 | Loss: -0.4538\n","Epoch 3000 | Loss: -0.4536\n","Epoch 4000 | Loss: -0.4536\n","Accuracy: 0.7533333333333333 Precision: 0.8079470198675497 Recall: 0.7305389221556886 F1: 0.7672955974842768\n","\n","================ Run 4 ================\n","Epoch 0 | Loss: -0.2817\n","Epoch 1000 | Loss: -0.4536\n","Epoch 2000 | Loss: -0.4535\n","Epoch 3000 | Loss: -0.4537\n","Epoch 4000 | Loss: -0.4538\n","Accuracy: 0.7566666666666667 Precision: 0.8133333333333334 Recall: 0.7305389221556886 F1: 0.7697160883280757\n","\n","================ Run 5 ================\n","Epoch 0 | Loss: -0.2867\n","Epoch 1000 | Loss: -0.4536\n","Epoch 2000 | Loss: -0.4538\n","Epoch 3000 | Loss: -0.4539\n","Epoch 4000 | Loss: -0.4538\n","Accuracy: 0.76 Precision: 0.8187919463087249 Recall: 0.7305389221556886 F1: 0.7721518987341772\n","\n","================ Run 6 ================\n","Epoch 0 | Loss: -0.2846\n","Epoch 1000 | Loss: -0.4534\n","Epoch 2000 | Loss: -0.4538\n","Epoch 3000 | Loss: -0.4539\n","Epoch 4000 | Loss: -0.4536\n","Accuracy: 0.77 Precision: 0.8266666666666667 Recall: 0.7425149700598802 F1: 0.7823343848580442\n","\n","================ Run 7 ================\n","Epoch 0 | Loss: -0.2848\n","Epoch 1000 | Loss: -0.4536\n","Epoch 2000 | Loss: -0.4539\n","Epoch 3000 | Loss: -0.4537\n","Epoch 4000 | Loss: -0.4540\n","Accuracy: 0.7633333333333333 Precision: 0.8243243243243243 Recall: 0.7305389221556886 F1: 0.7746031746031746\n","\n","================ Run 8 ================\n","Epoch 0 | Loss: -0.2782\n","Epoch 1000 | Loss: -0.4534\n","Epoch 2000 | Loss: -0.4539\n","Epoch 3000 | Loss: -0.4536\n","Epoch 4000 | Loss: -0.4537\n","Accuracy: 0.7466666666666667 Precision: 0.7973856209150327 Recall: 0.7305389221556886 F1: 0.7625\n","\n","================ Run 9 ================\n","Epoch 0 | Loss: -0.2817\n","Epoch 1000 | Loss: -0.4533\n","Epoch 2000 | Loss: -0.4538\n","Epoch 3000 | Loss: -0.4539\n","Epoch 4000 | Loss: -0.4538\n","Accuracy: 0.7433333333333333 Precision: 0.7960526315789473 Recall: 0.7245508982035929 F1: 0.7586206896551724\n","\n","===== Final Results across 10 runs =====\n","Accuracy: mean= 0.7566666666666666 std= 0.0076011695006609255\n","Precision: mean= 0.8130106220519785 std= 0.010291324033447145\n","Recall: mean= 0.7311377245508982 std= 0.004191616766467051\n","F1: mean= 0.7698836694078448 std= 0.006320904399762162\n"]}]}]}
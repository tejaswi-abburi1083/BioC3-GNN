{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "torch_env",
      "display_name": "PyTorch (torch_env)"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.utils import to_undirected\n",
        "from torch_sparse import SparseTensor\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    f1_score,\n",
        "    normalized_mutual_info_score,\n",
        "    adjusted_rand_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    roc_auc_score\n",
        ")\n",
        "from sklearn.cluster import SpectralClustering\n",
        "from sklearn.cluster import KMeans"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mFDWzckYT23",
        "outputId": "0f6da3df-8901-4fd0-8eef-176b267bda39"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/snu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_seed(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "setup_seed(42)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "e6QpgFLdYVVQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fa_cn = np.load(\"/home/snu/Downloads/Histogram_CN_FA_20bin_updated.npy\", allow_pickle=True)\n",
        "fa_ad = np.load(\"/home/snu/Downloads/Histogram_AD_FA_20bin_updated.npy\", allow_pickle=True)\n",
        "\n",
        "X = np.vstack([fa_cn, fa_ad]).astype(np.float32)\n",
        "y = np.hstack([\n",
        "    np.zeros(len(fa_cn), dtype=np.int64),\n",
        "    np.ones(len(fa_ad), dtype=np.int64)\n",
        "])\n",
        "np.random.seed(42)\n",
        "perm = np.random.permutation(X.shape[0])\n",
        "X = X[perm]\n",
        "y = y[perm]\n",
        "N, F_dim = X.shape\n",
        "print(\"Nodes:\", N, \"Features:\", F_dim)\n",
        "\n",
        "x = torch.from_numpy(X).to(device)\n",
        "y_torch = torch.from_numpy(y).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmlepJGdYXmn",
        "outputId": "eae54512-54ff-40ff-aa1d-16d012bd22a6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nodes: 223 Features: 180\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_adj(features, alpha=0.8):\n",
        "    f = features / np.linalg.norm(features, axis=1, keepdims=True)\n",
        "    W = np.dot(f, f.T)\n",
        "    W = (W >= alpha).astype(np.float32)\n",
        "    return W\n",
        "\n",
        "W = create_adj(X, alpha=0.8)\n",
        "\n",
        "rows, cols = np.nonzero(W)\n",
        "edge_index = torch.tensor([rows, cols], dtype=torch.long)\n",
        "edge_index = to_undirected(edge_index).to(device)\n",
        "\n",
        "adj = SparseTensor(\n",
        "    row=edge_index[0],\n",
        "    col=edge_index[1],\n",
        "    sparse_sizes=(N, N)\n",
        ").fill_value(1.).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giTxlmtDYaB6",
        "outputId": "5d150a08-fdb9-4962-9ada-80c05357b717"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_734298/1251816577.py:10: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
            "  edge_index = torch.tensor([rows, cols], dtype=torch.long)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sim(batch, adj, wt=100, wl=2):     # wt - number of walks per node and wl - walk length\n",
        "    rowptr, col, _ = adj.csr()\n",
        "    batch_size = batch.shape[0]\n",
        "    batch_repeat = batch.repeat(wt)\n",
        "    rw = adj.random_walk(batch_repeat, wl)[:, 1:]\n",
        "    rw = rw.t().reshape(-1, batch_size).t()\n",
        "\n",
        "    row, col_, val = [], [], []\n",
        "    for i in range(batch.shape[0]):\n",
        "        rw_nodes, rw_times = torch.unique(rw[i], return_counts=True)\n",
        "        row += [batch[i].item()] * rw_nodes.shape[0]\n",
        "        col_ += rw_nodes.tolist()\n",
        "        val += rw_times.tolist()\n",
        "\n",
        "    adj_rw = SparseTensor(\n",
        "        row=torch.tensor(row),\n",
        "        col=torch.tensor(col_),\n",
        "        value=torch.tensor(val),\n",
        "        sparse_sizes=(batch.shape[0], batch.shape[0])\n",
        "    )\n",
        "    adj_rw = adj_rw.set_diag(0.)\n",
        "    return adj_rw"
      ],
      "metadata": {
        "id": "tSx2YQqaYdGR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mask(adj):   # get_mask() - Remove weak edges\n",
        "    mean = adj.mean(dim=1)\n",
        "    mask = (adj.storage.value() -\n",
        "            mean[adj.storage.row()]) > -1e-10\n",
        "    return SparseTensor(\n",
        "        row=adj.storage.row()[mask],\n",
        "        col=adj.storage.col()[mask],\n",
        "        value=adj.storage.value()[mask],\n",
        "        sparse_sizes=adj.sizes()\n",
        "    )\n",
        "\n",
        "def scale(z): # scale() - Normalise embeddings\n",
        "    zmin = z.min(dim=1, keepdim=True)[0]\n",
        "    zmax = z.max(dim=1, keepdim=True)[0]\n",
        "    return (z - zmin) / (zmax - zmin + 1e-12)"
      ],
      "metadata": {
        "id": "5hP3UiZwYfvH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MAGILoss(nn.Module):\n",
        "    def __init__(self, tau=0.3):\n",
        "        super().__init__()\n",
        "        self.tau = tau\n",
        "\n",
        "    def forward(self, z, mask):\n",
        "        dot = torch.mm(z, z.t()) / self.tau\n",
        "        dot = dot - dot.max(dim=1, keepdim=True)[0].detach()\n",
        "\n",
        "        logits_mask = torch.ones_like(dot) - torch.eye(z.size(0), device=z.device)\n",
        "        exp_logits = torch.exp(dot) * logits_mask\n",
        "        log_prob = dot - torch.log(exp_logits.sum(1, keepdim=True))\n",
        "\n",
        "        row, col = mask.storage.row(), mask.storage.col()\n",
        "        loss = -log_prob[row, col].mean()\n",
        "        return loss"
      ],
      "metadata": {
        "id": "KEcxwizpYiQJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        self.conv = GCNConv(in_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv(x, edge_index)\n",
        "        x = F.leaky_relu(x, 0.5)\n",
        "        return x"
      ],
      "metadata": {
        "id": "zGxRFFVfYmuv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_runs = 10\n",
        "epochs = 5000\n",
        "\n",
        "acc_list, prec_list, rec_list, f1_list, auc_list = [], [], [], [], []\n",
        "\n",
        "for run in range(n_runs):\n",
        "    print(f\"\\n========== Run {run+1}/{n_runs} ==========\")\n",
        "\n",
        "    setup_seed(42 + run)\n",
        "\n",
        "    encoder = Encoder(F_dim, 256).to(device)\n",
        "    criterion = MAGILoss(tau=0.3)\n",
        "    optimizer = torch.optim.Adam(\n",
        "        encoder.parameters(), lr=5e-4, weight_decay=1e-3\n",
        "    )\n",
        "\n",
        "    batch = torch.arange(N, device=device)\n",
        "    adj_rw = get_sim(batch, adj)\n",
        "    mask = get_mask(adj_rw)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        encoder.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        z = encoder(x, edge_index)\n",
        "        z = scale(z)\n",
        "        z = F.normalize(z, dim=1)\n",
        "\n",
        "        loss = criterion(z, mask)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 500 == 0:\n",
        "            print(f\"Run {run+1} | Epoch {epoch} | Loss {loss.item():.4f}\")\n",
        "\n",
        "\n",
        "    encoder.eval()\n",
        "    with torch.no_grad():\n",
        "        z = encoder(x, edge_index)\n",
        "        z = scale(z)\n",
        "        z = F.normalize(z, dim=1).cpu().numpy()\n",
        "\n",
        "    kmeans = KMeans(n_clusters=2, n_init=20, random_state=run)\n",
        "    y_pred = kmeans.fit_predict(z)\n",
        "\n",
        "    acc1 = accuracy_score(y, y_pred)\n",
        "    acc2 = accuracy_score(y, 1 - y_pred)\n",
        "    if acc2 > acc1:\n",
        "        y_pred = 1 - y_pred\n",
        "\n",
        "    acc  = accuracy_score(y, y_pred)\n",
        "    prec = precision_score(y, y_pred)\n",
        "    rec  = recall_score(y, y_pred)\n",
        "    f1   = f1_score(y, y_pred)\n",
        "\n",
        "\n",
        "    acc_list.append(acc)\n",
        "    prec_list.append(prec)\n",
        "    rec_list.append(rec)\n",
        "    f1_list.append(f1)\n",
        "\n",
        "\n",
        "    print(\n",
        "        f\"Run {run+1} → \"\n",
        "        f\"ACC: {acc:.4f}, \"\n",
        "        f\"PREC: {prec:.4f}, \"\n",
        "        f\"REC: {rec:.4f}, \"\n",
        "        f\"F1: {f1:.4f}, \"\n",
        "    )\n",
        "print(\"\\n===== MAGI + K-Means (10 Runs) =====\")\n",
        "print(f\"ACC : {np.mean(acc_list):.4f} \\u00b1 {np.std(acc_list):.4f}\")\n",
        "print(f\"PREC: {np.mean(prec_list):.4f} \\u00b1 {np.std(prec_list):.4f}\")\n",
        "print(f\"REC : {np.mean(rec_list):.4f} \\u00b1 {np.std(rec_list):.4f}\")\n",
        "print(f\"F1  : {np.mean(f1_list):.4f} \\u00b1 {np.std(f1_list):.4f}\")\n",
        "# print(f\"AUC : {np.mean(auc_list):.4f} \\u00b1 {np.std(auc_list):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azDtBverV6sy",
        "outputId": "562c1fec-acfb-4165-a1d4-0d55f3c689b7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== Run 1/10 ==========\n",
            "Run 1 | Epoch 0 | Loss 5.3992\n",
            "Run 1 | Epoch 500 | Loss 5.3613\n",
            "Run 1 | Epoch 1000 | Loss 5.3604\n",
            "Run 1 | Epoch 1500 | Loss 5.3604\n",
            "Run 1 | Epoch 2000 | Loss 5.3604\n",
            "Run 1 | Epoch 2500 | Loss 5.3602\n",
            "Run 1 | Epoch 3000 | Loss 5.3603\n",
            "Run 1 | Epoch 3500 | Loss 5.3625\n",
            "Run 1 | Epoch 4000 | Loss 5.3603\n",
            "Run 1 | Epoch 4500 | Loss 5.3605\n",
            "Run 1 → ACC: 0.6592, PREC: 0.8889, REC: 0.1778, F1: 0.2963, \n",
            "\n",
            "========== Run 2/10 ==========\n",
            "Run 2 | Epoch 0 | Loss 5.3978\n",
            "Run 2 | Epoch 500 | Loss 5.3606\n",
            "Run 2 | Epoch 1000 | Loss 5.3594\n",
            "Run 2 | Epoch 1500 | Loss 5.3594\n",
            "Run 2 | Epoch 2000 | Loss 5.3592\n",
            "Run 2 | Epoch 2500 | Loss 5.3605\n",
            "Run 2 | Epoch 3000 | Loss 5.3590\n",
            "Run 2 | Epoch 3500 | Loss 5.3593\n",
            "Run 2 | Epoch 4000 | Loss 5.3593\n",
            "Run 2 | Epoch 4500 | Loss 5.3593\n",
            "Run 2 → ACC: 0.6592, PREC: 0.8182, REC: 0.2000, F1: 0.3214, \n",
            "\n",
            "========== Run 3/10 ==========\n",
            "Run 3 | Epoch 0 | Loss 5.3992\n",
            "Run 3 | Epoch 500 | Loss 5.3579\n",
            "Run 3 | Epoch 1000 | Loss 5.3577\n",
            "Run 3 | Epoch 1500 | Loss 5.3570\n",
            "Run 3 | Epoch 2000 | Loss 5.3568\n",
            "Run 3 | Epoch 2500 | Loss 5.3570\n",
            "Run 3 | Epoch 3000 | Loss 5.3567\n",
            "Run 3 | Epoch 3500 | Loss 5.3570\n",
            "Run 3 | Epoch 4000 | Loss 5.3580\n",
            "Run 3 | Epoch 4500 | Loss 5.3568\n",
            "Run 3 → ACC: 0.6682, PREC: 0.9000, REC: 0.2000, F1: 0.3273, \n",
            "\n",
            "========== Run 4/10 ==========\n",
            "Run 4 | Epoch 0 | Loss 5.3992\n",
            "Run 4 | Epoch 500 | Loss 5.3597\n",
            "Run 4 | Epoch 1000 | Loss 5.3601\n",
            "Run 4 | Epoch 1500 | Loss 5.3594\n",
            "Run 4 | Epoch 2000 | Loss 5.3597\n",
            "Run 4 | Epoch 2500 | Loss 5.3593\n",
            "Run 4 | Epoch 3000 | Loss 5.3593\n",
            "Run 4 | Epoch 3500 | Loss 5.3593\n",
            "Run 4 | Epoch 4000 | Loss 5.3592\n",
            "Run 4 | Epoch 4500 | Loss 5.3590\n",
            "Run 4 → ACC: 0.6637, PREC: 0.8947, REC: 0.1889, F1: 0.3119, \n",
            "\n",
            "========== Run 5/10 ==========\n",
            "Run 5 | Epoch 0 | Loss 5.3989\n",
            "Run 5 | Epoch 500 | Loss 5.3602\n",
            "Run 5 | Epoch 1000 | Loss 5.3601\n",
            "Run 5 | Epoch 1500 | Loss 5.3596\n",
            "Run 5 | Epoch 2000 | Loss 5.3615\n",
            "Run 5 | Epoch 2500 | Loss 5.3605\n",
            "Run 5 | Epoch 3000 | Loss 5.3614\n",
            "Run 5 | Epoch 3500 | Loss 5.3597\n",
            "Run 5 | Epoch 4000 | Loss 5.3597\n",
            "Run 5 | Epoch 4500 | Loss 5.3591\n",
            "Run 5 → ACC: 0.6637, PREC: 0.8261, REC: 0.2111, F1: 0.3363, \n",
            "\n",
            "========== Run 6/10 ==========\n",
            "Run 6 | Epoch 0 | Loss 5.3984\n",
            "Run 6 | Epoch 500 | Loss 5.3590\n",
            "Run 6 | Epoch 1000 | Loss 5.3580\n",
            "Run 6 | Epoch 1500 | Loss 5.3578\n",
            "Run 6 | Epoch 2000 | Loss 5.3584\n",
            "Run 6 | Epoch 2500 | Loss 5.3583\n",
            "Run 6 | Epoch 3000 | Loss 5.3579\n",
            "Run 6 | Epoch 3500 | Loss 5.3582\n",
            "Run 6 | Epoch 4000 | Loss 5.3580\n",
            "Run 6 | Epoch 4500 | Loss 5.3581\n",
            "Run 6 → ACC: 0.6682, PREC: 0.9000, REC: 0.2000, F1: 0.3273, \n",
            "\n",
            "========== Run 7/10 ==========\n",
            "Run 7 | Epoch 0 | Loss 5.3992\n",
            "Run 7 | Epoch 500 | Loss 5.3605\n",
            "Run 7 | Epoch 1000 | Loss 5.3603\n",
            "Run 7 | Epoch 1500 | Loss 5.3599\n",
            "Run 7 | Epoch 2000 | Loss 5.3601\n",
            "Run 7 | Epoch 2500 | Loss 5.3601\n",
            "Run 7 | Epoch 3000 | Loss 5.3597\n",
            "Run 7 | Epoch 3500 | Loss 5.3595\n",
            "Run 7 | Epoch 4000 | Loss 5.3595\n",
            "Run 7 | Epoch 4500 | Loss 5.3592\n",
            "Run 7 → ACC: 0.6637, PREC: 0.8261, REC: 0.2111, F1: 0.3363, \n",
            "\n",
            "========== Run 8/10 ==========\n",
            "Run 8 | Epoch 0 | Loss 5.3980\n",
            "Run 8 | Epoch 500 | Loss 5.3569\n",
            "Run 8 | Epoch 1000 | Loss 5.3570\n",
            "Run 8 | Epoch 1500 | Loss 5.3569\n",
            "Run 8 | Epoch 2000 | Loss 5.3564\n",
            "Run 8 | Epoch 2500 | Loss 5.3564\n",
            "Run 8 | Epoch 3000 | Loss 5.3565\n",
            "Run 8 | Epoch 3500 | Loss 5.3571\n",
            "Run 8 | Epoch 4000 | Loss 5.3569\n",
            "Run 8 | Epoch 4500 | Loss 5.3570\n",
            "Run 8 → ACC: 0.6592, PREC: 0.8889, REC: 0.1778, F1: 0.2963, \n",
            "\n",
            "========== Run 9/10 ==========\n",
            "Run 9 | Epoch 0 | Loss 5.3984\n",
            "Run 9 | Epoch 500 | Loss 5.3602\n",
            "Run 9 | Epoch 1000 | Loss 5.3601\n",
            "Run 9 | Epoch 1500 | Loss 5.3598\n",
            "Run 9 | Epoch 2000 | Loss 5.3597\n",
            "Run 9 | Epoch 2500 | Loss 5.3599\n",
            "Run 9 | Epoch 3000 | Loss 5.3605\n",
            "Run 9 | Epoch 3500 | Loss 5.3607\n",
            "Run 9 | Epoch 4000 | Loss 5.3604\n",
            "Run 9 | Epoch 4500 | Loss 5.3608\n",
            "Run 9 → ACC: 0.6637, PREC: 0.8261, REC: 0.2111, F1: 0.3363, \n",
            "\n",
            "========== Run 10/10 ==========\n",
            "Run 10 | Epoch 0 | Loss 5.3989\n",
            "Run 10 | Epoch 500 | Loss 5.3609\n",
            "Run 10 | Epoch 1000 | Loss 5.3609\n",
            "Run 10 | Epoch 1500 | Loss 5.3609\n",
            "Run 10 | Epoch 2000 | Loss 5.3601\n",
            "Run 10 | Epoch 2500 | Loss 5.3608\n",
            "Run 10 | Epoch 3000 | Loss 5.3616\n",
            "Run 10 | Epoch 3500 | Loss 5.3614\n",
            "Run 10 | Epoch 4000 | Loss 5.3604\n",
            "Run 10 | Epoch 4500 | Loss 5.3612\n",
            "Run 10 → ACC: 0.6816, PREC: 0.8519, REC: 0.2556, F1: 0.3932, \n",
            "\n",
            "===== MAGI + K-Means (10 Runs) =====\n",
            "ACC : 0.6650 ± 0.0064\n",
            "PREC: 0.8621 ± 0.0336\n",
            "REC : 0.2033 ± 0.0211\n",
            "F1  : 0.3283 ± 0.0260\n"
          ]
        }
      ]
    }
  ]
}
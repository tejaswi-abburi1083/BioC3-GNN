{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5894d292-3e35-467f-82c7-caba9a0b2876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 5856\n",
      "Balanced samples: 3583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/va797/tmp_pyg118/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/data/home/va797/tmp_pyg118/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix: (3583, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_435795/1012092146.py:109: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647327489/work/torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  edge_index = torch.tensor([rows, cols], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges: 1642013\n",
      "\n",
      "===== Run 1/10 =====\n",
      "Epoch 0 | Loss 8.1003\n",
      "Epoch 500 | Loss 7.3648\n",
      "Epoch 1000 | Loss 7.3471\n",
      "Epoch 1500 | Loss 7.3380\n",
      "Epoch 2000 | Loss 7.3324\n",
      "Epoch 2500 | Loss 7.3294\n",
      "Run 1 → ACC: 0.9191, PREC: 0.9416, REC: 0.9115, F1: 0.9263\n",
      "\n",
      "===== Run 2/10 =====\n",
      "Epoch 0 | Loss 8.1141\n",
      "Epoch 500 | Loss 7.3642\n",
      "Epoch 1000 | Loss 7.3478\n",
      "Epoch 1500 | Loss 7.3461\n",
      "Epoch 2000 | Loss 7.3343\n",
      "Epoch 2500 | Loss 7.3331\n",
      "Run 2 → ACC: 0.9199, PREC: 0.9422, REC: 0.9125, F1: 0.9271\n",
      "\n",
      "===== Run 3/10 =====\n",
      "Epoch 0 | Loss 8.1174\n",
      "Epoch 500 | Loss 7.3626\n",
      "Epoch 1000 | Loss 7.3457\n",
      "Epoch 1500 | Loss 7.3413\n",
      "Epoch 2000 | Loss 7.3329\n",
      "Epoch 2500 | Loss 7.3303\n",
      "Run 3 → ACC: 0.9191, PREC: 0.9412, REC: 0.9120, F1: 0.9264\n",
      "\n",
      "===== Run 4/10 =====\n",
      "Epoch 0 | Loss 8.1160\n",
      "Epoch 500 | Loss 7.3644\n",
      "Epoch 1000 | Loss 7.3471\n",
      "Epoch 1500 | Loss 7.3463\n",
      "Epoch 2000 | Loss 7.3355\n",
      "Epoch 2500 | Loss 7.3283\n",
      "Run 4 → ACC: 0.9193, PREC: 0.9403, REC: 0.9135, F1: 0.9267\n",
      "\n",
      "===== Run 5/10 =====\n",
      "Epoch 0 | Loss 8.0975\n",
      "Epoch 500 | Loss 7.3646\n",
      "Epoch 1000 | Loss 7.3482\n",
      "Epoch 1500 | Loss 7.3378\n",
      "Epoch 2000 | Loss 7.3340\n",
      "Epoch 2500 | Loss 7.3336\n",
      "Run 5 → ACC: 0.9182, PREC: 0.9402, REC: 0.9115, F1: 0.9256\n",
      "\n",
      "===== Run 6/10 =====\n",
      "Epoch 0 | Loss 8.1163\n",
      "Epoch 500 | Loss 7.3654\n",
      "Epoch 1000 | Loss 7.3504\n",
      "Epoch 1500 | Loss 7.3393\n",
      "Epoch 2000 | Loss 7.3391\n",
      "Epoch 2500 | Loss 7.3333\n",
      "Run 6 → ACC: 0.9191, PREC: 0.9425, REC: 0.9105, F1: 0.9262\n",
      "\n",
      "===== Run 7/10 =====\n",
      "Epoch 0 | Loss 8.1155\n",
      "Epoch 500 | Loss 7.3652\n",
      "Epoch 1000 | Loss 7.3477\n",
      "Epoch 1500 | Loss 7.3437\n",
      "Epoch 2000 | Loss 7.3339\n",
      "Epoch 2500 | Loss 7.3354\n",
      "Run 7 → ACC: 0.9182, PREC: 0.9402, REC: 0.9115, F1: 0.9256\n",
      "\n",
      "===== Run 8/10 =====\n",
      "Epoch 0 | Loss 8.1235\n",
      "Epoch 500 | Loss 7.3632\n",
      "Epoch 1000 | Loss 7.3507\n",
      "Epoch 1500 | Loss 7.3437\n",
      "Epoch 2000 | Loss 7.3408\n",
      "Epoch 2500 | Loss 7.3342\n",
      "Run 8 → ACC: 0.9191, PREC: 0.9421, REC: 0.9110, F1: 0.9263\n",
      "\n",
      "===== Run 9/10 =====\n",
      "Epoch 0 | Loss 8.0972\n",
      "Epoch 500 | Loss 7.3603\n",
      "Epoch 1000 | Loss 7.3458\n",
      "Epoch 1500 | Loss 7.3349\n",
      "Epoch 2000 | Loss 7.3393\n",
      "Epoch 2500 | Loss 7.3332\n",
      "Run 9 → ACC: 0.9193, PREC: 0.9408, REC: 0.9130, F1: 0.9267\n",
      "\n",
      "===== Run 10/10 =====\n",
      "Epoch 0 | Loss 8.1031\n",
      "Epoch 500 | Loss 7.3617\n",
      "Epoch 1000 | Loss 7.3504\n",
      "Epoch 1500 | Loss 7.3463\n",
      "Epoch 2000 | Loss 7.3542\n",
      "Epoch 2500 | Loss 7.3459\n",
      "Run 10 → ACC: 0.9185, PREC: 0.9420, REC: 0.9100, F1: 0.9257\n",
      "\n",
      "===== MAGI + KMeans (PneumoniaMNIST, 10 Runs) =====\n",
      "ACC : 0.9190 ± 0.0005\n",
      "PREC: 0.9413 ± 0.0009\n",
      "REC : 0.9117 ± 0.0010\n",
      "F1  : 0.9263 ± 0.0005\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "from torchvision import models\n",
    "\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import to_undirected\n",
    "from torch_sparse import SparseTensor\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def setup_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "setup_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# data = np.load('/content/drive/MyDrive/TejaswiAbburi_va797/Dataset/Medmnist_data/pneumoniamnist_224.npz', allow_pickle=True)\n",
    "data = np.load('pneumoniamnist_224.npz', allow_pickle=True)\n",
    "images = np.concatenate(\n",
    "    [data['train_images'], data['val_images'], data['test_images']], axis=0\n",
    ").astype(np.float32) / 255.0\n",
    "\n",
    "labels = np.concatenate(\n",
    "    [data['train_labels'], data['val_labels'], data['test_labels']], axis=0\n",
    ").squeeze().astype(np.int64)\n",
    "\n",
    "# Convert grayscale → 3-channel\n",
    "images = np.repeat(images[:, None, :, :], 3, axis=1)\n",
    "\n",
    "X_img = torch.tensor(images)\n",
    "y = labels\n",
    "\n",
    "print(\"Total samples:\", len(y))\n",
    "\n",
    "\n",
    "idx0 = np.where(y == 0)[0]\n",
    "idx1 = np.where(y == 1)[0]\n",
    "\n",
    "random.seed(42)\n",
    "idx0 = random.sample(idx0.tolist(), min(2000, len(idx0)))\n",
    "idx1 = random.sample(idx1.tolist(), min(2000, len(idx1)))\n",
    "\n",
    "indices = idx0 + idx1\n",
    "random.shuffle(indices)\n",
    "\n",
    "X_img = X_img[indices]\n",
    "y = y[indices]\n",
    "\n",
    "dataset = TensorDataset(X_img, torch.tensor(y))\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(\"Balanced samples:\", len(y))\n",
    "\n",
    "# ======================================================\n",
    "# ResNet18 Feature Extraction\n",
    "# ======================================================\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "resnet.fc = nn.Identity()\n",
    "resnet.eval().to(device)\n",
    "\n",
    "features = []\n",
    "labels_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, lbls in loader:\n",
    "        imgs = imgs.to(device)\n",
    "        feats = resnet(imgs)\n",
    "        features.append(feats.cpu())\n",
    "        labels_list.extend(lbls.numpy())\n",
    "\n",
    "X = torch.cat(features, dim=0).numpy().astype(np.float32)\n",
    "y = np.array(labels_list)\n",
    "\n",
    "N, F_dim = X.shape\n",
    "print(\"Feature matrix:\", X.shape)\n",
    "\n",
    "x = torch.from_numpy(X).to(device)\n",
    "\n",
    "# ======================================================\n",
    "# Build cosine similarity graph\n",
    "# ======================================================\n",
    "def create_adj(features, alpha=0.9):\n",
    "    f = features / np.linalg.norm(features, axis=1, keepdims=True)\n",
    "    W = np.dot(f, f.T)\n",
    "    W = (W >= alpha).astype(np.float32)\n",
    "    return W\n",
    "\n",
    "W = create_adj(X, alpha=0.9)\n",
    "\n",
    "rows, cols = np.nonzero(W)\n",
    "edge_index = torch.tensor([rows, cols], dtype=torch.long)\n",
    "edge_index = to_undirected(edge_index).to(device)\n",
    "\n",
    "adj = SparseTensor(\n",
    "    row=edge_index[0],\n",
    "    col=edge_index[1],\n",
    "    sparse_sizes=(N, N)\n",
    ").fill_value(1.).to(device)\n",
    "\n",
    "print(\"Edges:\", adj.nnz())\n",
    "\n",
    "# ======================================================\n",
    "# MAGI utilities\n",
    "# ======================================================\n",
    "def get_sim(batch, adj, wt=50, wl=2):\n",
    "    batch_size = batch.shape[0]\n",
    "    batch_repeat = batch.repeat(wt)\n",
    "\n",
    "    rw = adj.random_walk(batch_repeat, wl)[:, 1:]\n",
    "    rw = rw.t().reshape(-1, batch_size).t()\n",
    "\n",
    "    row, col, val = [], [], []\n",
    "    for i in range(batch_size):\n",
    "        nodes, counts = torch.unique(rw[i], return_counts=True)\n",
    "        row += [batch[i].item()] * nodes.shape[0]\n",
    "        col += nodes.tolist()\n",
    "        val += counts.tolist()\n",
    "\n",
    "    adj_rw = SparseTensor(\n",
    "        row=torch.tensor(row),\n",
    "        col=torch.tensor(col),\n",
    "        value=torch.tensor(val),\n",
    "        sparse_sizes=(batch_size, batch_size)\n",
    "    )\n",
    "    return adj_rw.set_diag(0.)\n",
    "\n",
    "def get_mask(adj):\n",
    "    mean = adj.mean(dim=1)\n",
    "    mask = (adj.storage.value() -\n",
    "            mean[adj.storage.row()]) > -1e-10\n",
    "\n",
    "    return SparseTensor(\n",
    "        row=adj.storage.row()[mask],\n",
    "        col=adj.storage.col()[mask],\n",
    "        value=adj.storage.value()[mask],\n",
    "        sparse_sizes=adj.sizes()\n",
    "    )\n",
    "\n",
    "def scale(z):\n",
    "    zmin = z.min(dim=1, keepdim=True)[0]\n",
    "    zmax = z.max(dim=1, keepdim=True)[0]\n",
    "    return (z - zmin) / (zmax - zmin + 1e-12)\n",
    "\n",
    "# ======================================================\n",
    "# MAGI Loss\n",
    "# ======================================================\n",
    "class MAGILoss(nn.Module):\n",
    "    def __init__(self, tau=0.3):\n",
    "        super().__init__()\n",
    "        self.tau = tau\n",
    "\n",
    "    def forward(self, z, mask):\n",
    "        sim = torch.mm(z, z.t()) / self.tau\n",
    "        sim = sim - sim.max(dim=1, keepdim=True)[0].detach()\n",
    "\n",
    "        logits_mask = torch.ones_like(sim) - torch.eye(z.size(0), device=z.device)\n",
    "        exp_sim = torch.exp(sim) * logits_mask\n",
    "        log_prob = sim - torch.log(exp_sim.sum(dim=1, keepdim=True))\n",
    "\n",
    "        row, col = mask.storage.row(), mask.storage.col()\n",
    "        return -log_prob[row, col].mean()\n",
    "\n",
    "# ======================================================\n",
    "# Encoder (GCN)\n",
    "# ======================================================\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.conv = GCNConv(in_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv(x, edge_index)\n",
    "        return F.leaky_relu(x, 0.2)\n",
    "\n",
    "# ======================================================\n",
    "# 10-Run MAGI + KMeans Evaluation\n",
    "# ======================================================\n",
    "n_runs = 10\n",
    "epochs = 3000\n",
    "\n",
    "accs, precs, recs, f1s = [], [], [], []\n",
    "\n",
    "for run in range(n_runs):\n",
    "    print(f\"\\n===== Run {run+1}/{n_runs} =====\")\n",
    "    setup_seed(42 + run)\n",
    "\n",
    "    model = Encoder(F_dim, 256).to(device)\n",
    "    loss_fn = MAGILoss(tau=0.3)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "\n",
    "    batch = torch.arange(N, device=device)\n",
    "    adj_rw = get_sim(batch, adj)\n",
    "    mask = get_mask(adj_rw)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        z = model(x, edge_index)\n",
    "        z = scale(z)\n",
    "        z = F.normalize(z, dim=1)\n",
    "\n",
    "        loss = loss_fn(z, mask)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 500 == 0:\n",
    "            print(f\"Epoch {epoch} | Loss {loss.item():.4f}\")\n",
    "\n",
    "    # Embeddings\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model(x, edge_index)\n",
    "        z = scale(z)\n",
    "        z = F.normalize(z, dim=1).cpu().numpy()\n",
    "\n",
    "    # KMeans\n",
    "    km = KMeans(n_clusters=2, n_init=20, random_state=run)\n",
    "    y_pred = km.fit_predict(z)\n",
    "\n",
    "    # Align labels\n",
    "    if accuracy_score(y, 1 - y_pred) > accuracy_score(y, y_pred):\n",
    "        y_pred = 1 - y_pred\n",
    "\n",
    "    accs.append(accuracy_score(y, y_pred))\n",
    "    precs.append(precision_score(y, y_pred))\n",
    "    recs.append(recall_score(y, y_pred))\n",
    "    f1s.append(f1_score(y, y_pred))\n",
    "\n",
    "    print(\n",
    "        f\"Run {run+1} → \"\n",
    "        f\"ACC: {accs[-1]:.4f}, \"\n",
    "        f\"PREC: {precs[-1]:.4f}, \"\n",
    "        f\"REC: {recs[-1]:.4f}, \"\n",
    "        f\"F1: {f1s[-1]:.4f}\"\n",
    "    )\n",
    "\n",
    "# ======================================================\n",
    "# Results\n",
    "# ======================================================\n",
    "print(\"\\n===== MAGI + KMeans (PneumoniaMNIST, 10 Runs) =====\")\n",
    "print(f\"ACC : {np.mean(accs):.4f} ± {np.std(accs):.4f}\")\n",
    "print(f\"PREC: {np.mean(precs):.4f} ± {np.std(precs):.4f}\")\n",
    "print(f\"REC : {np.mean(recs):.4f} ± {np.std(recs):.4f}\")\n",
    "print(f\"F1  : {np.mean(f1s):.4f} ± {np.std(f1s):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebaa5dc-e5fa-4491-9321-8067db4151e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

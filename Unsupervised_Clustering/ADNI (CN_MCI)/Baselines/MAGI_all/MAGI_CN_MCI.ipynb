{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "torch_env",
      "display_name": "PyTorch (torch_env)"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.utils import to_undirected\n",
        "from torch_sparse import SparseTensor\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    f1_score,\n",
        "    normalized_mutual_info_score,\n",
        "    adjusted_rand_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    roc_auc_score\n",
        ")\n",
        "from sklearn.cluster import SpectralClustering\n",
        "from sklearn.cluster import KMeans"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mFDWzckYT23",
        "outputId": "9cc1da45-de5b-4571-a841-e9622a4c0bee"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/snu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_seed(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "setup_seed(42)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "e6QpgFLdYVVQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fa_cn = np.load(\"/home/snu/Downloads/Histogram_CN_FA_20bin_updated.npy\", allow_pickle=True)\n",
        "fa_mci = np.load(\"/home/snu/Downloads/Histogram_MCI_FA_20bin_updated.npy\", allow_pickle=True)\n",
        "\n",
        "X = np.vstack([fa_cn, fa_mci]).astype(np.float32)\n",
        "y = np.hstack([\n",
        "    np.zeros(len(fa_cn), dtype=np.int64),\n",
        "    np.ones(len(fa_mci), dtype=np.int64)\n",
        "])\n",
        "np.random.seed(42)\n",
        "perm = np.random.permutation(X.shape[0])\n",
        "X = X[perm]\n",
        "y = y[perm]\n",
        "N, F_dim = X.shape\n",
        "print(\"Nodes:\", N, \"Features:\", F_dim)\n",
        "\n",
        "x = torch.from_numpy(X).to(device)\n",
        "y_torch = torch.from_numpy(y).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmlepJGdYXmn",
        "outputId": "6853c18d-f33e-4f3e-c28a-30eff473ac70"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nodes: 300 Features: 180\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_adj(features, alpha=0.92):\n",
        "    f = features / np.linalg.norm(features, axis=1, keepdims=True)\n",
        "    W = np.dot(f, f.T)\n",
        "    W = (W >= alpha).astype(np.float32)\n",
        "    return W\n",
        "\n",
        "W = create_adj(X, alpha=0.92)\n",
        "\n",
        "rows, cols = np.nonzero(W)\n",
        "edge_index = torch.tensor([rows, cols], dtype=torch.long)\n",
        "edge_index = to_undirected(edge_index).to(device)\n",
        "\n",
        "adj = SparseTensor(\n",
        "    row=edge_index[0],\n",
        "    col=edge_index[1],\n",
        "    sparse_sizes=(N, N)\n",
        ").fill_value(1.).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giTxlmtDYaB6",
        "outputId": "39258385-52a4-4327-e32d-6bacea067680"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipykernel_696981/280809173.py:10: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
            "  edge_index = torch.tensor([rows, cols], dtype=torch.long)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sim(batch, adj, wt=100, wl=2):     # wt - number of walks per node and wl - walk length\n",
        "    rowptr, col, _ = adj.csr()\n",
        "    batch_size = batch.shape[0]\n",
        "    batch_repeat = batch.repeat(wt)\n",
        "    rw = adj.random_walk(batch_repeat, wl)[:, 1:]\n",
        "    rw = rw.t().reshape(-1, batch_size).t()\n",
        "\n",
        "    row, col_, val = [], [], []\n",
        "    for i in range(batch.shape[0]):\n",
        "        rw_nodes, rw_times = torch.unique(rw[i], return_counts=True)\n",
        "        row += [batch[i].item()] * rw_nodes.shape[0]\n",
        "        col_ += rw_nodes.tolist()\n",
        "        val += rw_times.tolist()\n",
        "\n",
        "    adj_rw = SparseTensor(\n",
        "        row=torch.tensor(row),\n",
        "        col=torch.tensor(col_),\n",
        "        value=torch.tensor(val),\n",
        "        sparse_sizes=(batch.shape[0], batch.shape[0])\n",
        "    )\n",
        "    adj_rw = adj_rw.set_diag(0.)\n",
        "    return adj_rw"
      ],
      "metadata": {
        "id": "tSx2YQqaYdGR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mask(adj):   # get_mask() - Remove weak edges\n",
        "    mean = adj.mean(dim=1)\n",
        "    mask = (adj.storage.value() -\n",
        "            mean[adj.storage.row()]) > -1e-10\n",
        "    return SparseTensor(\n",
        "        row=adj.storage.row()[mask],\n",
        "        col=adj.storage.col()[mask],\n",
        "        value=adj.storage.value()[mask],\n",
        "        sparse_sizes=adj.sizes()\n",
        "    )\n",
        "\n",
        "def scale(z): # scale() - Normalise embeddings\n",
        "    zmin = z.min(dim=1, keepdim=True)[0]\n",
        "    zmax = z.max(dim=1, keepdim=True)[0]\n",
        "    return (z - zmin) / (zmax - zmin + 1e-12)"
      ],
      "metadata": {
        "id": "5hP3UiZwYfvH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MAGILoss(nn.Module):\n",
        "    def __init__(self, tau=0.3):\n",
        "        super().__init__()\n",
        "        self.tau = tau\n",
        "\n",
        "    def forward(self, z, mask):\n",
        "        dot = torch.mm(z, z.t()) / self.tau\n",
        "        dot = dot - dot.max(dim=1, keepdim=True)[0].detach()\n",
        "\n",
        "        logits_mask = torch.ones_like(dot) - torch.eye(z.size(0), device=z.device)\n",
        "        exp_logits = torch.exp(dot) * logits_mask\n",
        "        log_prob = dot - torch.log(exp_logits.sum(1, keepdim=True))\n",
        "\n",
        "        row, col = mask.storage.row(), mask.storage.col()\n",
        "        loss = -log_prob[row, col].mean()\n",
        "        return loss"
      ],
      "metadata": {
        "id": "KEcxwizpYiQJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        self.conv = GCNConv(in_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv(x, edge_index)\n",
        "        x = F.leaky_relu(x, 0.5)\n",
        "        return x"
      ],
      "metadata": {
        "id": "zGxRFFVfYmuv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_runs = 10\n",
        "epochs = 5000\n",
        "\n",
        "acc_list, prec_list, rec_list, f1_list, auc_list = [], [], [], [], []\n",
        "\n",
        "for run in range(n_runs):\n",
        "    print(f\"\\n========== Run {run+1}/{n_runs} ==========\")\n",
        "\n",
        "    setup_seed(42 + run)\n",
        "\n",
        "    encoder = Encoder(F_dim, 256).to(device)\n",
        "    criterion = MAGILoss(tau=0.3)\n",
        "    optimizer = torch.optim.Adam(\n",
        "        encoder.parameters(), lr=5e-4, weight_decay=1e-3\n",
        "    )\n",
        "\n",
        "    batch = torch.arange(N, device=device)\n",
        "    adj_rw = get_sim(batch, adj)\n",
        "    mask = get_mask(adj_rw)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        encoder.train()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        z = encoder(x, edge_index)\n",
        "        z = scale(z)\n",
        "        z = F.normalize(z, dim=1)\n",
        "\n",
        "        loss = criterion(z, mask)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 500 == 0:\n",
        "            print(f\"Run {run+1} | Epoch {epoch} | Loss {loss.item():.4f}\")\n",
        "\n",
        "\n",
        "    encoder.eval()\n",
        "    with torch.no_grad():\n",
        "        z = encoder(x, edge_index)\n",
        "        z = scale(z)\n",
        "        z = F.normalize(z, dim=1).cpu().numpy()\n",
        "\n",
        "    kmeans = KMeans(n_clusters=2, n_init=20, random_state=run)\n",
        "    y_pred = kmeans.fit_predict(z)\n",
        "\n",
        "    acc1 = accuracy_score(y, y_pred)\n",
        "    acc2 = accuracy_score(y, 1 - y_pred)\n",
        "    if acc2 > acc1:\n",
        "        y_pred = 1 - y_pred\n",
        "\n",
        "    acc  = accuracy_score(y, y_pred)\n",
        "    prec = precision_score(y, y_pred)\n",
        "    rec  = recall_score(y, y_pred)\n",
        "    f1   = f1_score(y, y_pred)\n",
        "\n",
        "\n",
        "    acc_list.append(acc)\n",
        "    prec_list.append(prec)\n",
        "    rec_list.append(rec)\n",
        "    f1_list.append(f1)\n",
        "\n",
        "\n",
        "    print(\n",
        "        f\"Run {run+1} → \"\n",
        "        f\"ACC: {acc:.4f}, \"\n",
        "        f\"PREC: {prec:.4f}, \"\n",
        "        f\"REC: {rec:.4f}, \"\n",
        "        f\"F1: {f1:.4f}, \"\n",
        "    )\n",
        "print(\"\\n===== MAGI + K-Means (10 Runs) =====\")\n",
        "print(f\"ACC : {np.mean(acc_list):.4f} \\u00b1 {np.std(acc_list):.4f}\")\n",
        "print(f\"PREC: {np.mean(prec_list):.4f} \\u00b1 {np.std(prec_list):.4f}\")\n",
        "print(f\"REC : {np.mean(rec_list):.4f} \\u00b1 {np.std(rec_list):.4f}\")\n",
        "print(f\"F1  : {np.mean(f1_list):.4f} \\u00b1 {np.std(f1_list):.4f}\")\n",
        "# print(f\"AUC : {np.mean(auc_list):.4f} \\u00b1 {np.std(auc_list):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azDtBverV6sy",
        "outputId": "5a4b21c8-2792-4d28-d5bd-a91c1ca91087"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== Run 1/10 ==========\n",
            "Run 1 | Epoch 0 | Loss 5.6747\n",
            "Run 1 | Epoch 500 | Loss 5.1354\n",
            "Run 1 | Epoch 1000 | Loss 5.1096\n",
            "Run 1 | Epoch 1500 | Loss 5.0956\n",
            "Run 1 | Epoch 2000 | Loss 5.0918\n",
            "Run 1 | Epoch 2500 | Loss 5.0875\n",
            "Run 1 | Epoch 3000 | Loss 5.0807\n",
            "Run 1 | Epoch 3500 | Loss 5.0742\n",
            "Run 1 | Epoch 4000 | Loss 5.0778\n",
            "Run 1 | Epoch 4500 | Loss 5.0740\n",
            "Run 1 → ACC: 0.7767, PREC: 0.8571, REC: 0.7186, F1: 0.7818, \n",
            "\n",
            "========== Run 2/10 ==========\n",
            "Run 2 | Epoch 0 | Loss 5.6700\n",
            "Run 2 | Epoch 500 | Loss 5.1300\n",
            "Run 2 | Epoch 1000 | Loss 5.1027\n",
            "Run 2 | Epoch 1500 | Loss 5.0913\n",
            "Run 2 | Epoch 2000 | Loss 5.0837\n",
            "Run 2 | Epoch 2500 | Loss 5.0796\n",
            "Run 2 | Epoch 3000 | Loss 5.0750\n",
            "Run 2 | Epoch 3500 | Loss 5.0697\n",
            "Run 2 | Epoch 4000 | Loss 5.0650\n",
            "Run 2 | Epoch 4500 | Loss 5.0649\n",
            "Run 2 → ACC: 0.7733, PREC: 0.8511, REC: 0.7186, F1: 0.7792, \n",
            "\n",
            "========== Run 3/10 ==========\n",
            "Run 3 | Epoch 0 | Loss 5.6774\n",
            "Run 3 | Epoch 500 | Loss 5.1331\n",
            "Run 3 | Epoch 1000 | Loss 5.1046\n",
            "Run 3 | Epoch 1500 | Loss 5.0980\n",
            "Run 3 | Epoch 2000 | Loss 5.0954\n",
            "Run 3 | Epoch 2500 | Loss 5.0862\n",
            "Run 3 | Epoch 3000 | Loss 5.0818\n",
            "Run 3 | Epoch 3500 | Loss 5.0780\n",
            "Run 3 | Epoch 4000 | Loss 5.0771\n",
            "Run 3 | Epoch 4500 | Loss 5.0763\n",
            "Run 3 → ACC: 0.7733, PREC: 0.8511, REC: 0.7186, F1: 0.7792, \n",
            "\n",
            "========== Run 4/10 ==========\n",
            "Run 4 | Epoch 0 | Loss 5.6758\n",
            "Run 4 | Epoch 500 | Loss 5.1326\n",
            "Run 4 | Epoch 1000 | Loss 5.1064\n",
            "Run 4 | Epoch 1500 | Loss 5.0932\n",
            "Run 4 | Epoch 2000 | Loss 5.0870\n",
            "Run 4 | Epoch 2500 | Loss 5.0820\n",
            "Run 4 | Epoch 3000 | Loss 5.0742\n",
            "Run 4 | Epoch 3500 | Loss 5.0759\n",
            "Run 4 | Epoch 4000 | Loss 5.0687\n",
            "Run 4 | Epoch 4500 | Loss 5.0763\n",
            "Run 4 → ACC: 0.7767, PREC: 0.8571, REC: 0.7186, F1: 0.7818, \n",
            "\n",
            "========== Run 5/10 ==========\n",
            "Run 5 | Epoch 0 | Loss 5.6749\n",
            "Run 5 | Epoch 500 | Loss 5.1183\n",
            "Run 5 | Epoch 1000 | Loss 5.0944\n",
            "Run 5 | Epoch 1500 | Loss 5.0822\n",
            "Run 5 | Epoch 2000 | Loss 5.0711\n",
            "Run 5 | Epoch 2500 | Loss 5.0655\n",
            "Run 5 | Epoch 3000 | Loss 5.0642\n",
            "Run 5 | Epoch 3500 | Loss 5.0589\n",
            "Run 5 | Epoch 4000 | Loss 5.0566\n",
            "Run 5 | Epoch 4500 | Loss 5.0518\n",
            "Run 5 → ACC: 0.7733, PREC: 0.8511, REC: 0.7186, F1: 0.7792, \n",
            "\n",
            "========== Run 6/10 ==========\n",
            "Run 6 | Epoch 0 | Loss 5.6722\n",
            "Run 6 | Epoch 500 | Loss 5.1277\n",
            "Run 6 | Epoch 1000 | Loss 5.0989\n",
            "Run 6 | Epoch 1500 | Loss 5.0927\n",
            "Run 6 | Epoch 2000 | Loss 5.0853\n",
            "Run 6 | Epoch 2500 | Loss 5.0790\n",
            "Run 6 | Epoch 3000 | Loss 5.0801\n",
            "Run 6 | Epoch 3500 | Loss 5.0707\n",
            "Run 6 | Epoch 4000 | Loss 5.0698\n",
            "Run 6 | Epoch 4500 | Loss 5.0662\n",
            "Run 6 → ACC: 0.7733, PREC: 0.8511, REC: 0.7186, F1: 0.7792, \n",
            "\n",
            "========== Run 7/10 ==========\n",
            "Run 7 | Epoch 0 | Loss 5.6784\n",
            "Run 7 | Epoch 500 | Loss 5.1256\n",
            "Run 7 | Epoch 1000 | Loss 5.0976\n",
            "Run 7 | Epoch 1500 | Loss 5.0848\n",
            "Run 7 | Epoch 2000 | Loss 5.0756\n",
            "Run 7 | Epoch 2500 | Loss 5.0756\n",
            "Run 7 | Epoch 3000 | Loss 5.0694\n",
            "Run 7 | Epoch 3500 | Loss 5.0682\n",
            "Run 7 | Epoch 4000 | Loss 5.0619\n",
            "Run 7 | Epoch 4500 | Loss 5.0660\n",
            "Run 7 → ACC: 0.7700, PREC: 0.8828, REC: 0.6766, F1: 0.7661, \n",
            "\n",
            "========== Run 8/10 ==========\n",
            "Run 8 | Epoch 0 | Loss 5.6691\n",
            "Run 8 | Epoch 500 | Loss 5.1288\n",
            "Run 8 | Epoch 1000 | Loss 5.1061\n",
            "Run 8 | Epoch 1500 | Loss 5.0936\n",
            "Run 8 | Epoch 2000 | Loss 5.0882\n",
            "Run 8 | Epoch 2500 | Loss 5.0818\n",
            "Run 8 | Epoch 3000 | Loss 5.0762\n",
            "Run 8 | Epoch 3500 | Loss 5.0829\n",
            "Run 8 | Epoch 4000 | Loss 5.0776\n",
            "Run 8 | Epoch 4500 | Loss 5.0686\n",
            "Run 8 → ACC: 0.7667, PREC: 0.8760, REC: 0.6766, F1: 0.7635, \n",
            "\n",
            "========== Run 9/10 ==========\n",
            "Run 9 | Epoch 0 | Loss 5.6691\n",
            "Run 9 | Epoch 500 | Loss 5.1297\n",
            "Run 9 | Epoch 1000 | Loss 5.1018\n",
            "Run 9 | Epoch 1500 | Loss 5.0897\n",
            "Run 9 | Epoch 2000 | Loss 5.0824\n",
            "Run 9 | Epoch 2500 | Loss 5.0792\n",
            "Run 9 | Epoch 3000 | Loss 5.0753\n",
            "Run 9 | Epoch 3500 | Loss 5.0709\n",
            "Run 9 | Epoch 4000 | Loss 5.0681\n",
            "Run 9 | Epoch 4500 | Loss 5.0697\n",
            "Run 9 → ACC: 0.7767, PREC: 0.8571, REC: 0.7186, F1: 0.7818, \n",
            "\n",
            "========== Run 10/10 ==========\n",
            "Run 10 | Epoch 0 | Loss 5.6753\n",
            "Run 10 | Epoch 500 | Loss 5.1204\n",
            "Run 10 | Epoch 1000 | Loss 5.0943\n",
            "Run 10 | Epoch 1500 | Loss 5.0837\n",
            "Run 10 | Epoch 2000 | Loss 5.0750\n",
            "Run 10 | Epoch 2500 | Loss 5.0711\n",
            "Run 10 | Epoch 3000 | Loss 5.0667\n",
            "Run 10 | Epoch 3500 | Loss 5.0627\n",
            "Run 10 | Epoch 4000 | Loss 5.0561\n",
            "Run 10 | Epoch 4500 | Loss 5.0548\n",
            "Run 10 → ACC: 0.7767, PREC: 0.8968, REC: 0.6766, F1: 0.7713, \n",
            "\n",
            "===== MAGI + K-Means (10 Runs) =====\n",
            "ACC : 0.7737 ± 0.0031\n",
            "PREC: 0.8631 ± 0.0154\n",
            "REC : 0.7060 ± 0.0192\n",
            "F1  : 0.7763 ± 0.0064\n"
          ]
        }
      ]
    }
  ]
}
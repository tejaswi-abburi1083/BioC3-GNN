{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LP54bJff1GAJ","executionInfo":{"status":"ok","timestamp":1762345793741,"user_tz":-330,"elapsed":2579700,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}},"outputId":"9d506559-5a70-4df9-b18e-c42244b1b3dd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset: CN vs AD — nodes=300, feats=180\n","W0: (300, 300)\n","Number of edges: 79964\n","Using device: cuda\n","\n","=== Fold 1 (CN vs AD) ===\n","Train CN: 119, Train AD: 150\n","Test CN: 14, Test AD: 17\n","Epoch 1: Sup=0.7011 | DGI=0.7096 | Reg=-0.2363 | Total=1.3870 | TrainAcc=0.4424\n","Epoch 500: Sup=0.3747 | DGI=0.6937 | Reg=-0.3689 | Total=1.0315 | TrainAcc=0.8699\n","Epoch 1000: Sup=0.3042 | DGI=0.6946 | Reg=-0.3841 | Total=0.9604 | TrainAcc=0.9368\n","Epoch 1500: Sup=0.2452 | DGI=0.6934 | Reg=-0.3908 | Total=0.8995 | TrainAcc=0.9517\n","Epoch 2000: Sup=0.2656 | DGI=0.7044 | Reg=-0.4087 | Total=0.9291 | TrainAcc=0.9628\n","Epoch 2500: Sup=0.1956 | DGI=0.6932 | Reg=-0.4273 | Total=0.8460 | TrainAcc=0.9888\n","Epoch 3000: Sup=0.2444 | DGI=0.6936 | Reg=-0.4311 | Total=0.8949 | TrainAcc=0.9963\n","Epoch 3500: Sup=0.1360 | DGI=0.6946 | Reg=-0.4243 | Total=0.7882 | TrainAcc=0.9926\n","Epoch 4000: Sup=0.2010 | DGI=0.6932 | Reg=-0.4169 | Total=0.8525 | TrainAcc=0.9888\n","Epoch 4500: Sup=0.2105 | DGI=0.6932 | Reg=-0.4161 | Total=0.8620 | TrainAcc=1.0000\n","Epoch 5000: Sup=0.1822 | DGI=0.6935 | Reg=-0.4136 | Total=0.8344 | TrainAcc=0.9888\n","Fold 1 → Acc=0.8710 Prec=0.9333 Rec=0.8235 F1=0.8750 AUC=0.9622\n","\n","=== Fold 2 (CN vs AD) ===\n","Train CN: 119, Train AD: 150\n","Test CN: 14, Test AD: 17\n","Epoch 1: Sup=0.6958 | DGI=0.7063 | Reg=-0.2343 | Total=1.3787 | TrainAcc=0.4424\n","Epoch 500: Sup=0.3769 | DGI=0.6933 | Reg=-0.3527 | Total=1.0349 | TrainAcc=0.8959\n","Epoch 1000: Sup=0.2952 | DGI=0.6948 | Reg=-0.3668 | Total=0.9533 | TrainAcc=0.9591\n","Epoch 1500: Sup=0.2784 | DGI=0.6935 | Reg=-0.3936 | Total=0.9326 | TrainAcc=0.9740\n","Epoch 2000: Sup=0.2063 | DGI=0.6933 | Reg=-0.3854 | Total=0.8610 | TrainAcc=0.9851\n","Epoch 2500: Sup=0.2191 | DGI=0.6932 | Reg=-0.4050 | Total=0.8718 | TrainAcc=0.9888\n","Epoch 3000: Sup=0.2009 | DGI=0.6932 | Reg=-0.4028 | Total=0.8538 | TrainAcc=0.9963\n","Epoch 3500: Sup=0.1807 | DGI=0.6936 | Reg=-0.4157 | Total=0.8327 | TrainAcc=0.9777\n","Epoch 4000: Sup=0.2018 | DGI=0.6932 | Reg=-0.4185 | Total=0.8532 | TrainAcc=0.9963\n","Epoch 4500: Sup=0.2095 | DGI=0.6932 | Reg=-0.4153 | Total=0.8611 | TrainAcc=0.9963\n","Epoch 5000: Sup=0.1031 | DGI=0.6946 | Reg=-0.4367 | Total=0.7540 | TrainAcc=0.9851\n","Fold 2 → Acc=0.9032 Prec=0.9375 Rec=0.8824 F1=0.9091 AUC=0.9832\n","\n","=== Fold 3 (CN vs AD) ===\n","Train CN: 119, Train AD: 150\n","Test CN: 14, Test AD: 17\n","Epoch 1: Sup=0.7264 | DGI=0.7128 | Reg=-0.2354 | Total=1.4157 | TrainAcc=0.5576\n","Epoch 500: Sup=0.3494 | DGI=0.6957 | Reg=-0.3631 | Total=1.0087 | TrainAcc=0.8662\n","Epoch 1000: Sup=0.3329 | DGI=0.6934 | Reg=-0.3835 | Total=0.9879 | TrainAcc=0.9368\n","Epoch 1500: Sup=0.2721 | DGI=0.6952 | Reg=-0.3908 | Total=0.9283 | TrainAcc=0.9703\n","Epoch 2000: Sup=0.2745 | DGI=0.6943 | Reg=-0.4016 | Total=0.9286 | TrainAcc=0.9814\n","Epoch 2500: Sup=0.2193 | DGI=0.6933 | Reg=-0.4138 | Total=0.8712 | TrainAcc=0.9851\n","Epoch 3000: Sup=0.1726 | DGI=0.6975 | Reg=-0.4247 | Total=0.8277 | TrainAcc=0.9963\n","Epoch 3500: Sup=0.2048 | DGI=0.6960 | Reg=-0.4232 | Total=0.8584 | TrainAcc=0.9814\n","Epoch 4000: Sup=0.1556 | DGI=0.6940 | Reg=-0.4240 | Total=0.8072 | TrainAcc=0.9888\n","Epoch 4500: Sup=0.1844 | DGI=0.6938 | Reg=-0.4299 | Total=0.8353 | TrainAcc=1.0000\n","Epoch 5000: Sup=0.1707 | DGI=0.6933 | Reg=-0.4120 | Total=0.8228 | TrainAcc=0.9814\n","Fold 3 → Acc=0.7742 Prec=0.9167 Rec=0.6471 F1=0.7586 AUC=0.9748\n","\n","=== Fold 4 (CN vs AD) ===\n","Train CN: 119, Train AD: 150\n","Test CN: 14, Test AD: 17\n","Epoch 1: Sup=0.6796 | DGI=0.7144 | Reg=-0.2365 | Total=1.3704 | TrainAcc=0.7100\n","Epoch 500: Sup=0.3795 | DGI=0.6934 | Reg=-0.3787 | Total=1.0350 | TrainAcc=0.9219\n","Epoch 1000: Sup=0.2940 | DGI=0.6933 | Reg=-0.3859 | Total=0.9486 | TrainAcc=0.9480\n","Epoch 1500: Sup=0.2628 | DGI=0.6932 | Reg=-0.3990 | Total=0.9161 | TrainAcc=0.9554\n","Epoch 2000: Sup=0.2343 | DGI=0.6942 | Reg=-0.4146 | Total=0.8871 | TrainAcc=0.9517\n","Epoch 2500: Sup=0.1810 | DGI=0.6933 | Reg=-0.4337 | Total=0.8310 | TrainAcc=0.9851\n","Epoch 3000: Sup=0.1646 | DGI=0.6932 | Reg=-0.4266 | Total=0.8151 | TrainAcc=0.9851\n","Epoch 3500: Sup=0.2042 | DGI=0.6932 | Reg=-0.3967 | Total=0.8577 | TrainAcc=0.9814\n","Epoch 4000: Sup=0.1551 | DGI=0.6936 | Reg=-0.4218 | Total=0.8066 | TrainAcc=0.9665\n","Epoch 4500: Sup=0.1910 | DGI=0.7002 | Reg=-0.4148 | Total=0.8496 | TrainAcc=0.9851\n","Epoch 5000: Sup=0.1306 | DGI=0.6934 | Reg=-0.4381 | Total=0.7801 | TrainAcc=0.9777\n","Fold 4 → Acc=0.6774 Prec=0.6842 Rec=0.7647 F1=0.7222 AUC=0.7353\n","\n","=== Fold 5 (CN vs AD) ===\n","Train CN: 119, Train AD: 150\n","Test CN: 14, Test AD: 17\n","Epoch 1: Sup=0.7006 | DGI=0.7169 | Reg=-0.2342 | Total=1.3941 | TrainAcc=0.6654\n","Epoch 500: Sup=0.3549 | DGI=0.6939 | Reg=-0.3768 | Total=1.0112 | TrainAcc=0.8996\n","Epoch 1000: Sup=0.2411 | DGI=0.6936 | Reg=-0.3804 | Total=0.8967 | TrainAcc=0.9145\n","Epoch 1500: Sup=0.2404 | DGI=0.6955 | Reg=-0.4027 | Total=0.8957 | TrainAcc=0.9888\n","Epoch 2000: Sup=0.1690 | DGI=0.6942 | Reg=-0.4058 | Total=0.8225 | TrainAcc=0.9740\n","Epoch 2500: Sup=0.2423 | DGI=0.6932 | Reg=-0.4214 | Total=0.8933 | TrainAcc=0.9777\n","Epoch 3000: Sup=0.1604 | DGI=0.6933 | Reg=-0.4252 | Total=0.8112 | TrainAcc=0.9963\n","Epoch 3500: Sup=0.2001 | DGI=0.6937 | Reg=-0.4218 | Total=0.8516 | TrainAcc=0.9888\n","Epoch 4000: Sup=0.1444 | DGI=0.6931 | Reg=-0.4298 | Total=0.7946 | TrainAcc=0.9926\n","Epoch 4500: Sup=0.1202 | DGI=0.6933 | Reg=-0.4251 | Total=0.7711 | TrainAcc=0.9926\n","Epoch 5000: Sup=0.1503 | DGI=0.6937 | Reg=-0.4401 | Total=0.8000 | TrainAcc=0.9888\n","Fold 5 → Acc=0.7419 Prec=0.8462 Rec=0.6471 F1=0.7333 AUC=0.8824\n","\n","=== Fold 6 (CN vs AD) ===\n","Train CN: 119, Train AD: 150\n","Test CN: 14, Test AD: 17\n","Epoch 1: Sup=0.6970 | DGI=0.7121 | Reg=-0.2340 | Total=1.3857 | TrainAcc=0.5576\n","Epoch 500: Sup=0.3313 | DGI=0.6938 | Reg=-0.3575 | Total=0.9894 | TrainAcc=0.8662\n","Epoch 1000: Sup=0.3636 | DGI=0.6940 | Reg=-0.3903 | Total=1.0185 | TrainAcc=0.9108\n","Epoch 1500: Sup=0.2667 | DGI=0.6960 | Reg=-0.3892 | Total=0.9238 | TrainAcc=0.9665\n","Epoch 2000: Sup=0.2706 | DGI=0.6957 | Reg=-0.3970 | Total=0.9266 | TrainAcc=0.9926\n","Epoch 2500: Sup=0.2174 | DGI=0.6931 | Reg=-0.4112 | Total=0.8694 | TrainAcc=0.9777\n","Epoch 3000: Sup=0.2150 | DGI=0.6963 | Reg=-0.4194 | Total=0.8694 | TrainAcc=0.9814\n","Epoch 3500: Sup=0.2031 | DGI=0.6964 | Reg=-0.4285 | Total=0.8566 | TrainAcc=0.9888\n","Epoch 4000: Sup=0.1614 | DGI=0.6931 | Reg=-0.4421 | Total=0.8103 | TrainAcc=1.0000\n","Epoch 4500: Sup=0.2053 | DGI=0.6932 | Reg=-0.4235 | Total=0.8562 | TrainAcc=0.9851\n","Epoch 5000: Sup=0.1746 | DGI=0.6931 | Reg=-0.4094 | Total=0.8268 | TrainAcc=0.9926\n","Fold 6 → Acc=0.8710 Prec=0.9333 Rec=0.8235 F1=0.8750 AUC=0.9706\n","\n","=== Fold 7 (CN vs AD) ===\n","Train CN: 119, Train AD: 150\n","Test CN: 14, Test AD: 17\n","Epoch 1: Sup=0.7099 | DGI=0.7104 | Reg=-0.2360 | Total=1.3967 | TrainAcc=0.4424\n","Epoch 500: Sup=0.3261 | DGI=0.6936 | Reg=-0.3649 | Total=0.9832 | TrainAcc=0.8848\n","Epoch 1000: Sup=0.3199 | DGI=0.6942 | Reg=-0.3906 | Total=0.9750 | TrainAcc=0.9554\n","Epoch 1500: Sup=0.2321 | DGI=0.7050 | Reg=-0.4099 | Total=0.8962 | TrainAcc=0.9926\n","Epoch 2000: Sup=0.1928 | DGI=0.6943 | Reg=-0.3963 | Total=0.8475 | TrainAcc=0.9888\n","Epoch 2500: Sup=0.2123 | DGI=0.6935 | Reg=-0.3976 | Total=0.8660 | TrainAcc=0.9963\n","Epoch 3000: Sup=0.2086 | DGI=0.6932 | Reg=-0.4181 | Total=0.8600 | TrainAcc=0.9963\n","Epoch 3500: Sup=0.1563 | DGI=0.6940 | Reg=-0.4249 | Total=0.8078 | TrainAcc=1.0000\n","Epoch 4000: Sup=0.2102 | DGI=0.6931 | Reg=-0.4114 | Total=0.8622 | TrainAcc=0.9963\n","Epoch 4500: Sup=0.1473 | DGI=0.6936 | Reg=-0.4349 | Total=0.7974 | TrainAcc=0.9963\n","Epoch 5000: Sup=0.1422 | DGI=0.6946 | Reg=-0.3990 | Total=0.7969 | TrainAcc=0.9963\n","Fold 7 → Acc=0.8710 Prec=0.8824 Rec=0.8824 F1=0.8824 AUC=0.9412\n","\n","=== Fold 8 (CN vs AD) ===\n","Train CN: 119, Train AD: 150\n","Test CN: 14, Test AD: 17\n","Epoch 1: Sup=0.7140 | DGI=0.7130 | Reg=-0.2338 | Total=1.4036 | TrainAcc=0.5576\n","Epoch 500: Sup=0.3624 | DGI=0.6935 | Reg=-0.3728 | Total=1.0187 | TrainAcc=0.9033\n","Epoch 1000: Sup=0.3215 | DGI=0.6943 | Reg=-0.3944 | Total=0.9764 | TrainAcc=0.9628\n","Epoch 1500: Sup=0.2715 | DGI=0.6934 | Reg=-0.3842 | Total=0.9265 | TrainAcc=0.9554\n","Epoch 2000: Sup=0.1824 | DGI=0.6966 | Reg=-0.4124 | Total=0.8377 | TrainAcc=0.9777\n","Epoch 2500: Sup=0.2318 | DGI=0.6931 | Reg=-0.4152 | Total=0.8834 | TrainAcc=0.9926\n","Epoch 3000: Sup=0.1612 | DGI=0.6985 | Reg=-0.4111 | Total=0.8186 | TrainAcc=0.9963\n","Epoch 3500: Sup=0.2591 | DGI=0.7018 | Reg=-0.4292 | Total=0.9180 | TrainAcc=1.0000\n","Epoch 4000: Sup=0.1500 | DGI=0.6936 | Reg=-0.3989 | Total=0.8037 | TrainAcc=1.0000\n","Epoch 4500: Sup=0.1542 | DGI=0.6948 | Reg=-0.4334 | Total=0.8057 | TrainAcc=1.0000\n","Epoch 5000: Sup=0.2284 | DGI=0.6933 | Reg=-0.3993 | Total=0.8818 | TrainAcc=1.0000\n","Fold 8 → Acc=0.8065 Prec=0.8667 Rec=0.7647 F1=0.8125 AUC=0.8403\n","\n","=== Fold 9 (CN vs AD) ===\n","Train CN: 119, Train AD: 150\n","Test CN: 14, Test AD: 17\n","Epoch 1: Sup=0.6836 | DGI=0.7193 | Reg=-0.2339 | Total=1.3795 | TrainAcc=0.5576\n","Epoch 500: Sup=0.3407 | DGI=0.6934 | Reg=-0.3703 | Total=0.9971 | TrainAcc=0.8662\n","Epoch 1000: Sup=0.3524 | DGI=0.6936 | Reg=-0.4032 | Total=1.0057 | TrainAcc=0.8810\n","Epoch 1500: Sup=0.2222 | DGI=0.6932 | Reg=-0.4095 | Total=0.8744 | TrainAcc=0.9368\n","Epoch 2000: Sup=0.1990 | DGI=0.6942 | Reg=-0.4156 | Total=0.8516 | TrainAcc=0.9554\n","Epoch 2500: Sup=0.2068 | DGI=0.6972 | Reg=-0.4211 | Total=0.8618 | TrainAcc=0.9665\n","Epoch 3000: Sup=0.1967 | DGI=0.6932 | Reg=-0.4282 | Total=0.8470 | TrainAcc=0.9145\n","Epoch 3500: Sup=0.1457 | DGI=0.6934 | Reg=-0.4466 | Total=0.7944 | TrainAcc=0.9405\n","Epoch 4000: Sup=0.1718 | DGI=0.6932 | Reg=-0.4374 | Total=0.8212 | TrainAcc=0.9591\n","Epoch 4500: Sup=0.1267 | DGI=0.6944 | Reg=-0.4287 | Total=0.7782 | TrainAcc=0.9628\n","Epoch 5000: Sup=0.1692 | DGI=0.6932 | Reg=-0.4131 | Total=0.8211 | TrainAcc=0.9331\n","Fold 9 → Acc=0.5806 Prec=0.7500 Rec=0.3529 F1=0.4800 AUC=0.7017\n","\n","=== Fold 10 (CN vs AD) ===\n","Train CN: 119, Train AD: 150\n","Test CN: 14, Test AD: 17\n","Epoch 1: Sup=0.7344 | DGI=0.7112 | Reg=-0.2359 | Total=1.4220 | TrainAcc=0.5762\n","Epoch 500: Sup=0.3544 | DGI=0.6972 | Reg=-0.3650 | Total=1.0151 | TrainAcc=0.8587\n","Epoch 1000: Sup=0.2861 | DGI=0.6944 | Reg=-0.3888 | Total=0.9416 | TrainAcc=0.9219\n","Epoch 1500: Sup=0.2326 | DGI=0.6945 | Reg=-0.3986 | Total=0.8872 | TrainAcc=0.9554\n","Epoch 2000: Sup=0.2089 | DGI=0.6936 | Reg=-0.4095 | Total=0.8616 | TrainAcc=0.9814\n","Epoch 2500: Sup=0.2244 | DGI=0.6935 | Reg=-0.3859 | Total=0.8793 | TrainAcc=0.9888\n","Epoch 3000: Sup=0.1742 | DGI=0.6932 | Reg=-0.4127 | Total=0.8261 | TrainAcc=0.9926\n","Epoch 3500: Sup=0.2298 | DGI=0.6937 | Reg=-0.4115 | Total=0.8823 | TrainAcc=0.9628\n","Epoch 4000: Sup=0.1360 | DGI=0.6932 | Reg=-0.4134 | Total=0.7879 | TrainAcc=0.9777\n","Epoch 4500: Sup=0.1543 | DGI=0.6953 | Reg=-0.4210 | Total=0.8075 | TrainAcc=0.9554\n","Epoch 5000: Sup=0.1591 | DGI=0.6940 | Reg=-0.4167 | Total=0.8115 | TrainAcc=0.9554\n","Fold 10 → Acc=0.8387 Prec=0.8750 Rec=0.8235 F1=0.8485 AUC=0.8403\n","\n","=== Fold 11 (CN vs AD) ===\n","Train CN: 119, Train AD: 150\n","Test CN: 14, Test AD: 17\n","Epoch 1: Sup=0.6893 | DGI=0.7168 | Reg=-0.2363 | Total=1.3824 | TrainAcc=0.5576\n","Epoch 500: Sup=0.3176 | DGI=0.6935 | Reg=-0.3657 | Total=0.9745 | TrainAcc=0.8959\n","Epoch 1000: Sup=0.2750 | DGI=0.6936 | Reg=-0.4031 | Total=0.9283 | TrainAcc=0.9145\n","Epoch 1500: Sup=0.2325 | DGI=0.6935 | Reg=-0.4015 | Total=0.8859 | TrainAcc=0.9703\n","Epoch 2000: Sup=0.1999 | DGI=0.6935 | Reg=-0.4124 | Total=0.8521 | TrainAcc=0.9554\n","Epoch 2500: Sup=0.2138 | DGI=0.6948 | Reg=-0.4083 | Total=0.8678 | TrainAcc=1.0000\n","Epoch 3000: Sup=0.1598 | DGI=0.6932 | Reg=-0.4289 | Total=0.8101 | TrainAcc=1.0000\n","Epoch 3500: Sup=0.1677 | DGI=0.6943 | Reg=-0.4238 | Total=0.8196 | TrainAcc=0.9740\n","Epoch 4000: Sup=0.1578 | DGI=0.6932 | Reg=-0.4202 | Total=0.8089 | TrainAcc=0.9777\n","Epoch 4500: Sup=0.1582 | DGI=0.6931 | Reg=-0.4001 | Total=0.8113 | TrainAcc=0.9963\n","Epoch 5000: Sup=0.1018 | DGI=0.6940 | Reg=-0.4201 | Total=0.7538 | TrainAcc=0.9963\n","Fold 11 → Acc=0.7742 Prec=0.8125 Rec=0.7647 F1=0.7879 AUC=0.7815\n","\n","=== Fold 12 (CN vs AD) ===\n","Train CN: 119, Train AD: 150\n","Test CN: 14, Test AD: 17\n","Epoch 1: Sup=0.7079 | DGI=0.7087 | Reg=-0.2354 | Total=1.3930 | TrainAcc=0.5576\n","Epoch 500: Sup=0.3397 | DGI=0.6939 | Reg=-0.3756 | Total=0.9961 | TrainAcc=0.9033\n","Epoch 1000: Sup=0.3027 | DGI=0.6945 | Reg=-0.3929 | Total=0.9580 | TrainAcc=0.9442\n","Epoch 1500: Sup=0.2407 | DGI=0.6941 | Reg=-0.3998 | Total=0.8948 | TrainAcc=0.9777\n","Epoch 2000: Sup=0.1989 | DGI=0.6945 | Reg=-0.3969 | Total=0.8537 | TrainAcc=1.0000\n","Epoch 2500: Sup=0.1695 | DGI=0.6937 | Reg=-0.4236 | Total=0.8208 | TrainAcc=1.0000\n","Epoch 3000: Sup=0.1899 | DGI=0.6932 | Reg=-0.4254 | Total=0.8406 | TrainAcc=0.9963\n","Epoch 3500: Sup=0.1351 | DGI=0.6933 | Reg=-0.4395 | Total=0.7844 | TrainAcc=0.9963\n","Epoch 4000: Sup=0.1557 | DGI=0.6999 | Reg=-0.4238 | Total=0.8132 | TrainAcc=0.9963\n","Epoch 4500: Sup=0.1629 | DGI=0.6934 | Reg=-0.4220 | Total=0.8142 | TrainAcc=0.9963\n","Epoch 5000: Sup=0.1405 | DGI=0.6932 | Reg=-0.4246 | Total=0.7912 | TrainAcc=0.9926\n","Fold 12 → Acc=0.8065 Prec=0.8667 Rec=0.7647 F1=0.8125 AUC=0.8866\n","\n","=== Fold 13 (CN vs AD) ===\n","Train CN: 119, Train AD: 150\n","Test CN: 14, Test AD: 17\n","Epoch 1: Sup=0.7310 | DGI=0.7139 | Reg=-0.2349 | Total=1.4214 | TrainAcc=0.5576\n","Epoch 500: Sup=0.3210 | DGI=0.6944 | Reg=-0.3764 | Total=0.9777 | TrainAcc=0.8848\n","Epoch 1000: Sup=0.2755 | DGI=0.6932 | Reg=-0.3862 | Total=0.9301 | TrainAcc=0.9554\n","Epoch 1500: Sup=0.2386 | DGI=0.6943 | Reg=-0.4028 | Total=0.8926 | TrainAcc=0.9665\n","Epoch 2000: Sup=0.2225 | DGI=0.6932 | Reg=-0.4058 | Total=0.8751 | TrainAcc=1.0000\n","Epoch 2500: Sup=0.2160 | DGI=0.6933 | Reg=-0.4270 | Total=0.8666 | TrainAcc=0.9963\n","Epoch 3000: Sup=0.2151 | DGI=0.6992 | Reg=-0.4199 | Total=0.8723 | TrainAcc=0.9963\n","Epoch 3500: Sup=0.2026 | DGI=0.7018 | Reg=-0.4141 | Total=0.8630 | TrainAcc=0.9926\n","Epoch 4000: Sup=0.1411 | DGI=0.6961 | Reg=-0.4209 | Total=0.7952 | TrainAcc=0.9851\n","Epoch 4500: Sup=0.1778 | DGI=0.6932 | Reg=-0.4234 | Total=0.8286 | TrainAcc=0.9703\n","Epoch 5000: Sup=0.1734 | DGI=0.6962 | Reg=-0.4501 | Total=0.8246 | TrainAcc=0.9851\n","Fold 13 → Acc=0.7742 Prec=0.9167 Rec=0.6471 F1=0.7586 AUC=0.8992\n","\n","=== Fold 14 (CN vs AD) ===\n","Train CN: 119, Train AD: 150\n","Test CN: 14, Test AD: 17\n","Epoch 1: Sup=0.7064 | DGI=0.7076 | Reg=-0.2357 | Total=1.3905 | TrainAcc=0.6729\n","Epoch 500: Sup=0.3097 | DGI=0.6935 | Reg=-0.3852 | Total=0.9648 | TrainAcc=0.9071\n","Epoch 1000: Sup=0.2559 | DGI=0.6941 | Reg=-0.4129 | Total=0.9087 | TrainAcc=0.9480\n","Epoch 1500: Sup=0.2294 | DGI=0.6935 | Reg=-0.4192 | Total=0.8810 | TrainAcc=0.9740\n","Epoch 2000: Sup=0.2511 | DGI=0.6932 | Reg=-0.3900 | Total=0.9052 | TrainAcc=0.9926\n","Epoch 2500: Sup=0.2180 | DGI=0.6980 | Reg=-0.4298 | Total=0.8730 | TrainAcc=0.9963\n","Epoch 3000: Sup=0.1905 | DGI=0.6933 | Reg=-0.4290 | Total=0.8409 | TrainAcc=1.0000\n","Epoch 3500: Sup=0.2050 | DGI=0.6932 | Reg=-0.4315 | Total=0.8551 | TrainAcc=1.0000\n","Epoch 4000: Sup=0.1369 | DGI=0.6950 | Reg=-0.4237 | Total=0.7895 | TrainAcc=1.0000\n","Epoch 4500: Sup=0.1336 | DGI=0.6932 | Reg=-0.4376 | Total=0.7830 | TrainAcc=1.0000\n","Epoch 5000: Sup=0.1464 | DGI=0.6932 | Reg=-0.4375 | Total=0.7958 | TrainAcc=1.0000\n","Fold 14 → Acc=0.6452 Prec=0.6667 Rec=0.7059 F1=0.6857 AUC=0.7563\n","\n","=== Fold 15 (CN vs AD) ===\n","Train CN: 119, Train AD: 150\n","Test CN: 14, Test AD: 17\n","Epoch 1: Sup=0.7685 | DGI=0.7099 | Reg=-0.2356 | Total=1.4548 | TrainAcc=0.4424\n","Epoch 500: Sup=0.3454 | DGI=0.6953 | Reg=-0.3768 | Total=1.0031 | TrainAcc=0.8773\n","Epoch 1000: Sup=0.3124 | DGI=0.6935 | Reg=-0.3739 | Total=0.9685 | TrainAcc=0.9219\n","Epoch 1500: Sup=0.2096 | DGI=0.6939 | Reg=-0.4272 | Total=0.8608 | TrainAcc=0.9442\n","Epoch 2000: Sup=0.2431 | DGI=0.6951 | Reg=-0.4211 | Total=0.8961 | TrainAcc=0.9219\n","Epoch 2500: Sup=0.2116 | DGI=0.6932 | Reg=-0.4229 | Total=0.8625 | TrainAcc=0.9368\n","Epoch 3000: Sup=0.1992 | DGI=0.6933 | Reg=-0.4181 | Total=0.8507 | TrainAcc=0.9740\n","Epoch 3500: Sup=0.1708 | DGI=0.6933 | Reg=-0.4180 | Total=0.8223 | TrainAcc=0.9814\n","Epoch 4000: Sup=0.1580 | DGI=0.6931 | Reg=-0.4212 | Total=0.8091 | TrainAcc=0.9851\n","Epoch 4500: Sup=0.1501 | DGI=0.6934 | Reg=-0.4259 | Total=0.8009 | TrainAcc=0.9442\n","Epoch 5000: Sup=0.1322 | DGI=0.6938 | Reg=-0.4206 | Total=0.7839 | TrainAcc=0.9703\n","Fold 15 → Acc=0.7742 Prec=0.8571 Rec=0.7059 F1=0.7742 AUC=0.9118\n","\n","=== Fold 16 (CN vs AD) ===\n","Train CN: 119, Train AD: 150\n","Test CN: 14, Test AD: 17\n","Epoch 1: Sup=0.7413 | DGI=0.7132 | Reg=-0.2382 | Total=1.4307 | TrainAcc=0.5465\n","Epoch 500: Sup=0.3579 | DGI=0.6952 | Reg=-0.3754 | Total=1.0156 | TrainAcc=0.8810\n","Epoch 1000: Sup=0.3021 | DGI=0.6939 | Reg=-0.3828 | Total=0.9577 | TrainAcc=0.9257\n","Epoch 1500: Sup=0.2716 | DGI=0.6932 | Reg=-0.4056 | Total=0.9242 | TrainAcc=0.9368\n","Epoch 2000: Sup=0.2326 | DGI=0.6978 | Reg=-0.4114 | Total=0.8893 | TrainAcc=0.9851\n","Epoch 2500: Sup=0.1970 | DGI=0.6959 | Reg=-0.4175 | Total=0.8511 | TrainAcc=0.9628\n","Epoch 3000: Sup=0.1479 | DGI=0.6939 | Reg=-0.4234 | Total=0.7995 | TrainAcc=0.9665\n","Epoch 3500: Sup=0.1746 | DGI=0.6934 | Reg=-0.4040 | Total=0.8277 | TrainAcc=0.9591\n","Epoch 4000: Sup=0.2220 | DGI=0.6933 | Reg=-0.4316 | Total=0.8721 | TrainAcc=0.9554\n","Epoch 4500: Sup=0.1644 | DGI=0.6941 | Reg=-0.4402 | Total=0.8144 | TrainAcc=0.9665\n","Epoch 5000: Sup=0.1465 | DGI=0.6938 | Reg=-0.4228 | Total=0.7981 | TrainAcc=0.9888\n","Fold 16 → Acc=0.6129 Prec=0.7273 Rec=0.4706 F1=0.5714 AUC=0.7899\n","\n","=== Fold 17 (CN vs AD) ===\n","Train CN: 119, Train AD: 150\n","Test CN: 14, Test AD: 17\n","Epoch 1: Sup=0.7309 | DGI=0.7147 | Reg=-0.2344 | Total=1.4221 | TrainAcc=0.4424\n","Epoch 500: Sup=0.3989 | DGI=0.6939 | Reg=-0.3747 | Total=1.0554 | TrainAcc=0.8773\n","Epoch 1000: Sup=0.3355 | DGI=0.6941 | Reg=-0.3707 | Total=0.9925 | TrainAcc=0.9145\n","Epoch 1500: Sup=0.2874 | DGI=0.6941 | Reg=-0.4032 | Total=0.9412 | TrainAcc=0.9257\n","Epoch 2000: Sup=0.2357 | DGI=0.6934 | Reg=-0.4080 | Total=0.8883 | TrainAcc=0.9294\n","Epoch 2500: Sup=0.2248 | DGI=0.6933 | Reg=-0.3996 | Total=0.8781 | TrainAcc=0.9628\n","Epoch 3000: Sup=0.1946 | DGI=0.6932 | Reg=-0.4082 | Total=0.8470 | TrainAcc=0.9517\n","Epoch 3500: Sup=0.1437 | DGI=0.6957 | Reg=-0.4171 | Total=0.7977 | TrainAcc=0.9517\n","Epoch 4000: Sup=0.2353 | DGI=0.6932 | Reg=-0.3977 | Total=0.8888 | TrainAcc=0.9777\n","Epoch 4500: Sup=0.1856 | DGI=0.6933 | Reg=-0.4422 | Total=0.8347 | TrainAcc=0.9628\n","Epoch 5000: Sup=0.1735 | DGI=0.6936 | Reg=-0.4360 | Total=0.8235 | TrainAcc=0.9219\n","Fold 17 → Acc=0.8387 Prec=1.0000 Rec=0.7059 F1=0.8276 AUC=0.9832\n","\n","=== Fold 18 (CN vs AD) ===\n","Train CN: 119, Train AD: 150\n","Test CN: 14, Test AD: 17\n","Epoch 1: Sup=0.7015 | DGI=0.7143 | Reg=-0.2360 | Total=1.3922 | TrainAcc=0.5576\n","Epoch 500: Sup=0.3627 | DGI=0.6942 | Reg=-0.3631 | Total=1.0206 | TrainAcc=0.9033\n","Epoch 1000: Sup=0.3103 | DGI=0.6957 | Reg=-0.3932 | Total=0.9667 | TrainAcc=0.9480\n","Epoch 1500: Sup=0.2696 | DGI=0.6938 | Reg=-0.4034 | Total=0.9231 | TrainAcc=0.9591\n","Epoch 2000: Sup=0.2134 | DGI=0.6932 | Reg=-0.4144 | Total=0.8651 | TrainAcc=0.9888\n","Epoch 2500: Sup=0.2043 | DGI=0.6932 | Reg=-0.4171 | Total=0.8557 | TrainAcc=0.9888\n","Epoch 3000: Sup=0.2053 | DGI=0.7003 | Reg=-0.4204 | Total=0.8635 | TrainAcc=0.9963\n","Epoch 3500: Sup=0.1807 | DGI=0.7027 | Reg=-0.4229 | Total=0.8411 | TrainAcc=1.0000\n","Epoch 4000: Sup=0.1723 | DGI=0.6932 | Reg=-0.4276 | Total=0.8227 | TrainAcc=1.0000\n","Epoch 4500: Sup=0.1648 | DGI=0.6936 | Reg=-0.4159 | Total=0.8168 | TrainAcc=1.0000\n","Epoch 5000: Sup=0.1872 | DGI=0.6946 | Reg=-0.4367 | Total=0.8382 | TrainAcc=1.0000\n","Fold 18 → Acc=0.8387 Prec=0.9286 Rec=0.7647 F1=0.8387 AUC=0.9244\n","\n","=== Fold 19 (CN vs AD) ===\n","Train CN: 119, Train AD: 150\n","Test CN: 14, Test AD: 17\n","Epoch 1: Sup=0.6913 | DGI=0.7121 | Reg=-0.2353 | Total=1.3799 | TrainAcc=0.5576\n","Epoch 500: Sup=0.3422 | DGI=0.6941 | Reg=-0.3645 | Total=0.9999 | TrainAcc=0.8699\n","Epoch 1000: Sup=0.2954 | DGI=0.6942 | Reg=-0.4110 | Total=0.9485 | TrainAcc=0.9108\n","Epoch 1500: Sup=0.2643 | DGI=0.6933 | Reg=-0.3877 | Total=0.9189 | TrainAcc=0.9591\n","Epoch 2000: Sup=0.2270 | DGI=0.6936 | Reg=-0.4082 | Total=0.8798 | TrainAcc=0.9814\n","Epoch 2500: Sup=0.2187 | DGI=0.6932 | Reg=-0.4011 | Total=0.8719 | TrainAcc=0.9777\n","Epoch 3000: Sup=0.2396 | DGI=0.6933 | Reg=-0.3746 | Total=0.8954 | TrainAcc=0.9963\n","Epoch 3500: Sup=0.1415 | DGI=0.6935 | Reg=-0.4147 | Total=0.7935 | TrainAcc=1.0000\n","Epoch 4000: Sup=0.1660 | DGI=0.6949 | Reg=-0.4032 | Total=0.8206 | TrainAcc=0.9963\n","Epoch 4500: Sup=0.1765 | DGI=0.7053 | Reg=-0.3780 | Total=0.8439 | TrainAcc=0.9888\n","Epoch 5000: Sup=0.1173 | DGI=0.6965 | Reg=-0.4447 | Total=0.7693 | TrainAcc=0.9851\n","Fold 19 → Acc=0.7742 Prec=0.8125 Rec=0.7647 F1=0.7879 AUC=0.8445\n","\n","=== Fold 20 (CN vs AD) ===\n","Train CN: 119, Train AD: 150\n","Test CN: 14, Test AD: 17\n","Epoch 1: Sup=0.7131 | DGI=0.7127 | Reg=-0.2354 | Total=1.4022 | TrainAcc=0.5576\n","Epoch 500: Sup=0.3720 | DGI=0.7012 | Reg=-0.3885 | Total=1.0343 | TrainAcc=0.8736\n","Epoch 1000: Sup=0.2685 | DGI=0.6950 | Reg=-0.3871 | Total=0.9248 | TrainAcc=0.9219\n","Epoch 1500: Sup=0.2757 | DGI=0.6937 | Reg=-0.4166 | Total=0.9278 | TrainAcc=0.9517\n","Epoch 2000: Sup=0.1955 | DGI=0.6938 | Reg=-0.4133 | Total=0.8480 | TrainAcc=0.9591\n","Epoch 2500: Sup=0.1881 | DGI=0.6938 | Reg=-0.4232 | Total=0.8396 | TrainAcc=0.9851\n","Epoch 3000: Sup=0.2276 | DGI=0.6932 | Reg=-0.4225 | Total=0.8785 | TrainAcc=0.9814\n","Epoch 3500: Sup=0.1536 | DGI=0.6931 | Reg=-0.4332 | Total=0.8034 | TrainAcc=0.9777\n","Epoch 4000: Sup=0.1272 | DGI=0.6933 | Reg=-0.4292 | Total=0.7776 | TrainAcc=0.9888\n","Epoch 4500: Sup=0.1437 | DGI=0.6974 | Reg=-0.4248 | Total=0.7985 | TrainAcc=0.9740\n","Epoch 5000: Sup=0.1320 | DGI=0.6944 | Reg=-0.4126 | Total=0.7852 | TrainAcc=0.9963\n","Fold 20 → Acc=0.7419 Prec=0.9091 Rec=0.5882 F1=0.7143 AUC=0.9034\n","\n","=== Average Results (CN vs AD) ===\n","Accuracy: 0.7758 ± 0.0868\n","Precision: 0.8561 ± 0.0875\n","Recall: 0.7147 ± 0.1279\n","F1: 0.7728 ± 0.1025\n","LogLoss: 0.5494 ± 0.2998\n","AUC: 0.8756 ± 0.0845\n"]}],"source":["import os\n","import sys\n","import numpy as np\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch_geometric.data import Data\n","from torch_geometric.nn import ARMAConv\n","from sklearn.model_selection import StratifiedShuffleSplit\n","from sklearn.metrics import (\n","    accuracy_score, precision_score, recall_score,\n","    f1_score, log_loss, roc_auc_score\n",")\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","SEED = 42\n","np.random.seed(SEED)\n","random.seed(SEED)\n","torch.manual_seed(SEED)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(SEED)\n","torch.set_num_threads(4)\n","\n","fa_cn = \"/home/snu/Downloads/Histogram_CN_FA_20bin_updated.npy\"\n","fa_ad = \"/home/snu/Downloads/Histogram_MCI_FA_20bin_updated.npy\"\n","\n","# ------------------------\n","# Load features & labels\n","# ------------------------\n","X_cn = np.load(fa_cn, allow_pickle=True)\n","X_ad = np.load(fa_ad, allow_pickle=True)\n","\n","# CN → label 0, AD → label 1\n","X = np.vstack([X_cn, X_ad]).astype(np.float32)\n","y = np.hstack([\n","    np.zeros(X_cn.shape[0], dtype=np.int64),\n","    np.ones(X_ad.shape[0], dtype=np.int64)\n","])\n","\n","# Shuffle dataset\n","perm = np.random.RandomState(SEED).permutation(len(X))\n","X = X[perm]\n","y = y[perm]\n","\n","num_nodes, num_feats = X.shape\n","print(f\"Dataset: CN vs AD — nodes={num_nodes}, feats={num_feats}\")\n","\n","# ------------------------\n","# Adjacency building\n","# ------------------------\n","def create_adj(F, alpha=1.0):\n","    norms = np.linalg.norm(F, axis=1, keepdims=True)\n","    norms[norms == 0] = 1.0\n","    F_norm = F / norms\n","    W = np.dot(F_norm, F_norm.T)\n","    W = (W >= alpha).astype(np.float32)\n","    return W\n","\n","W0 = create_adj(X, alpha=0.8)\n","print(f\"W0: {W0.shape}\")\n","\n","def load_graph_torch(adj, node_feats):\n","    node_feats_t = torch.from_numpy(node_feats).float()\n","    edge_idx = np.array(np.nonzero(adj))\n","    edge_index = torch.from_numpy(edge_idx).long()\n","    return node_feats_t, edge_index\n","\n","node_feats_all, edge_index_all = load_graph_torch(W0, X)\n","print(f\"Number of edges: {edge_index_all.size(1)}\")\n","\n","# ------------------------\n","# Model components\n","# ------------------------\n","class ARMAEncoder(torch.nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_stacks=2, num_layers=1, activ=\"RELU\"):\n","        super(ARMAEncoder, self).__init__()\n","        activations = {\n","            \"SELU\": F.selu,\n","            \"SiLU\": F.silu,\n","            \"GELU\": F.gelu,\n","            \"ELU\": F.elu,\n","            \"RELU\": F.relu\n","        }\n","        self.act = activations.get(activ, F.elu)\n","        self.arma = ARMAConv(\n","            input_dim, hidden_dim,\n","            num_stacks=num_stacks,\n","            num_layers=num_layers,\n","            shared_weights=True,\n","            dropout=0.3\n","        )\n","        self.batchnorm = nn.BatchNorm1d(hidden_dim)\n","        self.dropout = nn.Dropout(0.3)\n","        self.mlp = nn.Linear(hidden_dim, hidden_dim)\n","\n","    def forward(self, data):\n","        x, edge_index = data.x, data.edge_index\n","        x = self.arma(x, edge_index)\n","        x = self.act(x)\n","        x = self.dropout(x)\n","        x = self.batchnorm(x)\n","        logits = self.mlp(x)\n","        return logits\n","\n","class AvgReadout(nn.Module):\n","    def forward(self, seq, msk=None):\n","        if msk is None:\n","            return torch.mean(seq, 0)\n","        else:\n","            msk = torch.unsqueeze(msk, -1)\n","            return torch.sum(seq * msk, 0) / torch.sum(msk)\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, n_h):\n","        super().__init__()\n","        self.f_k = nn.Bilinear(n_h, n_h, 1)\n","        nn.init.xavier_uniform_(self.f_k.weight.data)\n","        if self.f_k.bias is not None:\n","            self.f_k.bias.data.fill_(0.0)\n","\n","    def forward(self, c, h_pl, h_mi):\n","        c_x = torch.unsqueeze(c, 0).expand_as(h_pl)\n","        sc_1 = torch.squeeze(self.f_k(h_pl, c_x), 1)\n","        sc_2 = torch.squeeze(self.f_k(h_mi, c_x), 1)\n","        return torch.cat((sc_1, sc_2), 0)\n","\n","class DGI(nn.Module):\n","    def __init__(self, n_in, n_h, num_stacks=2, num_layers=1):\n","        super().__init__()\n","        self.gcn1 = ARMAEncoder(n_in, n_h, num_stacks=num_stacks, num_layers=num_layers)\n","        self.read = AvgReadout()\n","        self.sigm = nn.Sigmoid()\n","        self.disc = Discriminator(n_h)\n","\n","    def forward(self, seq1, seq2, edge_index):\n","        data1 = Data(x=seq1, edge_index=edge_index)\n","        data2 = Data(x=seq2, edge_index=edge_index)\n","        h_1 = self.gcn1(data1)\n","        c = self.read(h_1)\n","        c = self.sigm(c)\n","        h_2 = self.gcn1(data2)\n","        logits = self.disc(c, h_1, h_2)\n","        return logits, h_1\n","\n","class DGI_with_classifier(DGI):\n","    def __init__(self, n_in, n_h, n_classes=2, cut=0, device='cpu', num_stacks=2, num_layers=1):\n","        super().__init__(n_in, n_h, num_stacks=num_stacks, num_layers=num_layers)\n","        self.classifier = nn.Linear(n_h, n_classes)\n","        self.cut = cut\n","        self.device = device\n","        self.n_clusters = n_classes\n","\n","    def get_embeddings(self, node_feats, edge_index):\n","        _, embeddings = self.forward(node_feats, node_feats, edge_index)\n","        return embeddings\n","\n","    def cut_loss(self, A, S):\n","        C = F.softmax(S, dim=1)\n","        A_pool = torch.matmul(torch.matmul(A, C).t(), C)\n","        num = torch.trace(A_pool)\n","        D = torch.diag(torch.sum(A, dim=-1))\n","        D_pooled = torch.matmul(torch.matmul(D, C).t(), C)\n","        den = torch.trace(D_pooled) + 1e-9\n","        mincut_loss = -(num / den)\n","        St_S = torch.matmul(C.t(), C)\n","        I_S = torch.eye(C.shape[1], device=A.device)\n","        ortho_loss = torch.norm(St_S / (torch.norm(St_S) + 1e-9) - I_S / (torch.norm(I_S) + 1e-9))\n","        return mincut_loss + ortho_loss\n","\n","    def modularity_loss(self, A, S):\n","        C = F.softmax(S, dim=1)\n","        d = torch.sum(A, dim=1)\n","        m = torch.sum(A) + 1e-9\n","        B = A - torch.outer(d, d) / (2 * m)\n","        I_S = torch.eye(C.shape[1], device=A.device)\n","        k = torch.norm(I_S)\n","        n = S.shape[0]\n","        modularity_term = (-1 / (2 * m)) * torch.trace(torch.mm(torch.mm(C.t(), B), C))\n","        collapse_reg_term = (torch.sqrt(k) / n) * torch.norm(torch.sum(C, dim=0), p='fro') - 1\n","        return modularity_term + collapse_reg_term\n","\n","    def Reg_loss(self, A, embeddings):\n","        logits = self.classifier(embeddings)\n","        if self.cut == 1:\n","            return self.cut_loss(A, logits)\n","        else:\n","            return self.modularity_loss(A, logits)\n","\n","# ------------------------\n","# Prepare cross-validation\n","# ------------------------\n","sss = StratifiedShuffleSplit(n_splits=20, test_size=0.5, random_state=SEED)\n","accuracies, precisions, recalls, f1_scores, losses, all_auc = [], [], [], [], [], []\n","\n","def get_device():\n","    try:\n","        if torch.cuda.is_available():\n","            dev = torch.device(\"cuda\")\n","            torch.tensor([1.0], device=dev) + 1.0\n","            return dev\n","    except Exception as e:\n","        print(\"CUDA not usable, falling back to CPU:\", e)\n","        try:\n","            torch.cuda.empty_cache()\n","        except:\n","            pass\n","    return torch.device(\"cpu\")\n","\n","device = get_device()\n","print(\"Using device:\", device)\n","\n","A_tensor = torch.from_numpy(W0).float().to(device)\n","hidden_dim = 512\n","cut = 1\n","num_epochs = 5000\n","lr = 1e-4\n","weight_decay = 1e-4\n","reg_weight = 0.1\n","num_stacks = 2\n","num_layers = 1\n","\n","node_feats = node_feats_all.to(device)\n","edge_index = edge_index_all.to(device)\n","\n","N = node_feats.size(0)\n","lbl = torch.cat([torch.ones(N, device=device), torch.zeros(N, device=device)]).float()\n","\n","for fold, (train_idx, test_idx) in enumerate(sss.split(X, y), start=1):\n","    print(f\"\\n=== Fold {fold} (CN vs AD) ===\")\n","\n","    cn_idx = np.where(y == 0)[0]\n","    ad_idx = np.where(y == 1)[0]\n","\n","    sss_class = StratifiedShuffleSplit(n_splits=20, test_size=0.1, random_state=fold)\n","    cn_train_idx, cn_test_idx = next(sss_class.split(X[cn_idx], y[cn_idx]))\n","    ad_train_idx, ad_test_idx = next(sss_class.split(X[ad_idx], y[ad_idx]))\n","\n","    cn_train = cn_idx[cn_train_idx]\n","    ad_train = ad_idx[ad_train_idx]\n","    cn_test = cn_idx[cn_test_idx]\n","    ad_test = ad_idx[ad_test_idx]\n","\n","    balanced_train_idx = np.concatenate([cn_train, ad_train])\n","    test_idx_final = np.concatenate([cn_test, ad_test])\n","    np.random.shuffle(balanced_train_idx)\n","    np.random.shuffle(test_idx_final)\n","\n","    print(f\"Train CN: {len(cn_train)}, Train AD: {len(ad_train)}\")\n","    print(f\"Test CN: {len(cn_test)}, Test AD: {len(ad_test)}\")\n","\n","    y_tensor = torch.from_numpy(y).long().to(device)\n","    train_idx_t = torch.from_numpy(balanced_train_idx).long().to(device)\n","    test_idx_t = torch.from_numpy(test_idx_final).long().to(device)\n","\n","    model = DGI_with_classifier(num_feats, hidden_dim, n_classes=2, cut=cut,\n","                                device=device, num_stacks=num_stacks, num_layers=num_layers).to(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n","    bce_loss = nn.BCEWithLogitsLoss()\n","    ce_loss = nn.CrossEntropyLoss()\n","\n","    for epoch in range(1, num_epochs + 1):\n","        model.train()\n","        optimizer.zero_grad()\n","        perm = torch.randperm(N, device=device)\n","        corrupt = node_feats[perm]\n","        logits_dgi, embeddings = model(node_feats, corrupt, edge_index)\n","        dgi_loss = bce_loss(logits_dgi.squeeze(), lbl)\n","        logits_cls = model.classifier(embeddings)\n","        train_logits = logits_cls[train_idx_t]\n","        train_labels = y_tensor[train_idx_t]\n","        supervised_loss = ce_loss(train_logits, train_labels)\n","        reg_loss_val = model.Reg_loss(A_tensor, embeddings)\n","        total_loss = supervised_loss + dgi_loss + reg_weight * reg_loss_val\n","        total_loss.backward()\n","        optimizer.step()\n","\n","        if epoch % 500 == 0 or epoch == 1:\n","            model.eval()\n","            with torch.no_grad():\n","                logits_eval = model.classifier(model.get_embeddings(node_feats, edge_index))\n","                preds_train = torch.argmax(logits_eval[train_idx_t], dim=1).cpu().numpy()\n","                acc = accuracy_score(train_labels.cpu().numpy(), preds_train)\n","            print(f\"Epoch {epoch}: Sup={supervised_loss.item():.4f} | DGI={dgi_loss.item():.4f} | \"\n","                  f\"Reg={reg_loss_val.item():.4f} | Total={total_loss.item():.4f} | TrainAcc={acc:.4f}\")\n","\n","    model.eval()\n","    with torch.no_grad():\n","        emb_final = model.get_embeddings(node_feats, edge_index)\n","        logits_final = model.classifier(emb_final)\n","        probs = F.softmax(logits_final, dim=1).cpu().numpy()\n","        y_pred = np.argmax(probs, axis=1)\n","\n","    y_test = y[test_idx_final]\n","    y_pred_test = y_pred[test_idx_final]\n","    y_proba_test = probs[test_idx_final, 1]\n","\n","    acc = accuracy_score(y_test, y_pred_test)\n","    prec = precision_score(y_test, y_pred_test, zero_division=0)\n","    rec = recall_score(y_test, y_pred_test, zero_division=0)\n","    f1 = f1_score(y_test, y_pred_test, zero_division=0)\n","    loss_val = log_loss(y_test, y_proba_test)\n","    auc_score = roc_auc_score(y_test, y_proba_test)\n","\n","    accuracies.append(acc)\n","    precisions.append(prec)\n","    recalls.append(rec)\n","    f1_scores.append(f1)\n","    losses.append(loss_val)\n","    all_auc.append(auc_score)\n","\n","    print(f\"Fold {fold} → Acc={acc:.4f} Prec={prec:.4f} Rec={rec:.4f} F1={f1:.4f} AUC={auc_score:.4f}\")\n","\n","print(\"\\n=== Average Results (CN vs AD) ===\")\n","print(f\"Accuracy: {np.mean(accuracies):.4f} ± {np.std(accuracies):.4f}\")\n","print(f\"Precision: {np.mean(precisions):.4f} ± {np.std(precisions):.4f}\")\n","print(f\"Recall: {np.mean(recalls):.4f} ± {np.std(recalls):.4f}\")\n","print(f\"F1: {np.mean(f1_scores):.4f} ± {np.std(f1_scores):.4f}\")\n","print(f\"LogLoss: {np.mean(losses):.4f} ± {np.std(losses):.4f}\")\n","print(f\"AUC: {np.mean(all_auc):.4f} ± {np.std(all_auc):.4f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EaFRNj1X1GAM"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.18"}},"nbformat":4,"nbformat_minor":0}
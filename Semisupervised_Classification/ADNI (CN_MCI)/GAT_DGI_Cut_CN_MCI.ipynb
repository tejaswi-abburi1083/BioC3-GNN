{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"kUy43ove6ics","executionInfo":{"status":"ok","timestamp":1767154941274,"user_tz":-330,"elapsed":1887,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3f69119f-462c-4f0e-96f4-3a8425742998"},"outputs":[{"output_type":"stream","name":"stderr","text":["/home/snu/anaconda3/envs/torch_env/lib/python3.10/site-packages/torch_geometric/typing.py:68: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /home/snu/anaconda3/envs/torch_env/lib/python3.10/site-packages/libpyg.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb\n","  warnings.warn(f\"An issue occurred while importing 'pyg-lib'. \"\n","/home/snu/anaconda3/envs/torch_env/lib/python3.10/site-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /home/snu/anaconda3/envs/torch_env/lib/python3.10/site-packages/torch_scatter/_version_cuda.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb\n","  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n","/home/snu/anaconda3/envs/torch_env/lib/python3.10/site-packages/torch_geometric/typing.py:97: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: /home/snu/anaconda3/envs/torch_env/lib/python3.10/site-packages/torch_cluster/_version_cuda.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb\n","  warnings.warn(f\"An issue occurred while importing 'torch-cluster'. \"\n","/home/snu/anaconda3/envs/torch_env/lib/python3.10/site-packages/torch_geometric/typing.py:113: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: /home/snu/anaconda3/envs/torch_env/lib/python3.10/site-packages/torch_spline_conv/_version_cuda.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb\n","  warnings.warn(\n","/home/snu/anaconda3/envs/torch_env/lib/python3.10/site-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /home/snu/anaconda3/envs/torch_env/lib/python3.10/site-packages/torch_sparse/_version_cuda.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb\n","  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n","/home/snu/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import os\n","import sys\n","import numpy as np\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch_geometric.data import Data\n","from torch_geometric.nn import GCNConv\n","from sklearn.model_selection import StratifiedShuffleSplit\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, log_loss, roc_auc_score, roc_curve\n","from sklearn.ensemble import RandomForestClassifier  # (only if you want to compare)\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"L4sula6n6kOg","executionInfo":{"status":"ok","timestamp":1767154941323,"user_tz":-330,"elapsed":47,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"outputs":[],"source":["SEED = 42\n","np.random.seed(SEED)\n","random.seed(SEED)\n","torch.manual_seed(SEED)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(SEED)\n","torch.set_num_threads(4)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dOF-mg_t6qv4","outputId":"094dc8d7-602b-4522-8f09-596a72e1bf2b","executionInfo":{"status":"ok","timestamp":1767154941329,"user_tz":-330,"elapsed":4,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset: nodes=300, feats=180\n"]}],"source":["SEED = 42\n","np.random.seed(SEED)\n","random.seed(SEED)\n","torch.manual_seed(SEED)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed_all(SEED)\n","torch.set_num_threads(4)\n","\n","fa_cn = \"/home/snu/Downloads/Histogram_CN_FA_20bin_updated.npy\"\n","fa_mci = \"/home/snu/Downloads/Histogram_MCI_FA_20bin_updated.npy\"\n","\n","X_cn = np.load(fa_cn, allow_pickle=True)\n","X_mci = np.load(fa_mci, allow_pickle=True)\n","\n","X = np.vstack([X_cn, X_mci]).astype(np.float32)\n","y = np.hstack([\n","    np.zeros(X_cn.shape[0], dtype=np.int64),\n","    np.ones(X_mci.shape[0], dtype=np.int64)\n","])\n","\n","perm = np.random.RandomState(SEED).permutation(len(X))\n","X = X[perm]\n","y = y[perm]\n","\n","num_nodes, num_feats = X.shape\n","print(f\"Dataset: nodes={num_nodes}, feats={num_feats}\")"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"OqUGh2rVUScO","executionInfo":{"status":"ok","timestamp":1767154941374,"user_tz":-330,"elapsed":44,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"outputs":[],"source":["def create_adj(F, alpha=1):\n","    norms = np.linalg.norm(F, axis=1, keepdims=True)\n","    norms[norms == 0] = 1.0\n","    F_norm = F / norms\n","    W = np.dot(F_norm, F_norm.T)\n","    W = (W >= alpha).astype(np.float32)\n","    return W"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YKJYa-ooUYqY","outputId":"b6351ac2-e417-4f1d-f060-6c9f284c6581","executionInfo":{"status":"ok","timestamp":1767154941380,"user_tz":-330,"elapsed":4,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["W0: (300, 300)\n"]}],"source":["W0 = create_adj(X, alpha=0.92)\n","print(f\"W0: {W0.shape}\")"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tTM0EadR6_su","outputId":"822da5da-7334-4da8-fb0d-72d178b4b6ef","executionInfo":{"status":"ok","timestamp":1767154941386,"user_tz":-330,"elapsed":4,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of edges: 13604\n"]}],"source":["def load_graph_torch(adj, node_feats):\n","    node_feats_t = torch.from_numpy(node_feats).float()\n","    edge_idx = np.array(np.nonzero(adj))\n","    edge_index = torch.from_numpy(edge_idx).long()\n","    return node_feats_t, edge_index\n","node_feats_all, edge_index_all = load_graph_torch(W0, X)\n","print(f\"Number of edges: {edge_index_all.size(1)}\")"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"eFHRKND87G91","executionInfo":{"status":"ok","timestamp":1767154941433,"user_tz":-330,"elapsed":46,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch_geometric.nn import GATConv\n","\n","class GATEncoder(torch.nn.Module):\n","    def __init__(self, input_dim, hidden_dim, device, activ=\"SELU\", heads=1):\n","        super(GATEncoder, self).__init__()\n","\n","        activations = {\n","            \"SELU\": F.selu,\n","            \"SiLU\": F.silu,\n","            \"GELU\": F.gelu,\n","            \"ELU\": F.elu,\n","            \"RELU\": F.relu\n","        }\n","        self.act = activations.get(activ, F.elu)\n","\n","        self.gat = GATConv(\n","            in_channels=input_dim,\n","            out_channels=hidden_dim,\n","            heads=heads,\n","            dropout=0.3,\n","            concat=False\n","        )\n","\n","        self.batchnorm = nn.BatchNorm1d(hidden_dim)\n","        self.dropout = nn.Dropout(0.3)\n","        self.mlp = nn.Linear(hidden_dim, hidden_dim)\n","\n","    def forward(self, data):\n","        x, edge_index = data.x, data.edge_index\n","        x = self.gat(x, edge_index)\n","        x = self.act(x)\n","        x = self.dropout(x)\n","        x = self.batchnorm(x)\n","        logits = self.mlp(x)\n","        return logits\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"r-0cvCOJ7KB9","executionInfo":{"status":"ok","timestamp":1767154941437,"user_tz":-330,"elapsed":3,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"outputs":[],"source":["class AvgReadout(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","    def forward(self, seq, msk=None):\n","        if msk is None:\n","            return torch.mean(seq, 0)\n","        else:\n","            msk = torch.unsqueeze(msk, -1)\n","            return torch.sum(seq * msk, 0) / torch.sum(msk)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"GmIdJDnB7MVL","executionInfo":{"status":"ok","timestamp":1767154941443,"user_tz":-330,"elapsed":8,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"outputs":[],"source":["class Discriminator(nn.Module):\n","    def __init__(self, n_h):\n","        super().__init__()\n","        self.f_k = nn.Bilinear(n_h, n_h, 1)\n","        nn.init.xavier_uniform_(self.f_k.weight.data)\n","        if self.f_k.bias is not None:\n","            self.f_k.bias.data.fill_(0.0)\n","    def forward(self, c, h_pl, h_mi):\n","        c_x = torch.unsqueeze(c, 0).expand_as(h_pl)\n","        sc_1 = torch.squeeze(self.f_k(h_pl, c_x), 1)\n","        sc_2 = torch.squeeze(self.f_k(h_mi, c_x), 1)\n","        logits = torch.cat((sc_1, sc_2), 0)\n","        return logits"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"QIVIEMU37T-c","executionInfo":{"status":"ok","timestamp":1767154941501,"user_tz":-330,"elapsed":56,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"outputs":[],"source":["class DGI(nn.Module):\n","    def __init__(self, n_in, n_h, heads, dropout=0.25):\n","        super().__init__()\n","        self.gat1 = GATEncoder(n_in, n_h, heads=heads, device='cuda' if torch.cuda.is_available() else 'cpu', activ=nn.ELU())\n","        self.read = AvgReadout()\n","        self.sigm = nn.Sigmoid()\n","        self.disc = Discriminator(n_h * heads)\n","\n","    def forward(self, seq1, seq2, edge_index):\n","        # Create Data objects for the GATEncoder\n","        data1 = Data(x=seq1, edge_index=edge_index)\n","        data2 = Data(x=seq2, edge_index=edge_index)\n","\n","        h_1 = self.gat1(data1)\n","        c = self.read(h_1)\n","        c = self.sigm(c)\n","        h_2 = self.gat1(data2)\n","        logits = self.disc(c, h_1, h_2)\n","        return logits, h_1"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"2zhIeEMT7Xum","executionInfo":{"status":"ok","timestamp":1767154941510,"user_tz":-330,"elapsed":2,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"outputs":[],"source":["class DGI_with_classifier(DGI):\n","    def __init__(self, n_in, n_h, heads, n_classes=2, cut=0, dropout=0.25):\n","        super().__init__(n_in, n_h, heads, dropout=dropout)\n","        self.classifier = nn.Linear(n_h * heads, n_classes)\n","        self.cut = cut\n","\n","    def get_embeddings(self, node_feats, edge_index):\n","        _, embeddings = self.forward(node_feats, node_feats, edge_index)\n","        return embeddings\n","\n","    def cut_loss(self, A, S):\n","        S = F.softmax(S, dim=1)\n","        A_pool = torch.matmul(torch.matmul(A, S).t(), S)\n","        num = torch.trace(A_pool)\n","        D = torch.diag(torch.sum(A, dim=-1))\n","        D_pooled = torch.matmul(torch.matmul(D, S).t(), S)\n","        den = torch.trace(D_pooled)\n","        mincut_loss = -(num / den)\n","        St_S = torch.matmul(S.t(), S)\n","        I_S = torch.eye(S.shape[1], device=A.device)\n","        ortho_loss = torch.norm(St_S / torch.norm(St_S) - I_S / torch.norm(I_S))\n","        return mincut_loss + ortho_loss\n","\n","    def modularity_loss(self, A, S):\n","        C = F.softmax(S, dim=1)\n","        d = torch.sum(A, dim=1)\n","        m = torch.sum(A)\n","        B = A - torch.ger(d, d) / (2 * m)\n","        I_S = torch.eye(C.shape[1], device=A.device)\n","        k = torch.norm(I_S)\n","        n = S.shape[0]\n","        modularity_term = (-1 / (2 * m)) * torch.trace(torch.mm(torch.mm(C.t(), B), C))\n","        collapse_reg_term = (torch.sqrt(k) / n) * torch.norm(torch.sum(C, dim=0), p='fro') - 1\n","        return modularity_term + collapse_reg_term\n","\n","    def Reg_loss(self, A, embeddings):\n","        logits = self.classifier(embeddings)\n","        if self.cut == 1:\n","            return self.cut_loss(A, logits)\n","        else:\n","            return self.modularity_loss(A, logits)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"zkwkwr1c7g2t","executionInfo":{"status":"ok","timestamp":1767154941512,"user_tz":-330,"elapsed":1,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"outputs":[],"source":["from sklearn.model_selection import StratifiedShuffleSplit\n","sss = StratifiedShuffleSplit(n_splits=20, test_size=0.5, random_state=SEED)\n","accuracies, precisions, recalls, f1_scores, losses = [], [], [], [], []\n","all_y_true = []\n","all_y_proba = []\n","all_fpr, all_tpr, all_auc = [], [], []"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e32yknZH7msQ","outputId":"e3356558-1795-4e4b-8aaa-5c4e80e2cc0b","executionInfo":{"status":"ok","timestamp":1767154941629,"user_tz":-330,"elapsed":116,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}],"source":["def get_device():\n","    try:\n","        if torch.cuda.is_available():\n","            dev = torch.device(\"cuda\")\n","            # test a tiny tensor operation to check CUDA health\n","            torch.tensor([1.0], device=dev) + 1.0\n","            return dev\n","    except Exception as e:\n","        print(\"CUDA not usable, falling back to CPU:\", e)\n","        try:\n","            torch.cuda.empty_cache()\n","        except:\n","            pass\n","    return torch.device(\"cpu\")\n","\n","device = get_device()\n","print(\"Using device:\", device)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"GTnCirG97sG2","executionInfo":{"status":"ok","timestamp":1767154941633,"user_tz":-330,"elapsed":3,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"outputs":[],"source":["A_tensor = torch.from_numpy(W0).to(device)\n","hidden_dim = 512\n","heads = 1\n","dropout = 0.2\n","cut = 1\n","num_epochs = 2500\n","lr = 1e-4\n","weight_decay = 1e-4\n","# sup_weight = 1.0\n","# dgi_weight = 0.3\n","reg_weight = 0.001"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O4B70EKr9Rqu","outputId":"08ef400f-2bf2-4b5f-f871-f7ef2826e2a6","executionInfo":{"status":"ok","timestamp":1767155381937,"user_tz":-330,"elapsed":440303,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== Fold 1 ===\n","Train CN: 13, Train MCI: 16\n","Test CN: 120, Test MCI: 151\n","Epoch 1: Sup=0.6098 | DGI=0.7148 | Reg=-0.2523 | Total=1.0008 | TrainAcc=0.7586\n","Epoch 500: Sup=0.2304 | DGI=0.7162 | Reg=-0.7916 | Total=0.0833 | TrainAcc=0.9655\n","Epoch 1000: Sup=0.0760 | DGI=0.6939 | Reg=-0.7969 | Total=-0.0963 | TrainAcc=1.0000\n","Epoch 1500: Sup=0.0237 | DGI=0.7071 | Reg=-0.7916 | Total=-0.1316 | TrainAcc=1.0000\n","Epoch 2000: Sup=0.0047 | DGI=0.6979 | Reg=-0.7934 | Total=-0.1606 | TrainAcc=1.0000\n","Epoch 2500: Sup=0.0266 | DGI=0.6943 | Reg=-0.7995 | Total=-0.1480 | TrainAcc=1.0000\n","Fold 1 → Acc=0.7675 Prec=0.8284 Rec=0.7351 F1=0.7789 AUC=0.8505\n","\n","=== Fold 2 ===\n","Train CN: 13, Train MCI: 16\n","Test CN: 120, Test MCI: 151\n","Epoch 1: Sup=0.6313 | DGI=0.7147 | Reg=-0.2412 | Total=1.0334 | TrainAcc=0.5517\n","Epoch 500: Sup=0.1578 | DGI=0.6953 | Reg=-0.7443 | Total=0.0392 | TrainAcc=0.8966\n","Epoch 1000: Sup=0.1388 | DGI=0.6939 | Reg=-0.7279 | Total=0.0354 | TrainAcc=0.9310\n","Epoch 1500: Sup=0.2008 | DGI=0.7008 | Reg=-0.7597 | Total=0.0719 | TrainAcc=0.9310\n","Epoch 2000: Sup=0.0478 | DGI=0.6942 | Reg=-0.7562 | Total=-0.0836 | TrainAcc=1.0000\n","Epoch 2500: Sup=0.0067 | DGI=0.7023 | Reg=-0.7698 | Total=-0.1310 | TrainAcc=0.9310\n","Fold 2 → Acc=0.7675 Prec=0.7444 Rec=0.8874 F1=0.8097 AUC=0.8471\n","\n","=== Fold 3 ===\n","Train CN: 13, Train MCI: 16\n","Test CN: 120, Test MCI: 151\n","Epoch 1: Sup=0.6521 | DGI=0.7104 | Reg=-0.2511 | Total=1.0403 | TrainAcc=0.5517\n","Epoch 500: Sup=0.2752 | DGI=0.6991 | Reg=-0.7823 | Total=0.1222 | TrainAcc=0.9310\n","Epoch 1000: Sup=0.1642 | DGI=0.6993 | Reg=-0.7644 | Total=0.0292 | TrainAcc=0.9310\n","Epoch 1500: Sup=0.0999 | DGI=0.6933 | Reg=-0.7843 | Total=-0.0604 | TrainAcc=0.9655\n","Epoch 2000: Sup=0.0503 | DGI=0.6944 | Reg=-0.7605 | Total=-0.0853 | TrainAcc=1.0000\n","Epoch 2500: Sup=0.0589 | DGI=0.6949 | Reg=-0.7492 | Total=-0.0649 | TrainAcc=1.0000\n","Fold 3 → Acc=0.7712 Prec=0.7799 Rec=0.8212 F1=0.8000 AUC=0.8500\n","\n","=== Fold 4 ===\n","Train CN: 13, Train MCI: 16\n","Test CN: 120, Test MCI: 151\n","Epoch 1: Sup=0.7748 | DGI=0.7111 | Reg=-0.2365 | Total=1.1783 | TrainAcc=0.5517\n","Epoch 500: Sup=0.1179 | DGI=0.7097 | Reg=-0.7684 | Total=-0.0118 | TrainAcc=0.9655\n","Epoch 1000: Sup=0.0607 | DGI=0.7028 | Reg=-0.7857 | Total=-0.0926 | TrainAcc=1.0000\n","Epoch 1500: Sup=0.0081 | DGI=0.6935 | Reg=-0.7676 | Total=-0.1354 | TrainAcc=1.0000\n","Epoch 2000: Sup=0.0365 | DGI=0.6945 | Reg=-0.7944 | Total=-0.1328 | TrainAcc=1.0000\n","Epoch 2500: Sup=0.0137 | DGI=0.6932 | Reg=-0.7961 | Total=-0.1585 | TrainAcc=1.0000\n","Fold 4 → Acc=0.7897 Prec=0.8615 Rec=0.7417 F1=0.7972 AUC=0.8578\n","\n","=== Fold 5 ===\n","Train CN: 13, Train MCI: 16\n","Test CN: 120, Test MCI: 151\n","Epoch 1: Sup=0.6904 | DGI=0.7104 | Reg=-0.2392 | Total=1.0906 | TrainAcc=0.5517\n","Epoch 500: Sup=0.0947 | DGI=0.7098 | Reg=-0.7007 | Total=0.0329 | TrainAcc=0.9310\n","Epoch 1000: Sup=0.0258 | DGI=0.7002 | Reg=-0.7438 | Total=-0.0877 | TrainAcc=1.0000\n","Epoch 1500: Sup=0.1712 | DGI=0.6955 | Reg=-0.7218 | Total=0.0753 | TrainAcc=1.0000\n","Epoch 2000: Sup=0.0069 | DGI=0.6946 | Reg=-0.7568 | Total=-0.1247 | TrainAcc=1.0000\n","Epoch 2500: Sup=0.0352 | DGI=0.7111 | Reg=-0.7599 | Total=-0.0847 | TrainAcc=1.0000\n","Fold 5 → Acc=0.7675 Prec=0.8438 Rec=0.7152 F1=0.7742 AUC=0.8379\n","\n","=== Fold 6 ===\n","Train CN: 13, Train MCI: 16\n","Test CN: 120, Test MCI: 151\n","Epoch 1: Sup=0.7134 | DGI=0.7131 | Reg=-0.2411 | Total=1.1141 | TrainAcc=0.5517\n","Epoch 500: Sup=0.3370 | DGI=0.6935 | Reg=-0.7198 | Total=0.2414 | TrainAcc=0.8966\n","Epoch 1000: Sup=0.2331 | DGI=0.6943 | Reg=-0.7255 | Total=0.1324 | TrainAcc=0.9310\n","Epoch 1500: Sup=0.1789 | DGI=0.7000 | Reg=-0.7203 | Total=0.0887 | TrainAcc=0.9655\n","Epoch 2000: Sup=0.0468 | DGI=0.6968 | Reg=-0.7363 | Total=-0.0624 | TrainAcc=1.0000\n","Epoch 2500: Sup=0.0046 | DGI=0.7074 | Reg=-0.7585 | Total=-0.1172 | TrainAcc=1.0000\n","Fold 6 → Acc=0.7860 Prec=0.8780 Rec=0.7152 F1=0.7883 AUC=0.8724\n","\n","=== Fold 7 ===\n","Train CN: 13, Train MCI: 16\n","Test CN: 120, Test MCI: 151\n","Epoch 1: Sup=0.7096 | DGI=0.7113 | Reg=-0.2512 | Total=1.0986 | TrainAcc=0.5517\n","Epoch 500: Sup=0.1830 | DGI=0.6969 | Reg=-0.7630 | Total=0.0472 | TrainAcc=0.9310\n","Epoch 1000: Sup=0.1310 | DGI=0.6979 | Reg=-0.7677 | Total=-0.0086 | TrainAcc=0.8966\n","Epoch 1500: Sup=0.1115 | DGI=0.7032 | Reg=-0.7637 | Total=-0.0193 | TrainAcc=0.9310\n","Epoch 2000: Sup=0.0330 | DGI=0.7007 | Reg=-0.7798 | Total=-0.1162 | TrainAcc=1.0000\n","Epoch 2500: Sup=0.0275 | DGI=0.6932 | Reg=-0.7796 | Total=-0.1282 | TrainAcc=0.9655\n","Fold 7 → Acc=0.8118 Prec=0.8472 Rec=0.8079 F1=0.8271 AUC=0.8558\n","\n","=== Fold 8 ===\n","Train CN: 13, Train MCI: 16\n","Test CN: 120, Test MCI: 151\n","Epoch 1: Sup=0.7225 | DGI=0.7143 | Reg=-0.2421 | Total=1.1233 | TrainAcc=0.5517\n","Epoch 500: Sup=0.2016 | DGI=0.6936 | Reg=-0.7423 | Total=0.0835 | TrainAcc=0.8621\n","Epoch 1000: Sup=0.1071 | DGI=0.7250 | Reg=-0.7321 | Total=0.0275 | TrainAcc=0.9655\n","Epoch 1500: Sup=0.1738 | DGI=0.6942 | Reg=-0.7163 | Total=0.0823 | TrainAcc=1.0000\n","Epoch 2000: Sup=0.0747 | DGI=0.7123 | Reg=-0.7530 | Total=-0.0372 | TrainAcc=0.9655\n","Epoch 2500: Sup=0.0276 | DGI=0.7085 | Reg=-0.7587 | Total=-0.0935 | TrainAcc=0.9655\n","Fold 8 → Acc=0.7823 Prec=0.7805 Rec=0.8477 F1=0.8127 AUC=0.8465\n","\n","=== Fold 9 ===\n","Train CN: 13, Train MCI: 16\n","Test CN: 120, Test MCI: 151\n","Epoch 1: Sup=0.7394 | DGI=0.7138 | Reg=-0.2422 | Total=1.1397 | TrainAcc=0.5517\n","Epoch 500: Sup=0.2695 | DGI=0.6950 | Reg=-0.7250 | Total=0.1701 | TrainAcc=1.0000\n","Epoch 1000: Sup=0.1301 | DGI=0.7002 | Reg=-0.7033 | Total=0.0570 | TrainAcc=1.0000\n","Epoch 1500: Sup=0.0252 | DGI=0.7001 | Reg=-0.6949 | Total=-0.0396 | TrainAcc=1.0000\n","Epoch 2000: Sup=0.1123 | DGI=0.6935 | Reg=-0.7169 | Total=0.0195 | TrainAcc=0.9655\n","Epoch 2500: Sup=0.0791 | DGI=0.6970 | Reg=-0.7284 | Total=-0.0220 | TrainAcc=1.0000\n","Fold 9 → Acc=0.7786 Prec=0.8527 Rec=0.7285 F1=0.7857 AUC=0.8433\n","\n","=== Fold 10 ===\n","Train CN: 13, Train MCI: 16\n","Test CN: 120, Test MCI: 151\n","Epoch 1: Sup=0.7496 | DGI=0.7169 | Reg=-0.2416 | Total=1.1532 | TrainAcc=0.4483\n","Epoch 500: Sup=0.2872 | DGI=0.6933 | Reg=-0.7231 | Total=0.1881 | TrainAcc=0.9310\n","Epoch 1000: Sup=0.1264 | DGI=0.6941 | Reg=-0.7350 | Total=0.0161 | TrainAcc=0.9655\n","Epoch 1500: Sup=0.2381 | DGI=0.7013 | Reg=-0.7247 | Total=0.1445 | TrainAcc=1.0000\n","Epoch 2000: Sup=0.0097 | DGI=0.6940 | Reg=-0.7532 | Total=-0.1189 | TrainAcc=0.9310\n","Epoch 2500: Sup=0.0806 | DGI=0.7026 | Reg=-0.7481 | Total=-0.0351 | TrainAcc=0.9655\n","Fold 10 → Acc=0.7970 Prec=0.8243 Rec=0.8079 F1=0.8161 AUC=0.8707\n","\n","=== Fold 11 ===\n","Train CN: 13, Train MCI: 16\n","Test CN: 120, Test MCI: 151\n","Epoch 1: Sup=0.7816 | DGI=0.7107 | Reg=-0.2421 | Total=1.1792 | TrainAcc=0.5517\n","Epoch 500: Sup=0.2003 | DGI=0.6936 | Reg=-0.7190 | Total=0.1056 | TrainAcc=0.8966\n","Epoch 1000: Sup=0.1995 | DGI=0.6978 | Reg=-0.7257 | Total=0.1018 | TrainAcc=1.0000\n","Epoch 1500: Sup=0.0228 | DGI=0.7246 | Reg=-0.7328 | Total=-0.0578 | TrainAcc=0.9310\n","Epoch 2000: Sup=0.0255 | DGI=0.6953 | Reg=-0.7461 | Total=-0.0948 | TrainAcc=1.0000\n","Epoch 2500: Sup=0.0099 | DGI=0.7046 | Reg=-0.7498 | Total=-0.1057 | TrainAcc=1.0000\n","Fold 11 → Acc=0.7638 Prec=0.8480 Rec=0.7020 F1=0.7681 AUC=0.8466\n","\n","=== Fold 12 ===\n","Train CN: 13, Train MCI: 16\n","Test CN: 120, Test MCI: 151\n","Epoch 1: Sup=0.7332 | DGI=0.7149 | Reg=-0.2460 | Total=1.1306 | TrainAcc=0.5517\n","Epoch 500: Sup=0.1019 | DGI=0.6985 | Reg=-0.7143 | Total=0.0163 | TrainAcc=0.9655\n","Epoch 1000: Sup=0.1982 | DGI=0.6933 | Reg=-0.7207 | Total=0.1016 | TrainAcc=0.9655\n","Epoch 1500: Sup=0.1933 | DGI=0.7073 | Reg=-0.7012 | Total=0.1286 | TrainAcc=0.9655\n","Epoch 2000: Sup=0.0037 | DGI=0.6953 | Reg=-0.7352 | Total=-0.1057 | TrainAcc=0.9310\n","Epoch 2500: Sup=0.0024 | DGI=0.6938 | Reg=-0.7248 | Total=-0.0980 | TrainAcc=0.9655\n","Fold 12 → Acc=0.6568 Prec=0.6813 Rec=0.7219 F1=0.7010 AUC=0.6536\n","\n","=== Fold 13 ===\n","Train CN: 13, Train MCI: 16\n","Test CN: 120, Test MCI: 151\n","Epoch 1: Sup=0.7225 | DGI=0.7124 | Reg=-0.2456 | Total=1.1181 | TrainAcc=0.5517\n","Epoch 500: Sup=0.2269 | DGI=0.6957 | Reg=-0.7526 | Total=0.1004 | TrainAcc=0.9310\n","Epoch 1000: Sup=0.2307 | DGI=0.6949 | Reg=-0.7311 | Total=0.1250 | TrainAcc=0.9310\n","Epoch 1500: Sup=0.2268 | DGI=0.6939 | Reg=-0.7732 | Total=0.0782 | TrainAcc=1.0000\n","Epoch 2000: Sup=0.1833 | DGI=0.6934 | Reg=-0.7406 | Total=0.0667 | TrainAcc=0.9310\n","Epoch 2500: Sup=0.0201 | DGI=0.6932 | Reg=-0.7327 | Total=-0.0888 | TrainAcc=0.9310\n","Fold 13 → Acc=0.7491 Prec=0.7345 Rec=0.8609 F1=0.7927 AUC=0.8413\n","\n","=== Fold 14 ===\n","Train CN: 13, Train MCI: 16\n","Test CN: 120, Test MCI: 151\n","Epoch 1: Sup=0.7341 | DGI=0.7122 | Reg=-0.2454 | Total=1.1297 | TrainAcc=0.5517\n","Epoch 500: Sup=0.2586 | DGI=0.6966 | Reg=-0.7675 | Total=0.1180 | TrainAcc=0.8276\n","Epoch 1000: Sup=0.1092 | DGI=0.6963 | Reg=-0.7691 | Total=-0.0332 | TrainAcc=1.0000\n","Epoch 1500: Sup=0.0856 | DGI=0.7066 | Reg=-0.7719 | Total=-0.0504 | TrainAcc=1.0000\n","Epoch 2000: Sup=0.0121 | DGI=0.6945 | Reg=-0.7862 | Total=-0.1491 | TrainAcc=0.9310\n","Epoch 2500: Sup=0.0024 | DGI=0.6976 | Reg=-0.7804 | Total=-0.1502 | TrainAcc=1.0000\n","Fold 14 → Acc=0.7934 Prec=0.8519 Rec=0.7616 F1=0.8042 AUC=0.8656\n","\n","=== Fold 15 ===\n","Train CN: 13, Train MCI: 16\n","Test CN: 120, Test MCI: 151\n","Epoch 1: Sup=0.6300 | DGI=0.7072 | Reg=-0.2443 | Total=1.0222 | TrainAcc=0.4483\n","Epoch 500: Sup=0.1534 | DGI=0.7053 | Reg=-0.7415 | Total=0.0466 | TrainAcc=0.8966\n","Epoch 1000: Sup=0.1084 | DGI=0.7004 | Reg=-0.7287 | Total=0.0101 | TrainAcc=0.8966\n","Epoch 1500: Sup=0.1727 | DGI=0.6939 | Reg=-0.7584 | Total=0.0388 | TrainAcc=1.0000\n","Epoch 2000: Sup=0.0179 | DGI=0.6974 | Reg=-0.7380 | Total=-0.0925 | TrainAcc=1.0000\n","Epoch 2500: Sup=0.0094 | DGI=0.6982 | Reg=-0.7033 | Total=-0.0655 | TrainAcc=1.0000\n","Fold 15 → Acc=0.8044 Prec=0.8889 Rec=0.7417 F1=0.8087 AUC=0.8734\n","\n","=== Fold 16 ===\n","Train CN: 13, Train MCI: 16\n","Test CN: 120, Test MCI: 151\n","Epoch 1: Sup=0.6927 | DGI=0.7130 | Reg=-0.2527 | Total=1.0817 | TrainAcc=0.4483\n","Epoch 500: Sup=0.0816 | DGI=0.6970 | Reg=-0.7404 | Total=-0.0315 | TrainAcc=0.8621\n","Epoch 1000: Sup=0.0817 | DGI=0.6979 | Reg=-0.7730 | Total=-0.0631 | TrainAcc=1.0000\n","Epoch 1500: Sup=0.1006 | DGI=0.6947 | Reg=-0.7499 | Total=-0.0240 | TrainAcc=0.8621\n","Epoch 2000: Sup=0.0309 | DGI=0.6938 | Reg=-0.7638 | Total=-0.1085 | TrainAcc=1.0000\n","Epoch 2500: Sup=0.0873 | DGI=0.6944 | Reg=-0.7600 | Total=-0.0477 | TrainAcc=0.9655\n","Fold 16 → Acc=0.7970 Prec=0.8200 Rec=0.8146 F1=0.8173 AUC=0.8729\n","\n","=== Fold 17 ===\n","Train CN: 13, Train MCI: 16\n","Test CN: 120, Test MCI: 151\n","Epoch 1: Sup=0.6894 | DGI=0.7123 | Reg=-0.2485 | Total=1.0820 | TrainAcc=0.5517\n","Epoch 500: Sup=0.2832 | DGI=0.6935 | Reg=-0.7247 | Total=0.1826 | TrainAcc=0.8966\n","Epoch 1000: Sup=0.1084 | DGI=0.6948 | Reg=-0.7239 | Total=0.0098 | TrainAcc=0.9655\n","Epoch 1500: Sup=0.1208 | DGI=0.6963 | Reg=-0.7424 | Total=0.0051 | TrainAcc=1.0000\n","Epoch 2000: Sup=0.0126 | DGI=0.6934 | Reg=-0.7285 | Total=-0.0919 | TrainAcc=1.0000\n","Epoch 2500: Sup=0.0236 | DGI=0.6949 | Reg=-0.7613 | Total=-0.1123 | TrainAcc=1.0000\n","Fold 17 → Acc=0.7675 Prec=0.8188 Rec=0.7483 F1=0.7820 AUC=0.8249\n","\n","=== Fold 18 ===\n","Train CN: 13, Train MCI: 16\n","Test CN: 120, Test MCI: 151\n","Epoch 1: Sup=0.6306 | DGI=0.7137 | Reg=-0.2556 | Total=1.0173 | TrainAcc=0.4483\n","Epoch 500: Sup=0.1652 | DGI=0.6945 | Reg=-0.7001 | Total=0.0902 | TrainAcc=0.8966\n","Epoch 1000: Sup=0.1281 | DGI=0.6942 | Reg=-0.7304 | Total=0.0225 | TrainAcc=0.9310\n","Epoch 1500: Sup=0.0698 | DGI=0.6935 | Reg=-0.7104 | Total=-0.0166 | TrainAcc=0.9310\n","Epoch 2000: Sup=0.2199 | DGI=0.6950 | Reg=-0.7362 | Total=0.1092 | TrainAcc=1.0000\n","Epoch 2500: Sup=0.1090 | DGI=0.7032 | Reg=-0.7299 | Total=0.0120 | TrainAcc=1.0000\n","Fold 18 → Acc=0.7897 Prec=0.8176 Rec=0.8013 F1=0.8094 AUC=0.8480\n","\n","=== Fold 19 ===\n","Train CN: 13, Train MCI: 16\n","Test CN: 120, Test MCI: 151\n","Epoch 1: Sup=0.6635 | DGI=0.7146 | Reg=-0.2401 | Total=1.0665 | TrainAcc=0.4483\n","Epoch 500: Sup=0.1158 | DGI=0.7131 | Reg=-0.7375 | Total=0.0200 | TrainAcc=0.9655\n","Epoch 1000: Sup=0.1288 | DGI=0.7011 | Reg=-0.7508 | Total=0.0090 | TrainAcc=1.0000\n","Epoch 1500: Sup=0.1560 | DGI=0.7041 | Reg=-0.7409 | Total=0.0488 | TrainAcc=1.0000\n","Epoch 2000: Sup=0.0355 | DGI=0.6940 | Reg=-0.7657 | Total=-0.1056 | TrainAcc=1.0000\n","Epoch 2500: Sup=0.0777 | DGI=0.6939 | Reg=-0.7649 | Total=-0.0628 | TrainAcc=1.0000\n","Fold 19 → Acc=0.7749 Prec=0.8409 Rec=0.7351 F1=0.7845 AUC=0.8329\n","\n","=== Fold 20 ===\n","Train CN: 13, Train MCI: 16\n","Test CN: 120, Test MCI: 151\n","Epoch 1: Sup=0.7756 | DGI=0.7091 | Reg=-0.2432 | Total=1.1706 | TrainAcc=0.5862\n","Epoch 500: Sup=0.1431 | DGI=0.6962 | Reg=-0.7593 | Total=0.0104 | TrainAcc=0.8966\n","Epoch 1000: Sup=0.0895 | DGI=0.6948 | Reg=-0.7568 | Total=-0.0419 | TrainAcc=0.9310\n","Epoch 1500: Sup=0.0878 | DGI=0.7010 | Reg=-0.7719 | Total=-0.0532 | TrainAcc=0.9310\n","Epoch 2000: Sup=0.0094 | DGI=0.6935 | Reg=-0.7608 | Total=-0.1272 | TrainAcc=0.9655\n","Epoch 2500: Sup=0.0014 | DGI=0.6932 | Reg=-0.7639 | Total=-0.1386 | TrainAcc=0.9310\n","Fold 20 → Acc=0.7601 Prec=0.7792 Rec=0.7947 F1=0.7869 AUC=0.8409\n","\n","=== Average Results ===\n","Accuracy: 0.7738 ± 0.0311\n","Precision: 0.8161 ± 0.0507\n","Recall: 0.7745 ± 0.0529\n","F1: 0.7922 ± 0.0261\n","LogLoss: 2.0366 ± 0.5180\n","AUC: 0.8416 ± 0.0452\n"]}],"source":["for fold, (train_idx, test_idx) in enumerate(sss.split(X, y)):\n","    print(f\"\\n=== Fold {fold+1} ===\")\n","\n","    cn_idx = np.where(y == 0)[0]\n","    mci_idx = np.where(y == 1)[0]\n","\n","    sss_class = StratifiedShuffleSplit(n_splits=20, test_size=0.9, random_state=fold)\n","    cn_train_idx, cn_test_idx = next(sss_class.split(X[cn_idx], y[cn_idx]))\n","    mci_train_idx, mci_test_idx = next(sss_class.split(X[mci_idx], y[mci_idx]))\n","\n","    cn_train = cn_idx[cn_train_idx]\n","    mci_train = mci_idx[mci_train_idx]\n","    cn_test = cn_idx[cn_test_idx]\n","    mci_test = mci_idx[mci_test_idx]\n","\n","    balanced_train_idx = np.concatenate([cn_train, mci_train])\n","    test_idx = np.concatenate([cn_test, mci_test])\n","    np.random.shuffle(balanced_train_idx)\n","    np.random.shuffle(test_idx)\n","\n","    print(f\"Train CN: {len(cn_train)}, Train MCI: {len(mci_train)}\")\n","    print(f\"Test CN: {len(cn_test)}, Test MCI: {len(mci_test)}\")\n","\n","    node_feats = node_feats_all.to(device)\n","    edge_index = edge_index_all.to(device)\n","    y_tensor = torch.from_numpy(y).long().to(device)\n","    train_idx_t = torch.from_numpy(balanced_train_idx).long().to(device)\n","    test_idx_t = torch.from_numpy(test_idx).long().to(device)\n","\n","    N = node_feats.size(0)\n","    lbl = torch.cat([torch.ones(N, device=device), torch.zeros(N, device=device)])\n","\n","    model = DGI_with_classifier(num_feats, hidden_dim, heads=heads, n_classes=2, cut=cut, dropout=dropout).to(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n","    bce_loss = nn.BCEWithLogitsLoss()\n","    ce_loss = nn.CrossEntropyLoss()\n","\n","    for epoch in range(1, num_epochs + 1):\n","        model.train()\n","        optimizer.zero_grad()\n","\n","        perm = torch.randperm(N, device=device)\n","        corrupt = node_feats[perm]\n","\n","        logits_dgi, embeddings = model(node_feats, corrupt, edge_index)\n","        dgi_loss = bce_loss(logits_dgi.squeeze(), lbl)\n","\n","        logits_cls = model.classifier(embeddings)\n","        train_logits = logits_cls[train_idx_t]\n","        train_labels = y_tensor[train_idx_t]\n","        supervised_loss = ce_loss(train_logits, train_labels)\n","\n","        reg_loss_val = model.Reg_loss(A_tensor, embeddings)\n","        total_loss = supervised_loss + 0.9 * dgi_loss + reg_loss_val\n","\n","        total_loss.backward()\n","        optimizer.step()\n","\n","        if epoch % 500 == 0 or epoch == 1:\n","            model.eval()\n","            with torch.no_grad():\n","                logits_eval = model.classifier(model.get_embeddings(node_feats, edge_index))\n","                preds_train = torch.argmax(logits_eval[train_idx_t], dim=1).cpu().numpy()\n","                acc = accuracy_score(train_labels.cpu().numpy(), preds_train)\n","            print(f\"Epoch {epoch}: Sup={supervised_loss.item():.4f} | DGI={dgi_loss.item():.4f} | Reg={reg_loss_val.item():.4f} | Total={total_loss.item():.4f} | TrainAcc={acc:.4f}\")\n","    model.eval()\n","    with torch.no_grad():\n","        emb_final = model.get_embeddings(node_feats, edge_index)\n","        logits_final = model.classifier(emb_final)\n","        probs = F.softmax(logits_final, dim=1).cpu().numpy()\n","        y_pred = np.argmax(probs, axis=1)\n","\n","    y_test = y[test_idx]\n","    y_pred_test = y_pred[test_idx]\n","    y_proba_test = probs[test_idx, 1]\n","\n","    acc = accuracy_score(y_test, y_pred_test)\n","    prec = precision_score(y_test, y_pred_test, zero_division=0)\n","    rec = recall_score(y_test, y_pred_test, zero_division=0)\n","    f1 = f1_score(y_test, y_pred_test, zero_division=0)\n","    loss_val = log_loss(y_test, y_proba_test)\n","    auc_score = roc_auc_score(y_test, y_proba_test)\n","\n","    accuracies.append(acc)\n","    precisions.append(prec)\n","    recalls.append(rec)\n","    f1_scores.append(f1)\n","    losses.append(loss_val)\n","    all_auc.append(auc_score)\n","\n","    print(f\"Fold {fold+1} → Acc={acc:.4f} Prec={prec:.4f} Rec={rec:.4f} F1={f1:.4f} AUC={auc_score:.4f}\")\n","\n","print(\"\\n=== Average Results ===\")\n","print(f\"Accuracy: {np.mean(accuracies):.4f} ± {np.std(accuracies):.4f}\")\n","print(f\"Precision: {np.mean(precisions):.4f} ± {np.std(precisions):.4f}\")\n","print(f\"Recall: {np.mean(recalls):.4f} ± {np.std(recalls):.4f}\")\n","print(f\"F1: {np.mean(f1_scores):.4f} ± {np.std(f1_scores):.4f}\")\n","print(f\"LogLoss: {np.mean(losses):.4f} ± {np.std(losses):.4f}\")\n","print(f\"AUC: {np.mean(all_auc):.4f} ± {np.std(all_auc):.4f}\")"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"qtQCDZ4I4DQq","executionInfo":{"status":"ok","timestamp":1767155381940,"user_tz":-330,"elapsed":1,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"outputs":[],"source":["# import torch\n","# import torch.nn as nn\n","# import torch.nn.functional as F\n","# import numpy as np\n","# import pandas as pd\n","# from sklearn.model_selection import StratifiedShuffleSplit\n","# from sklearn.metrics import (\n","#     accuracy_score, precision_score, recall_score, f1_score,\n","#     log_loss, roc_auc_score\n","# )\n","\n","# A_tensor = torch.from_numpy(W0).to(device)\n","# hidden_dim = 512\n","# cut = 0\n","# num_epochs = 5000\n","# lr = 1e-4\n","# weight_decay = 1e-4\n","# reg_weights = [0.001, 0.005, 0.009, 0.01, 0.05, 0.09, 0.1, 0.3, 0.5, 0.9, 1, 2, 5, 8]\n","# sss = StratifiedShuffleSplit(n_splits=20, test_size=0.9, random_state=SEED)\n","\n","# results_summary = []\n","\n","# for reg_weight in reg_weights:\n","#     print(f\"\\n================ REG_WEIGHT = {reg_weight} ================\")\n","#     accuracies, precisions, recalls, f1_scores, losses, all_auc = [], [], [], [], [], []\n","\n","#     for fold, (train_idx, test_idx) in enumerate(sss.split(X, y)):\n","#         print(f\"\\n=== Fold {fold+1} ===\")\n","\n","#         cn_idx = np.where(y == 0)[0]\n","#         mci_idx = np.where(y == 1)[0]\n","#         sss_class = StratifiedShuffleSplit(n_splits=20, test_size=0.5, random_state=fold)\n","#         cn_train_idx, cn_test_idx = next(sss_class.split(X[cn_idx], y[cn_idx]))\n","#         mci_train_idx, mci_test_idx = next(sss_class.split(X[mci_idx], y[mci_idx]))\n","\n","#         cn_train = cn_idx[cn_train_idx]\n","#         mci_train = mci_idx[mci_train_idx]\n","#         cn_test = cn_idx[cn_test_idx]\n","#         mci_test = mci_idx[mci_test_idx]\n","\n","#         balanced_train_idx = np.concatenate([cn_train, mci_train])\n","#         test_idx = np.concatenate([cn_test, mci_test])\n","#         np.random.shuffle(balanced_train_idx)\n","#         np.random.shuffle(test_idx)\n","\n","#         print(f\"Train CN: {len(cn_train)}, Train MCI: {len(mci_train)}\")\n","#         print(f\"Test CN: {len(cn_test)}, Test MCI: {len(mci_test)}\")\n","\n","#         node_feats = node_feats_all.to(device)\n","#         edge_index = edge_index_all.to(device)\n","#         y_tensor = torch.from_numpy(y).long().to(device)\n","#         train_idx_t = torch.from_numpy(balanced_train_idx).long().to(device)\n","#         test_idx_t = torch.from_numpy(test_idx).long().to(device)\n","\n","#         N = node_feats.size(0)\n","#         lbl = torch.cat([torch.ones(N, device=device), torch.zeros(N, device=device)])\n","\n","#         model = DGI_with_classifier(num_feats, hidden_dim, n_classes=2, cut=cut).to(device)\n","#         optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n","#         bce_loss = nn.BCEWithLogitsLoss()\n","#         ce_loss = nn.CrossEntropyLoss()\n","\n","#         for epoch in range(1, num_epochs + 1):\n","#             model.train()\n","#             optimizer.zero_grad()\n","#             perm = torch.randperm(N, device=device)\n","#             corrupt = node_feats[perm]\n","#             logits_dgi, embeddings = model(node_feats, corrupt, edge_index)\n","#             dgi_loss = bce_loss(logits_dgi.squeeze(), lbl)\n","#             logits_cls = model.classifier(embeddings)\n","#             train_logits = logits_cls[train_idx_t]\n","#             train_labels = y_tensor[train_idx_t]\n","#             supervised_loss = ce_loss(train_logits, train_labels)\n","#             reg_loss_val = model.Reg_loss(A_tensor, embeddings)\n","#             total_loss = supervised_loss + dgi_loss + reg_weight * reg_loss_val\n","#             total_loss.backward()\n","#             optimizer.step()\n","#             if epoch % 500 == 0 or epoch == 1:\n","#                 model.eval()\n","#                 with torch.no_grad():\n","#                     logits_eval = model.classifier(model.get_embeddings(node_feats, edge_index))\n","#                     preds_train = torch.argmax(logits_eval[train_idx_t], dim=1).cpu().numpy()\n","#                     acc = accuracy_score(train_labels.cpu().numpy(), preds_train)\n","#                 print(f\"Epoch {epoch}: Sup={supervised_loss.item():.4f} | \"\n","#                       f\"DGI={dgi_loss.item():.4f} | Reg={reg_loss_val.item():.4f} | \"\n","#                       f\"Total={total_loss.item():.4f} | TrainAcc={acc:.4f}\")\n","\n","#         model.eval()\n","#         with torch.no_grad():\n","#             emb_final = model.get_embeddings(node_feats, edge_index)\n","#             logits_final = model.classifier(emb_final)\n","#             probs = F.softmax(logits_final, dim=1).cpu().numpy()\n","#             y_pred = np.argmax(probs, axis=1)\n","\n","#         y_test = y[test_idx]\n","#         y_pred_test = y_pred[test_idx]\n","#         y_proba_test = probs[test_idx, 1]\n","\n","#         acc = accuracy_score(y_test, y_pred_test)\n","#         prec = precision_score(y_test, y_pred_test, zero_division=0)\n","#         rec = recall_score(y_test, y_pred_test, zero_division=0)\n","#         f1 = f1_score(y_test, y_pred_test, zero_division=0)\n","#         loss_val = log_loss(y_test, y_proba_test)\n","#         auc_score = roc_auc_score(y_test, y_proba_test)\n","\n","#         accuracies.append(acc)\n","#         precisions.append(prec)\n","#         recalls.append(rec)\n","#         f1_scores.append(f1)\n","#         losses.append(loss_val)\n","#         all_auc.append(auc_score)\n","#         print(f\"Fold {fold+1} → Acc={acc:.4f} Prec={prec:.4f} Rec={rec:.4f} \"\n","#               f\"F1={f1:.4f} AUC={auc_score:.4f}\")\n","\n","#     mean_acc, std_acc = np.mean(accuracies), np.std(accuracies)\n","#     mean_prec, std_prec = np.mean(precisions), np.std(precisions)\n","#     mean_rec, std_rec = np.mean(recalls), np.std(recalls)\n","#     mean_f1, std_f1 = np.mean(f1_scores), np.std(f1_scores)\n","#     mean_loss, std_loss = np.mean(losses), np.std(losses)\n","#     mean_auc, std_auc = np.mean(all_auc), np.std(all_auc)\n","\n","#     results_summary.append({\n","#         \"Reg_Weight\": reg_weight,\n","#         \"Accuracy\": f\"{mean_acc:.4f} ± {std_acc:.4f}\",\n","#         \"Precision\": f\"{mean_prec:.4f} ± {std_prec:.4f}\",\n","#         \"Recall\": f\"{mean_rec:.4f} ± {std_rec:.4f}\",\n","#         \"F1\": f\"{mean_f1:.4f} ± {std_f1:.4f}\",\n","#         \"LogLoss\": f\"{mean_loss:.4f} ± {std_loss:.4f}\",\n","#         \"AUC\": f\"{mean_auc:.4f} ± {std_auc:.4f}\"\n","#     })\n","\n","#     print(f\"\\n=== Average Results for reg_weight = {reg_weight} ===\")\n","#     print(f\"Accuracy: {mean_acc:.4f} ± {std_acc:.4f}\")\n","#     print(f\"Precision: {mean_prec:.4f} ± {std_prec:.4f}\")\n","#     print(f\"Recall: {mean_rec:.4f} ± {std_rec:.4f}\")\n","#     print(f\"F1: {mean_f1:.4f} ± {std_f1:.4f}\")\n","#     print(f\"LogLoss: {mean_loss:.4f} ± {std_loss:.4f}\")\n","#     print(f\"AUC: {mean_auc:.4f} ± {std_auc:.4f}\")\n","\n","# # ============================\n","# # Final summary table\n","# # ============================\n","# print(\"\\n\\n========== FINAL SUMMARY TABLE ==========\")\n","# results_df = pd.DataFrame(results_summary)\n","# print(results_df.to_string(index=False))\n"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"n8p12s0K4DQq","executionInfo":{"status":"ok","timestamp":1767155382001,"user_tz":-330,"elapsed":60,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"outputs":[],"source":["# import torch\n","# import torch.nn as nn\n","# import torch.nn.functional as F\n","# import numpy as np\n","# import pandas as pd\n","# from sklearn.model_selection import StratifiedShuffleSplit\n","# from sklearn.metrics import (\n","#     accuracy_score, precision_score, recall_score, f1_score,\n","#     log_loss, roc_auc_score\n","# )\n","\n","# A_tensor = torch.from_numpy(W0).to(device)\n","# hidden_dim = 512\n","# cut = 0\n","# num_epochs = 5000\n","# lr = 1e-4\n","# weight_decay = 1e-4\n","# reg_weights = [0.001, 0.005, 0.009, 0.01, 0.05, 0.09, 0.1, 0.3, 0.5, 0.9, 1, 2, 5, 8]\n","# sss = StratifiedShuffleSplit(n_splits=20, test_size=0.9, random_state=SEED)\n","\n","# results_summary = []\n","\n","# for reg_weight in reg_weights:\n","#     print(f\"\\n================ REG_WEIGHT = {reg_weight} ================\")\n","#     accuracies, precisions, recalls, f1_scores, losses, all_auc = [], [], [], [], [], []\n","\n","#     for fold, (train_idx, test_idx) in enumerate(sss.split(X, y)):\n","#         print(f\"\\n=== Fold {fold+1} ===\")\n","\n","#         cn_idx = np.where(y == 0)[0]\n","#         mci_idx = np.where(y == 1)[0]\n","#         sss_class = StratifiedShuffleSplit(n_splits=20, test_size=0.1, random_state=fold)\n","#         cn_train_idx, cn_test_idx = next(sss_class.split(X[cn_idx], y[cn_idx]))\n","#         mci_train_idx, mci_test_idx = next(sss_class.split(X[mci_idx], y[mci_idx]))\n","\n","#         cn_train = cn_idx[cn_train_idx]\n","#         mci_train = mci_idx[mci_train_idx]\n","#         cn_test = cn_idx[cn_test_idx]\n","#         mci_test = mci_idx[mci_test_idx]\n","\n","#         balanced_train_idx = np.concatenate([cn_train, mci_train])\n","#         test_idx = np.concatenate([cn_test, mci_test])\n","#         np.random.shuffle(balanced_train_idx)\n","#         np.random.shuffle(test_idx)\n","\n","#         print(f\"Train CN: {len(cn_train)}, Train MCI: {len(mci_train)}\")\n","#         print(f\"Test CN: {len(cn_test)}, Test MCI: {len(mci_test)}\")\n","\n","#         node_feats = node_feats_all.to(device)\n","#         edge_index = edge_index_all.to(device)\n","#         y_tensor = torch.from_numpy(y).long().to(device)\n","#         train_idx_t = torch.from_numpy(balanced_train_idx).long().to(device)\n","#         test_idx_t = torch.from_numpy(test_idx).long().to(device)\n","\n","#         N = node_feats.size(0)\n","#         lbl = torch.cat([torch.ones(N, device=device), torch.zeros(N, device=device)])\n","\n","#         model = DGI_with_classifier(num_feats, hidden_dim, n_classes=2, cut=cut).to(device)\n","#         optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n","#         bce_loss = nn.BCEWithLogitsLoss()\n","#         ce_loss = nn.CrossEntropyLoss()\n","\n","#         for epoch in range(1, num_epochs + 1):\n","#             model.train()\n","#             optimizer.zero_grad()\n","#             perm = torch.randperm(N, device=device)\n","#             corrupt = node_feats[perm]\n","#             logits_dgi, embeddings = model(node_feats, corrupt, edge_index)\n","#             dgi_loss = bce_loss(logits_dgi.squeeze(), lbl)\n","#             logits_cls = model.classifier(embeddings)\n","#             train_logits = logits_cls[train_idx_t]\n","#             train_labels = y_tensor[train_idx_t]\n","#             supervised_loss = ce_loss(train_logits, train_labels)\n","#             reg_loss_val = model.Reg_loss(A_tensor, embeddings)\n","#             total_loss = supervised_loss + dgi_loss + reg_weight * reg_loss_val\n","#             total_loss.backward()\n","#             optimizer.step()\n","#             if epoch % 500 == 0 or epoch == 1:\n","#                 model.eval()\n","#                 with torch.no_grad():\n","#                     logits_eval = model.classifier(model.get_embeddings(node_feats, edge_index))\n","#                     preds_train = torch.argmax(logits_eval[train_idx_t], dim=1).cpu().numpy()\n","#                     acc = accuracy_score(train_labels.cpu().numpy(), preds_train)\n","#                 print(f\"Epoch {epoch}: Sup={supervised_loss.item():.4f} | \"\n","#                       f\"DGI={dgi_loss.item():.4f} | Reg={reg_loss_val.item():.4f} | \"\n","#                       f\"Total={total_loss.item():.4f} | TrainAcc={acc:.4f}\")\n","\n","#         model.eval()\n","#         with torch.no_grad():\n","#             emb_final = model.get_embeddings(node_feats, edge_index)\n","#             logits_final = model.classifier(emb_final)\n","#             probs = F.softmax(logits_final, dim=1).cpu().numpy()\n","#             y_pred = np.argmax(probs, axis=1)\n","\n","#         y_test = y[test_idx]\n","#         y_pred_test = y_pred[test_idx]\n","#         y_proba_test = probs[test_idx, 1]\n","\n","#         acc = accuracy_score(y_test, y_pred_test)\n","#         prec = precision_score(y_test, y_pred_test, zero_division=0)\n","#         rec = recall_score(y_test, y_pred_test, zero_division=0)\n","#         f1 = f1_score(y_test, y_pred_test, zero_division=0)\n","#         loss_val = log_loss(y_test, y_proba_test)\n","#         auc_score = roc_auc_score(y_test, y_proba_test)\n","\n","#         accuracies.append(acc)\n","#         precisions.append(prec)\n","#         recalls.append(rec)\n","#         f1_scores.append(f1)\n","#         losses.append(loss_val)\n","#         all_auc.append(auc_score)\n","#         print(f\"Fold {fold+1} → Acc={acc:.4f} Prec={prec:.4f} Rec={rec:.4f} \"\n","#               f\"F1={f1:.4f} AUC={auc_score:.4f}\")\n","\n","#     mean_acc, std_acc = np.mean(accuracies), np.std(accuracies)\n","#     mean_prec, std_prec = np.mean(precisions), np.std(precisions)\n","#     mean_rec, std_rec = np.mean(recalls), np.std(recalls)\n","#     mean_f1, std_f1 = np.mean(f1_scores), np.std(f1_scores)\n","#     mean_loss, std_loss = np.mean(losses), np.std(losses)\n","#     mean_auc, std_auc = np.mean(all_auc), np.std(all_auc)\n","\n","#     results_summary.append({\n","#         \"Reg_Weight\": reg_weight,\n","#         \"Accuracy\": f\"{mean_acc:.4f} ± {std_acc:.4f}\",\n","#         \"Precision\": f\"{mean_prec:.4f} ± {std_prec:.4f}\",\n","#         \"Recall\": f\"{mean_rec:.4f} ± {std_rec:.4f}\",\n","#         \"F1\": f\"{mean_f1:.4f} ± {std_f1:.4f}\",\n","#         \"LogLoss\": f\"{mean_loss:.4f} ± {std_loss:.4f}\",\n","#         \"AUC\": f\"{mean_auc:.4f} ± {std_auc:.4f}\"\n","#     })\n","\n","#     print(f\"\\n=== Average Results for reg_weight = {reg_weight} ===\")\n","#     print(f\"Accuracy: {mean_acc:.4f} ± {std_acc:.4f}\")\n","#     print(f\"Precision: {mean_prec:.4f} ± {std_prec:.4f}\")\n","#     print(f\"Recall: {mean_rec:.4f} ± {std_rec:.4f}\")\n","#     print(f\"F1: {mean_f1:.4f} ± {std_f1:.4f}\")\n","#     print(f\"LogLoss: {mean_loss:.4f} ± {std_loss:.4f}\")\n","#     print(f\"AUC: {mean_auc:.4f} ± {std_auc:.4f}\")\n","\n","# # ============================\n","# # Final summary table\n","# # ============================\n","# print(\"\\n\\n========== FINAL SUMMARY TABLE ==========\")\n","# results_df = pd.DataFrame(results_summary)\n","# print(results_df.to_string(index=False))\n"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"6K2FkKgh4DQr","executionInfo":{"status":"ok","timestamp":1767155382005,"user_tz":-330,"elapsed":2,"user":{"displayName":"Saurabh Shigwan","userId":"13392925412739105503"}}},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.18"}},"nbformat":4,"nbformat_minor":0}
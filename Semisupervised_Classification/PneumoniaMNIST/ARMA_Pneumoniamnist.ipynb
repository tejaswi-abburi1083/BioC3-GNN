{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZhoWIduU0285",
    "outputId": "b1598b8b-7794-4586-80b7-940fb286d5e8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as nnFn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import ARMAConv\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, log_loss\n",
    ")\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import ARMAConv\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hgv5wIW403rR",
    "outputId": "301dac59-8db0-47bf-8374-0bf3ceaa8fb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw images: torch.Size([5856, 3, 224, 224]), Labels: torch.Size([5856])\n"
     ]
    }
   ],
   "source": [
    "data = np.load('pneumoniamnist_224.npz', allow_pickle=True)\n",
    "all_images = np.concatenate([data['train_images'], data['val_images'], data['test_images']], axis=0)\n",
    "all_labels = np.concatenate([data['train_labels'], data['val_labels'], data['test_labels']], axis=0).squeeze()\n",
    "\n",
    "# Convert to 3-channel RGB\n",
    "images = all_images.astype(np.float32) / 255.0\n",
    "images = np.repeat(images[:, None, :, :], 3, axis=1)  # (N, 3, 224, 224)\n",
    "X_torch = torch.tensor(images)\n",
    "y_torch = torch.tensor(all_labels).long()\n",
    "print(f\"Raw images: {X_torch.shape}, Labels: {y_torch.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU Name: NVIDIA A100 80GB PCIe\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "RravF_31Ppdw"
   },
   "outputs": [],
   "source": [
    "class0_idx = [i for i in range(len(y_torch)) if y_torch[i] == 0]\n",
    "class1_idx = [i for i in range(len(y_torch)) if y_torch[i] == 1]\n",
    "\n",
    "random.seed(42)\n",
    "sampled_class0 = random.sample(class0_idx, min(2000, len(class0_idx)))\n",
    "sampled_class1 = random.sample(class1_idx, min(2000, len(class1_idx)))\n",
    "\n",
    "selected_indices = sampled_class0 + sampled_class1\n",
    "random.shuffle(selected_indices)\n",
    "\n",
    "subset_dataset = Subset(TensorDataset(X_torch, y_torch), selected_indices)\n",
    "subset_loader = DataLoader(subset_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zhfaxAc3PrBv",
    "outputId": "067314de-6ae8-4986-f0ce-4a6db901ecd6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/va797/tmp_pyg118/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/data/home/va797/tmp_pyg118/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape: (3583, 512) Label shape: (3583,)\n"
     ]
    }
   ],
   "source": [
    "resnet = models.resnet18(pretrained=True)\n",
    "resnet.fc = nn.Identity()\n",
    "resnet = resnet.to(device)\n",
    "resnet.eval()\n",
    "\n",
    "resnet_feats = []\n",
    "y_list = []\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in subset_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        feats = resnet(imgs)\n",
    "        resnet_feats.append(feats.cpu())\n",
    "        y_list.extend(labels.cpu().tolist())\n",
    "\n",
    "features = torch.cat(resnet_feats, dim=0).numpy().astype(np.float32)  # shape (N, feat_dim)\n",
    "y_labels = np.array(y_list).astype(np.float32)\n",
    "print(\"Feature shape:\", features.shape, \"Label shape:\", y_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "bdbHMxznPMMz"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as nnFn\n",
    "from torch_geometric.nn import ARMAConv\n",
    "\n",
    "class ARMA_Supervised(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, device, num_stacks=1, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # Only one ARMAConv layer\n",
    "        self.conv1 = ARMAConv(input_dim, hidden_dim,\n",
    "                              num_stacks=num_stacks, num_layers=num_layers,\n",
    "                              shared_weights=True, dropout=0.2)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        # Activation function\n",
    "        activations = {\n",
    "            \"SELU\": nnFn.selu,\n",
    "            \"SiLU\": nnFn.silu,\n",
    "            \"GELU\": nnFn.gelu,\n",
    "            \"RELU\": nnFn.relu,\n",
    "            \"ELU\": nnFn.elu,\n",
    "            \"PReLU\": nnFn.prelu,\n",
    "            \"LeakyReLU\": nnFn.leaky_relu\n",
    "        }\n",
    "        self.act = activations.get(\"RELU\", nnFn.relu)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        # Single layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        logits = self.fc(x)\n",
    "        return logits\n",
    "\n",
    "    def modularity_loss(self, A, logits):\n",
    "        S = nnFn.softmax(logits, dim=1)\n",
    "        d = torch.sum(A, dim=1)\n",
    "        m = torch.sum(A)\n",
    "        B = A - torch.outer(d, d) / (2 * m)\n",
    "\n",
    "        modularity_term = (-1 / (2 * m)) * torch.trace(S.T @ B @ S)\n",
    "\n",
    "        I_S = torch.eye(S.shape[1], device=self.device)\n",
    "        k = torch.norm(I_S)\n",
    "        n = S.shape[0]\n",
    "        collapse_reg = (torch.sqrt(k) / n) * torch.norm(torch.sum(S, dim=0), p='fro') - 1\n",
    "        entropy_reg = -torch.mean(torch.sum(S * torch.log(S + 1e-9), dim=1))\n",
    "\n",
    "        return modularity_term + 0.1 * collapse_reg + 0.01 * entropy_reg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8AUYk2fh0_iR"
   },
   "outputs": [],
   "source": [
    "def create_adj(F, alpha=1):\n",
    "    F_norm = F / np.linalg.norm(F, axis=1, keepdims=True)\n",
    "    W = np.dot(F_norm, F_norm.T)\n",
    "    W = np.where(W >= alpha, 1, 0).astype(np.float32)\n",
    "    W = W / W.max()\n",
    "    return W\n",
    "\n",
    "def load_data(adj, node_feats):\n",
    "    node_feats = torch.from_numpy(node_feats).float()\n",
    "    edge_index = torch.from_numpy(np.array(np.nonzero((adj > 0))))\n",
    "    return Data(x=node_feats, edge_index=edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "5xD2P7ic1B65"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "alpha = 0.9\n",
    "feats_dim = features.shape[1]\n",
    "hidden_dim = 256\n",
    "num_classes = 2\n",
    "num_epochs = 2000\n",
    "lr = 0.0001\n",
    "weight_decay = 1e-4\n",
    "batch_print_freq = 500\n",
    "lambda_mod =  0.001  #0.2 #0.001  # weight for modularity loss\n",
    "# lambda_sup = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-mfsholh1Erx",
    "outputId": "04fe4c3e-66c1-4823-8e69-974731a95145"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[3583, 512], edge_index=[2, 1642037])\n"
     ]
    }
   ],
   "source": [
    "W = create_adj(features, alpha)\n",
    "data = load_data(W, features).to(device)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CBy-hReA1Hqa",
    "outputId": "918da329-ec88-4f4a-806f-7b980e79f927"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1 ===\n",
      "Fold 1 Epoch 1: Loss=0.742786 | TrainAcc=0.5531\n",
      "Fold 1 Epoch 500: Loss=0.059521 | TrainAcc=0.9721\n",
      "Fold 1 Epoch 1000: Loss=0.021893 | TrainAcc=0.9972\n",
      "Fold 1 Epoch 1500: Loss=0.011881 | TrainAcc=1.0000\n",
      "Fold 1 Epoch 2000: Loss=0.012379 | TrainAcc=0.9944\n",
      "Fold 1 → Acc=0.9398 | Prec=0.9573 | Rec=0.9339 | F1=0.9454 | AUC=0.9858 | CE Loss=0.2281\n",
      "\n",
      "=== Fold 2 ===\n",
      "Fold 2 Epoch 1: Loss=0.878341 | TrainAcc=0.5140\n",
      "Fold 2 Epoch 500: Loss=0.061693 | TrainAcc=0.9777\n",
      "Fold 2 Epoch 1000: Loss=0.030698 | TrainAcc=0.9888\n",
      "Fold 2 Epoch 1500: Loss=0.015907 | TrainAcc=1.0000\n",
      "Fold 2 Epoch 2000: Loss=0.015814 | TrainAcc=0.9972\n",
      "Fold 2 → Acc=0.9358 | Prec=0.9364 | Rec=0.9494 | F1=0.9429 | AUC=0.9830 | CE Loss=0.2675\n",
      "\n",
      "=== Fold 3 ===\n",
      "Fold 3 Epoch 1: Loss=0.779951 | TrainAcc=0.4693\n",
      "Fold 3 Epoch 500: Loss=0.080536 | TrainAcc=0.9749\n",
      "Fold 3 Epoch 1000: Loss=0.037646 | TrainAcc=0.9944\n",
      "Fold 3 Epoch 1500: Loss=0.019973 | TrainAcc=0.9972\n",
      "Fold 3 Epoch 2000: Loss=0.008963 | TrainAcc=1.0000\n",
      "Fold 3 → Acc=0.9392 | Prec=0.9470 | Rec=0.9439 | F1=0.9455 | AUC=0.9850 | CE Loss=0.2163\n",
      "\n",
      "=== Fold 4 ===\n",
      "Fold 4 Epoch 1: Loss=0.782640 | TrainAcc=0.5168\n",
      "Fold 4 Epoch 500: Loss=0.075982 | TrainAcc=0.9777\n",
      "Fold 4 Epoch 1000: Loss=0.025066 | TrainAcc=0.9888\n",
      "Fold 4 Epoch 1500: Loss=0.019803 | TrainAcc=0.9944\n",
      "Fold 4 Epoch 2000: Loss=0.007517 | TrainAcc=1.0000\n",
      "Fold 4 → Acc=0.9312 | Prec=0.9433 | Rec=0.9328 | F1=0.9380 | AUC=0.9819 | CE Loss=0.2419\n",
      "\n",
      "=== Fold 5 ===\n",
      "Fold 5 Epoch 1: Loss=0.739997 | TrainAcc=0.5140\n",
      "Fold 5 Epoch 500: Loss=0.054095 | TrainAcc=0.9832\n",
      "Fold 5 Epoch 1000: Loss=0.028029 | TrainAcc=0.9916\n",
      "Fold 5 Epoch 1500: Loss=0.015554 | TrainAcc=0.9972\n",
      "Fold 5 Epoch 2000: Loss=0.009037 | TrainAcc=0.9972\n",
      "Fold 5 → Acc=0.9367 | Prec=0.9458 | Rec=0.9406 | F1=0.9432 | AUC=0.9823 | CE Loss=0.2455\n",
      "\n",
      "=== Fold 6 ===\n",
      "Fold 6 Epoch 1: Loss=0.774533 | TrainAcc=0.4246\n",
      "Fold 6 Epoch 500: Loss=0.073045 | TrainAcc=0.9777\n",
      "Fold 6 Epoch 1000: Loss=0.035212 | TrainAcc=0.9888\n",
      "Fold 6 Epoch 1500: Loss=0.028710 | TrainAcc=0.9888\n",
      "Fold 6 Epoch 2000: Loss=0.014196 | TrainAcc=0.9944\n",
      "Fold 6 → Acc=0.9457 | Prec=0.9732 | Rec=0.9283 | F1=0.9502 | AUC=0.9884 | CE Loss=0.1913\n",
      "\n",
      "=== Fold 7 ===\n",
      "Fold 7 Epoch 1: Loss=1.214438 | TrainAcc=0.5559\n",
      "Fold 7 Epoch 500: Loss=0.098433 | TrainAcc=0.9637\n",
      "Fold 7 Epoch 1000: Loss=0.054330 | TrainAcc=0.9832\n",
      "Fold 7 Epoch 1500: Loss=0.026736 | TrainAcc=0.9944\n",
      "Fold 7 Epoch 2000: Loss=0.019488 | TrainAcc=0.9944\n",
      "Fold 7 → Acc=0.9420 | Prec=0.9564 | Rec=0.9389 | F1=0.9476 | AUC=0.9851 | CE Loss=0.1929\n",
      "\n",
      "=== Fold 8 ===\n",
      "Fold 8 Epoch 1: Loss=1.000325 | TrainAcc=0.4469\n",
      "Fold 8 Epoch 500: Loss=0.052350 | TrainAcc=0.9860\n",
      "Fold 8 Epoch 1000: Loss=0.021770 | TrainAcc=0.9944\n",
      "Fold 8 Epoch 1500: Loss=0.007654 | TrainAcc=1.0000\n",
      "Fold 8 Epoch 2000: Loss=0.005244 | TrainAcc=1.0000\n",
      "Fold 8 → Acc=0.9377 | Prec=0.9259 | Rec=0.9656 | F1=0.9453 | AUC=0.9839 | CE Loss=0.2835\n",
      "\n",
      "=== Fold 9 ===\n",
      "Fold 9 Epoch 1: Loss=0.783373 | TrainAcc=0.4665\n",
      "Fold 9 Epoch 500: Loss=0.023893 | TrainAcc=0.9916\n",
      "Fold 9 Epoch 1000: Loss=0.009480 | TrainAcc=0.9972\n",
      "Fold 9 Epoch 1500: Loss=0.005855 | TrainAcc=1.0000\n",
      "Fold 9 Epoch 2000: Loss=0.001602 | TrainAcc=1.0000\n",
      "Fold 9 → Acc=0.9309 | Prec=0.9368 | Rec=0.9394 | F1=0.9381 | AUC=0.9805 | CE Loss=0.3613\n",
      "\n",
      "=== Fold 10 ===\n",
      "Fold 10 Epoch 1: Loss=0.914746 | TrainAcc=0.4525\n",
      "Fold 10 Epoch 500: Loss=0.051880 | TrainAcc=0.9832\n",
      "Fold 10 Epoch 1000: Loss=0.020467 | TrainAcc=0.9944\n",
      "Fold 10 Epoch 1500: Loss=0.015794 | TrainAcc=0.9972\n",
      "Fold 10 Epoch 2000: Loss=0.008988 | TrainAcc=1.0000\n",
      "Fold 10 → Acc=0.9501 | Prec=0.9510 | Rec=0.9600 | F1=0.9555 | AUC=0.9858 | CE Loss=0.2114\n",
      "\n",
      "=== Fold 11 ===\n",
      "Fold 11 Epoch 1: Loss=0.792059 | TrainAcc=0.5056\n",
      "Fold 11 Epoch 500: Loss=0.058625 | TrainAcc=0.9804\n",
      "Fold 11 Epoch 1000: Loss=0.021820 | TrainAcc=1.0000\n",
      "Fold 11 Epoch 1500: Loss=0.010389 | TrainAcc=1.0000\n",
      "Fold 11 Epoch 2000: Loss=0.008667 | TrainAcc=1.0000\n",
      "Fold 11 → Acc=0.9349 | Prec=0.9590 | Rec=0.9228 | F1=0.9405 | AUC=0.9823 | CE Loss=0.2396\n",
      "\n",
      "=== Fold 12 ===\n",
      "Fold 12 Epoch 1: Loss=0.973748 | TrainAcc=0.5615\n",
      "Fold 12 Epoch 500: Loss=0.027133 | TrainAcc=0.9888\n",
      "Fold 12 Epoch 1000: Loss=0.010324 | TrainAcc=1.0000\n",
      "Fold 12 Epoch 1500: Loss=0.010090 | TrainAcc=0.9972\n",
      "Fold 12 Epoch 2000: Loss=0.007908 | TrainAcc=0.9972\n",
      "Fold 12 → Acc=0.9405 | Prec=0.9674 | Rec=0.9244 | F1=0.9455 | AUC=0.9851 | CE Loss=0.2920\n",
      "\n",
      "=== Fold 13 ===\n",
      "Fold 13 Epoch 1: Loss=0.721801 | TrainAcc=0.5503\n",
      "Fold 13 Epoch 500: Loss=0.081597 | TrainAcc=0.9749\n",
      "Fold 13 Epoch 1000: Loss=0.032130 | TrainAcc=0.9916\n",
      "Fold 13 Epoch 1500: Loss=0.019978 | TrainAcc=0.9944\n",
      "Fold 13 Epoch 2000: Loss=0.018646 | TrainAcc=0.9916\n",
      "Fold 13 → Acc=0.9442 | Prec=0.9576 | Rec=0.9417 | F1=0.9496 | AUC=0.9866 | CE Loss=0.2049\n",
      "\n",
      "=== Fold 14 ===\n",
      "Fold 14 Epoch 1: Loss=0.957587 | TrainAcc=0.4860\n",
      "Fold 14 Epoch 500: Loss=0.091770 | TrainAcc=0.9665\n",
      "Fold 14 Epoch 1000: Loss=0.035606 | TrainAcc=0.9832\n",
      "Fold 14 Epoch 1500: Loss=0.028603 | TrainAcc=0.9916\n",
      "Fold 14 Epoch 2000: Loss=0.015655 | TrainAcc=0.9972\n",
      "Fold 14 → Acc=0.9343 | Prec=0.9563 | Rec=0.9244 | F1=0.9401 | AUC=0.9819 | CE Loss=0.2519\n",
      "\n",
      "=== Fold 15 ===\n",
      "Fold 15 Epoch 1: Loss=0.759296 | TrainAcc=0.5056\n",
      "Fold 15 Epoch 500: Loss=0.049374 | TrainAcc=0.9832\n",
      "Fold 15 Epoch 1000: Loss=0.026860 | TrainAcc=0.9972\n",
      "Fold 15 Epoch 1500: Loss=0.012611 | TrainAcc=0.9944\n",
      "Fold 15 Epoch 2000: Loss=0.013384 | TrainAcc=0.9972\n",
      "Fold 15 → Acc=0.9349 | Prec=0.9481 | Rec=0.9344 | F1=0.9412 | AUC=0.9830 | CE Loss=0.2830\n",
      "\n",
      "=== Fold 16 ===\n",
      "Fold 16 Epoch 1: Loss=0.847949 | TrainAcc=0.5028\n",
      "Fold 16 Epoch 500: Loss=0.061843 | TrainAcc=0.9804\n",
      "Fold 16 Epoch 1000: Loss=0.033465 | TrainAcc=0.9888\n",
      "Fold 16 Epoch 1500: Loss=0.014535 | TrainAcc=1.0000\n",
      "Fold 16 Epoch 2000: Loss=0.007271 | TrainAcc=1.0000\n",
      "Fold 16 → Acc=0.9377 | Prec=0.9525 | Rec=0.9350 | F1=0.9437 | AUC=0.9843 | CE Loss=0.2284\n",
      "\n",
      "=== Fold 17 ===\n",
      "Fold 17 Epoch 1: Loss=0.921342 | TrainAcc=0.4274\n",
      "Fold 17 Epoch 500: Loss=0.075773 | TrainAcc=0.9749\n",
      "Fold 17 Epoch 1000: Loss=0.024932 | TrainAcc=0.9944\n",
      "Fold 17 Epoch 1500: Loss=0.028438 | TrainAcc=0.9916\n",
      "Fold 17 Epoch 2000: Loss=0.014130 | TrainAcc=0.9944\n",
      "Fold 17 → Acc=0.9491 | Prec=0.9685 | Rec=0.9394 | F1=0.9538 | AUC=0.9886 | CE Loss=0.2037\n",
      "\n",
      "=== Fold 18 ===\n",
      "Fold 18 Epoch 1: Loss=0.813220 | TrainAcc=0.4804\n",
      "Fold 18 Epoch 500: Loss=0.078113 | TrainAcc=0.9777\n",
      "Fold 18 Epoch 1000: Loss=0.030131 | TrainAcc=0.9944\n",
      "Fold 18 Epoch 1500: Loss=0.019153 | TrainAcc=0.9944\n",
      "Fold 18 Epoch 2000: Loss=0.012546 | TrainAcc=1.0000\n",
      "Fold 18 → Acc=0.9436 | Prec=0.9682 | Rec=0.9294 | F1=0.9484 | AUC=0.9855 | CE Loss=0.2188\n",
      "\n",
      "=== Fold 19 ===\n",
      "Fold 19 Epoch 1: Loss=0.955005 | TrainAcc=0.5363\n",
      "Fold 19 Epoch 500: Loss=0.066965 | TrainAcc=0.9721\n",
      "Fold 19 Epoch 1000: Loss=0.032257 | TrainAcc=0.9916\n",
      "Fold 19 Epoch 1500: Loss=0.019331 | TrainAcc=0.9944\n",
      "Fold 19 Epoch 2000: Loss=0.018006 | TrainAcc=0.9972\n",
      "Fold 19 → Acc=0.9358 | Prec=0.9677 | Rec=0.9156 | F1=0.9409 | AUC=0.9837 | CE Loss=0.2573\n",
      "\n",
      "=== Fold 20 ===\n",
      "Fold 20 Epoch 1: Loss=0.716278 | TrainAcc=0.5391\n",
      "Fold 20 Epoch 500: Loss=0.071009 | TrainAcc=0.9749\n",
      "Fold 20 Epoch 1000: Loss=0.021860 | TrainAcc=1.0000\n",
      "Fold 20 Epoch 1500: Loss=0.014597 | TrainAcc=0.9972\n",
      "Fold 20 Epoch 2000: Loss=0.011034 | TrainAcc=0.9944\n",
      "Fold 20 → Acc=0.9464 | Prec=0.9568 | Rec=0.9467 | F1=0.9517 | AUC=0.9862 | CE Loss=0.2092\n",
      "\n",
      "=== Average Results Across 20 Folds ===\n",
      "Accuracy:  0.9395 ± 0.0054\n",
      "Precision: 0.9538 ± 0.0120\n",
      "Recall:    0.9373 ± 0.0119\n",
      "F1-score:  0.9454 ± 0.0049\n",
      "AUC:       0.9844 ± 0.0021\n",
      "CE Loss:   0.2414 ± 0.0403\n"
     ]
    }
   ],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=20, test_size=0.9, random_state=42)\n",
    "\n",
    "accuracies, precisions, recalls, f1_scores, aucs, ce_losses = [], [], [], [], [], []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(sss.split(features, y_labels.astype(np.int64)), start=1):\n",
    "    print(f\"\\n=== Fold {fold} ===\")\n",
    "\n",
    "    train_idx_t = torch.from_numpy(train_idx).long().to(device)\n",
    "    y_train_tensor = torch.from_numpy(y_labels[train_idx]).long().to(device)\n",
    "\n",
    "    model = ARMA_Supervised(feats_dim, hidden_dim, num_classes, device).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(data)\n",
    "        loss = ce_loss(logits[train_idx_t], y_train_tensor)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % batch_print_freq == 0 or epoch == 1:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                preds_train = logits[train_idx_t].argmax(dim=1)\n",
    "                acc_train = accuracy_score(y_train_tensor.cpu(), preds_train.cpu())\n",
    "            print(f\"Fold {fold} Epoch {epoch}: Loss={loss.item():.6f} | TrainAcc={acc_train:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data)\n",
    "        preds = out.argmax(dim=1).cpu().numpy()\n",
    "        probs = torch.softmax(out, dim=1)[:, 1].cpu().numpy()\n",
    "\n",
    "    y_test = y_labels[test_idx]\n",
    "    y_pred_test = preds[test_idx]\n",
    "    y_pred_test = preds[test_idx]\n",
    "    y_prob_test = probs[test_idx]\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred_test)\n",
    "    prec = precision_score(y_test, y_pred_test, zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred_test, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred_test, zero_division=0)\n",
    "    auc = roc_auc_score(y_test, y_prob_test)\n",
    "    ce = log_loss(y_test, y_prob_test)\n",
    "\n",
    "    accuracies.append(acc)\n",
    "    precisions.append(prec)\n",
    "    recalls.append(rec)\n",
    "    f1_scores.append(f1)\n",
    "    aucs.append(auc)\n",
    "    ce_losses.append(ce)\n",
    "\n",
    "    print(f\"Fold {fold} → \"\n",
    "          f\"Acc={acc:.4f} | Prec={prec:.4f} | Rec={rec:.4f} | \"\n",
    "          f\"F1={f1:.4f} | AUC={auc:.4f} | CE Loss={ce:.4f}\")\n",
    "\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n=== Average Results Across 20 Folds ===\")\n",
    "print(f\"Accuracy:  {np.mean(accuracies):.4f} \\u00b1 {np.std(accuracies):.4f}\")\n",
    "print(f\"Precision: {np.mean(precisions):.4f} \\u00b1 {np.std(precisions):.4f}\")\n",
    "print(f\"Recall:    {np.mean(recalls):.4f} \\u00b1 {np.std(recalls):.4f}\")\n",
    "print(f\"F1-score:  {np.mean(f1_scores):.4f} \\u00b1 {np.std(f1_scores):.4f}\")\n",
    "print(f\"AUC:       {np.mean(aucs):.4f} \\u00b1 {np.std(aucs):.4f}\")\n",
    "print(f\"CE Loss:   {np.mean(ce_losses):.4f} \\u00b1 {np.std(ce_losses):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZhoWIduU0285"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as nnFn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import ARMAConv\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, log_loss\n",
    ")\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import ARMAConv\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QMcTVrCjjNZc",
    "outputId": "631186d9-bcbf-4b66-99e4-377e7b2db34c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU Name: NVIDIA A100 80GB PCIe\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hgv5wIW403rR",
    "outputId": "5e469bf4-1878-49e7-e85d-0becdcbefb17"
   },
   "outputs": [],
   "source": [
    "# data = np.load('/home/snu/Downloads/breastmnist_224.npz', allow_pickle=True)\n",
    "# all_images = np.concatenate([data['train_images'], data['val_images'], data['test_images']], axis=0)\n",
    "# all_labels = np.concatenate([data['train_labels'], data['val_labels'], data['test_labels']], axis=0).squeeze()\n",
    "\n",
    "# # Convert to 3-channel RGB\n",
    "# images = all_images.astype(np.float32) / 255.0\n",
    "# images = np.repeat(images[:, None, :, :], 3, axis=1)  # (N, 3, 224, 224)\n",
    "# X_torch = torch.tensor(images)\n",
    "# y_torch = torch.tensor(all_labels).long()\n",
    "# print(f\"Raw images: {X_torch.shape}, Labels: {y_torch.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cepApjvKiSCo",
    "outputId": "2f0cf63c-0e2d-43fe-ee6c-aebc0b25f8b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw images: torch.Size([5856, 3, 224, 224]), Labels: torch.Size([5856])\n"
     ]
    }
   ],
   "source": [
    "data = np.load('pneumoniamnist_224.npz', allow_pickle=True)\n",
    "all_images = np.concatenate([data['train_images'], data['val_images'], data['test_images']], axis=0)\n",
    "all_labels = np.concatenate([data['train_labels'], data['val_labels'], data['test_labels']], axis=0).squeeze()\n",
    "\n",
    "# Convert to 3-channel RGB\n",
    "images = all_images.astype(np.float32) / 255.0\n",
    "images = np.repeat(images[:, None, :, :], 3, axis=1)  # (N, 3, 224, 224)\n",
    "X_torch = torch.tensor(images)\n",
    "y_torch = torch.tensor(all_labels).long()\n",
    "print(f\"Raw images: {X_torch.shape}, Labels: {y_torch.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "8SWzeJfy-NHL"
   },
   "outputs": [],
   "source": [
    "class0_idx = [i for i in range(len(y_torch)) if y_torch[i] == 0]\n",
    "class1_idx = [i for i in range(len(y_torch)) if y_torch[i] == 1]\n",
    "\n",
    "random.seed(42)\n",
    "sampled_class0 = random.sample(class0_idx, min(2000, len(class0_idx)))\n",
    "sampled_class1 = random.sample(class1_idx, min(2000, len(class1_idx)))\n",
    "\n",
    "selected_indices = sampled_class0 + sampled_class1\n",
    "random.shuffle(selected_indices)\n",
    "\n",
    "subset_dataset = Subset(TensorDataset(X_torch, y_torch), selected_indices)\n",
    "subset_loader = DataLoader(subset_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LB9Nehldij_q",
    "outputId": "3c69b506-b301-4edc-ad3f-f0b04498c668"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/home/va797/tmp_pyg118/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/data/home/va797/tmp_pyg118/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape: (3583, 512) Label shape: (3583,)\n"
     ]
    }
   ],
   "source": [
    "resnet = models.resnet18(pretrained=True)\n",
    "resnet.fc = nn.Identity()\n",
    "resnet = resnet.to(device)\n",
    "resnet.eval()\n",
    "\n",
    "resnet_feats = []\n",
    "y_list = []\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in subset_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        feats = resnet(imgs)\n",
    "        resnet_feats.append(feats.cpu())\n",
    "        y_list.extend(labels.cpu().tolist())\n",
    "\n",
    "features = torch.cat(resnet_feats, dim=0).numpy().astype(np.float32)  # shape (N, feat_dim)\n",
    "y_labels = np.array(y_list).astype(np.float32)\n",
    "print(\"Feature shape:\", features.shape, \"Label shape:\", y_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "mTPzJR5gQzXf"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as nnFn\n",
    "from torch_geometric.nn import ARMAConv\n",
    "\n",
    "class ARMA_SemiSupervised(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, device, num_stacks=1, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # Only one ARMAConv layer\n",
    "        self.conv1 = ARMAConv(input_dim, hidden_dim,\n",
    "                              num_stacks=num_stacks, num_layers=num_layers,\n",
    "                              shared_weights=True, dropout=0.2)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        # Activation function\n",
    "        activations = {\n",
    "            \"SELU\": nnFn.selu,\n",
    "            \"SiLU\": nnFn.silu,\n",
    "            \"GELU\": nnFn.gelu,\n",
    "            \"RELU\": nnFn.relu,\n",
    "            \"ELU\": nnFn.elu,\n",
    "            \"PReLU\": nnFn.prelu,\n",
    "            \"LeakyReLU\": nnFn.leaky_relu\n",
    "        }\n",
    "        self.act = activations.get(\"RELU\", nnFn.relu)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        # Single layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        logits = self.fc(x)\n",
    "        return logits\n",
    "\n",
    "    def modularity_loss(self, A, logits):\n",
    "        S = nnFn.softmax(logits, dim=1)\n",
    "        d = torch.sum(A, dim=1)\n",
    "        m = torch.sum(A)\n",
    "        B = A - torch.outer(d, d) / (2 * m)\n",
    "\n",
    "        modularity_term = (-1 / (2 * m)) * torch.trace(S.T @ B @ S)\n",
    "\n",
    "        I_S = torch.eye(S.shape[1], device=self.device)\n",
    "        k = torch.norm(I_S)\n",
    "        n = S.shape[0]\n",
    "        collapse_reg = (torch.sqrt(k) / n) * torch.norm(torch.sum(S, dim=0), p='fro') - 1\n",
    "        entropy_reg = -torch.mean(torch.sum(S * torch.log(S + 1e-9), dim=1))\n",
    "\n",
    "        return modularity_term + 0.1 * collapse_reg + 0.01 * entropy_reg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "8AUYk2fh0_iR"
   },
   "outputs": [],
   "source": [
    "def create_adj(F, alpha=1):\n",
    "    F_norm = F / np.linalg.norm(F, axis=1, keepdims=True)\n",
    "    W = np.dot(F_norm, F_norm.T)\n",
    "    W = np.where(W >= alpha, 1, 0).astype(np.float32)\n",
    "    W = W / W.max()\n",
    "    return W\n",
    "\n",
    "def load_data(adj, node_feats):\n",
    "    node_feats = torch.from_numpy(node_feats).float()\n",
    "    edge_index = torch.from_numpy(np.array(np.nonzero((adj > 0))))\n",
    "    return Data(x=node_feats, edge_index=edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "3TfsmjGLumYY"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "alpha = 0.9\n",
    "feats_dim = features.shape[1]\n",
    "hidden_dim = 256\n",
    "num_classes = 2\n",
    "num_epochs = 2000\n",
    "lr = 0.0001\n",
    "weight_decay = 1e-4\n",
    "batch_print_freq = 500\n",
    "lambda_mod = 0.1  #0.2 #0.001  # weight for modularity loss\n",
    "# lambda_sup = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-mfsholh1Erx",
    "outputId": "12772b53-94ab-4b57-aaca-d9d0e0508aa1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[3583, 512], edge_index=[2, 1642037])\n"
     ]
    }
   ],
   "source": [
    "W = create_adj(features, alpha)\n",
    "data = load_data(W, features).to(device)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CBy-hReA1Hqa",
    "outputId": "a072b047-8747-4181-cda9-7eb42ea0f0bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1 ===\n",
      "Fold 1 Epoch 1: TotalLoss=1.207757 | Sup=1.227604 | Unsup=-0.198469 | TrainAcc=0.4469\n",
      "Fold 1 Epoch 500: TotalLoss=0.032692 | Sup=0.062821 | Unsup=-0.301290 | TrainAcc=0.9749\n",
      "Fold 1 Epoch 1000: TotalLoss=0.004902 | Sup=0.035261 | Unsup=-0.303587 | TrainAcc=0.9888\n",
      "Fold 1 Epoch 1500: TotalLoss=-0.010118 | Sup=0.020522 | Unsup=-0.306400 | TrainAcc=0.9972\n",
      "Fold 1 Epoch 2000: TotalLoss=-0.022606 | Sup=0.008174 | Unsup=-0.307795 | TrainAcc=1.0000\n",
      "Fold 1 → Acc=0.9380 | Prec=0.9598 | Rec=0.9278 | F1=0.9435 | AUC=0.9857 | CE Loss=0.2313\n",
      "\n",
      "=== Fold 2 ===\n",
      "Fold 2 Epoch 1: TotalLoss=0.734119 | Sup=0.747978 | Unsup=-0.138590 | TrainAcc=0.5251\n",
      "Fold 2 Epoch 500: TotalLoss=0.028507 | Sup=0.058345 | Unsup=-0.298378 | TrainAcc=0.9804\n",
      "Fold 2 Epoch 1000: TotalLoss=-0.002587 | Sup=0.027578 | Unsup=-0.301653 | TrainAcc=0.9944\n",
      "Fold 2 Epoch 1500: TotalLoss=-0.012624 | Sup=0.017604 | Unsup=-0.302275 | TrainAcc=0.9916\n",
      "Fold 2 Epoch 2000: TotalLoss=-0.021421 | Sup=0.008987 | Unsup=-0.304078 | TrainAcc=1.0000\n",
      "Fold 2 → Acc=0.9364 | Prec=0.9351 | Rec=0.9522 | F1=0.9436 | AUC=0.9830 | CE Loss=0.2760\n",
      "\n",
      "=== Fold 3 ===\n",
      "Fold 3 Epoch 1: TotalLoss=0.896479 | Sup=0.912640 | Unsup=-0.161612 | TrainAcc=0.5112\n",
      "Fold 3 Epoch 500: TotalLoss=0.063166 | Sup=0.092698 | Unsup=-0.295316 | TrainAcc=0.9665\n",
      "Fold 3 Epoch 1000: TotalLoss=0.003334 | Sup=0.033474 | Unsup=-0.301406 | TrainAcc=0.9916\n",
      "Fold 3 Epoch 1500: TotalLoss=-0.009681 | Sup=0.020936 | Unsup=-0.306173 | TrainAcc=0.9972\n",
      "Fold 3 Epoch 2000: TotalLoss=-0.011599 | Sup=0.018839 | Unsup=-0.304386 | TrainAcc=0.9944\n",
      "Fold 3 → Acc=0.9398 | Prec=0.9496 | Rec=0.9422 | F1=0.9459 | AUC=0.9852 | CE Loss=0.2079\n",
      "\n",
      "=== Fold 4 ===\n",
      "Fold 4 Epoch 1: TotalLoss=0.771250 | Sup=0.785043 | Unsup=-0.137931 | TrainAcc=0.4246\n",
      "Fold 4 Epoch 500: TotalLoss=0.028307 | Sup=0.058132 | Unsup=-0.298247 | TrainAcc=0.9832\n",
      "Fold 4 Epoch 1000: TotalLoss=0.000664 | Sup=0.031014 | Unsup=-0.303497 | TrainAcc=0.9888\n",
      "Fold 4 Epoch 1500: TotalLoss=-0.022535 | Sup=0.008070 | Unsup=-0.306046 | TrainAcc=1.0000\n",
      "Fold 4 Epoch 2000: TotalLoss=-0.025371 | Sup=0.005408 | Unsup=-0.307790 | TrainAcc=1.0000\n",
      "Fold 4 → Acc=0.9330 | Prec=0.9552 | Rec=0.9233 | F1=0.9390 | AUC=0.9820 | CE Loss=0.2547\n",
      "\n",
      "=== Fold 5 ===\n",
      "Fold 5 Epoch 1: TotalLoss=0.967321 | Sup=0.983888 | Unsup=-0.165678 | TrainAcc=0.4497\n",
      "Fold 5 Epoch 500: TotalLoss=0.028861 | Sup=0.059089 | Unsup=-0.302276 | TrainAcc=0.9860\n",
      "Fold 5 Epoch 1000: TotalLoss=-0.002972 | Sup=0.027515 | Unsup=-0.304876 | TrainAcc=0.9944\n",
      "Fold 5 Epoch 1500: TotalLoss=-0.007321 | Sup=0.023460 | Unsup=-0.307812 | TrainAcc=0.9916\n",
      "Fold 5 Epoch 2000: TotalLoss=-0.015525 | Sup=0.015303 | Unsup=-0.308275 | TrainAcc=0.9972\n",
      "Fold 5 → Acc=0.9386 | Prec=0.9525 | Rec=0.9367 | F1=0.9445 | AUC=0.9831 | CE Loss=0.2425\n",
      "\n",
      "=== Fold 6 ===\n",
      "Fold 6 Epoch 1: TotalLoss=1.225833 | Sup=1.245382 | Unsup=-0.195498 | TrainAcc=0.5391\n",
      "Fold 6 Epoch 500: TotalLoss=0.054561 | Sup=0.084430 | Unsup=-0.298687 | TrainAcc=0.9665\n",
      "Fold 6 Epoch 1000: TotalLoss=0.014068 | Sup=0.044507 | Unsup=-0.304387 | TrainAcc=0.9916\n",
      "Fold 6 Epoch 1500: TotalLoss=0.007910 | Sup=0.038590 | Unsup=-0.306807 | TrainAcc=0.9888\n",
      "Fold 6 Epoch 2000: TotalLoss=-0.009224 | Sup=0.021530 | Unsup=-0.307533 | TrainAcc=0.9888\n",
      "Fold 6 → Acc=0.9436 | Prec=0.9742 | Rec=0.9233 | F1=0.9481 | AUC=0.9881 | CE Loss=0.1950\n",
      "\n",
      "=== Fold 7 ===\n",
      "Fold 7 Epoch 1: TotalLoss=0.737121 | Sup=0.750612 | Unsup=-0.134908 | TrainAcc=0.5447\n",
      "Fold 7 Epoch 500: TotalLoss=0.050195 | Sup=0.079560 | Unsup=-0.293645 | TrainAcc=0.9693\n",
      "Fold 7 Epoch 1000: TotalLoss=0.005393 | Sup=0.035303 | Unsup=-0.299102 | TrainAcc=0.9944\n",
      "Fold 7 Epoch 1500: TotalLoss=-0.002930 | Sup=0.027201 | Unsup=-0.301306 | TrainAcc=0.9972\n",
      "Fold 7 Epoch 2000: TotalLoss=-0.023132 | Sup=0.007169 | Unsup=-0.303017 | TrainAcc=1.0000\n",
      "Fold 7 → Acc=0.9414 | Prec=0.9564 | Rec=0.9378 | F1=0.9470 | AUC=0.9855 | CE Loss=0.2000\n",
      "\n",
      "=== Fold 8 ===\n",
      "Fold 8 Epoch 1: TotalLoss=0.739977 | Sup=0.754790 | Unsup=-0.148130 | TrainAcc=0.5950\n",
      "Fold 8 Epoch 500: TotalLoss=0.017829 | Sup=0.047895 | Unsup=-0.300658 | TrainAcc=0.9888\n",
      "Fold 8 Epoch 1000: TotalLoss=-0.004320 | Sup=0.026243 | Unsup=-0.305634 | TrainAcc=0.9916\n",
      "Fold 8 Epoch 1500: TotalLoss=-0.024366 | Sup=0.006198 | Unsup=-0.305641 | TrainAcc=1.0000\n",
      "Fold 8 Epoch 2000: TotalLoss=-0.022262 | Sup=0.008448 | Unsup=-0.307096 | TrainAcc=0.9972\n",
      "Fold 8 → Acc=0.9405 | Prec=0.9351 | Rec=0.9600 | F1=0.9474 | AUC=0.9839 | CE Loss=0.2845\n",
      "\n",
      "=== Fold 9 ===\n",
      "Fold 9 Epoch 1: TotalLoss=1.031164 | Sup=1.049167 | Unsup=-0.180027 | TrainAcc=0.5587\n",
      "Fold 9 Epoch 500: TotalLoss=-0.001426 | Sup=0.029792 | Unsup=-0.312181 | TrainAcc=0.9916\n",
      "Fold 9 Epoch 1000: TotalLoss=-0.017187 | Sup=0.014260 | Unsup=-0.314473 | TrainAcc=0.9944\n",
      "Fold 9 Epoch 1500: TotalLoss=-0.025305 | Sup=0.006226 | Unsup=-0.315309 | TrainAcc=0.9972\n",
      "Fold 9 Epoch 2000: TotalLoss=-0.029235 | Sup=0.002434 | Unsup=-0.316692 | TrainAcc=1.0000\n",
      "Fold 9 → Acc=0.9352 | Prec=0.9467 | Rec=0.9367 | F1=0.9416 | AUC=0.9802 | CE Loss=0.3582\n",
      "\n",
      "=== Fold 10 ===\n",
      "Fold 10 Epoch 1: TotalLoss=1.064234 | Sup=1.081619 | Unsup=-0.173854 | TrainAcc=0.4441\n",
      "Fold 10 Epoch 500: TotalLoss=0.029260 | Sup=0.059132 | Unsup=-0.298724 | TrainAcc=0.9749\n",
      "Fold 10 Epoch 1000: TotalLoss=-0.008536 | Sup=0.021593 | Unsup=-0.301290 | TrainAcc=1.0000\n",
      "Fold 10 Epoch 1500: TotalLoss=-0.020641 | Sup=0.009876 | Unsup=-0.305172 | TrainAcc=1.0000\n",
      "Fold 10 Epoch 2000: TotalLoss=-0.009760 | Sup=0.020977 | Unsup=-0.307369 | TrainAcc=0.9916\n",
      "Fold 10 → Acc=0.9498 | Prec=0.9606 | Rec=0.9489 | F1=0.9547 | AUC=0.9855 | CE Loss=0.2187\n",
      "\n",
      "=== Fold 11 ===\n",
      "Fold 11 Epoch 1: TotalLoss=0.798820 | Sup=0.812691 | Unsup=-0.138718 | TrainAcc=0.4497\n",
      "Fold 11 Epoch 500: TotalLoss=0.029328 | Sup=0.058928 | Unsup=-0.296010 | TrainAcc=0.9749\n",
      "Fold 11 Epoch 1000: TotalLoss=0.002796 | Sup=0.032658 | Unsup=-0.298629 | TrainAcc=0.9888\n",
      "Fold 11 Epoch 1500: TotalLoss=-0.016812 | Sup=0.013214 | Unsup=-0.300267 | TrainAcc=0.9972\n",
      "Fold 11 Epoch 2000: TotalLoss=-0.021520 | Sup=0.008725 | Unsup=-0.302452 | TrainAcc=1.0000\n",
      "Fold 11 → Acc=0.9358 | Prec=0.9612 | Rec=0.9222 | F1=0.9413 | AUC=0.9831 | CE Loss=0.2422\n",
      "\n",
      "=== Fold 12 ===\n",
      "Fold 12 Epoch 1: TotalLoss=0.759181 | Sup=0.773875 | Unsup=-0.146949 | TrainAcc=0.4749\n",
      "Fold 12 Epoch 500: TotalLoss=-0.000528 | Sup=0.030695 | Unsup=-0.312237 | TrainAcc=0.9888\n",
      "Fold 12 Epoch 1000: TotalLoss=-0.014901 | Sup=0.016521 | Unsup=-0.314222 | TrainAcc=0.9944\n",
      "Fold 12 Epoch 1500: TotalLoss=-0.023586 | Sup=0.007990 | Unsup=-0.315764 | TrainAcc=0.9972\n",
      "Fold 12 Epoch 2000: TotalLoss=-0.026221 | Sup=0.005381 | Unsup=-0.316023 | TrainAcc=1.0000\n",
      "Fold 12 → Acc=0.9426 | Prec=0.9617 | Rec=0.9344 | F1=0.9479 | AUC=0.9849 | CE Loss=0.2950\n",
      "\n",
      "=== Fold 13 ===\n",
      "Fold 13 Epoch 1: TotalLoss=0.968472 | Sup=0.984939 | Unsup=-0.164675 | TrainAcc=0.4721\n",
      "Fold 13 Epoch 500: TotalLoss=0.072532 | Sup=0.102423 | Unsup=-0.298914 | TrainAcc=0.9665\n",
      "Fold 13 Epoch 1000: TotalLoss=0.014978 | Sup=0.045237 | Unsup=-0.302593 | TrainAcc=0.9888\n",
      "Fold 13 Epoch 1500: TotalLoss=-0.013041 | Sup=0.017174 | Unsup=-0.302149 | TrainAcc=1.0000\n",
      "Fold 13 Epoch 2000: TotalLoss=-0.014274 | Sup=0.016075 | Unsup=-0.303489 | TrainAcc=1.0000\n",
      "Fold 13 → Acc=0.9442 | Prec=0.9597 | Rec=0.9394 | F1=0.9495 | AUC=0.9869 | CE Loss=0.1959\n",
      "\n",
      "=== Fold 14 ===\n",
      "Fold 14 Epoch 1: TotalLoss=0.746284 | Sup=0.761558 | Unsup=-0.152736 | TrainAcc=0.5196\n",
      "Fold 14 Epoch 500: TotalLoss=0.037469 | Sup=0.067450 | Unsup=-0.299816 | TrainAcc=0.9749\n",
      "Fold 14 Epoch 1000: TotalLoss=0.017378 | Sup=0.047745 | Unsup=-0.303669 | TrainAcc=0.9860\n",
      "Fold 14 Epoch 1500: TotalLoss=-0.014090 | Sup=0.016325 | Unsup=-0.304144 | TrainAcc=1.0000\n",
      "Fold 14 Epoch 2000: TotalLoss=-0.020045 | Sup=0.010491 | Unsup=-0.305358 | TrainAcc=0.9972\n",
      "Fold 14 → Acc=0.9371 | Prec=0.9555 | Rec=0.9306 | F1=0.9429 | AUC=0.9824 | CE Loss=0.2427\n",
      "\n",
      "=== Fold 15 ===\n",
      "Fold 15 Epoch 1: TotalLoss=0.859158 | Sup=0.874549 | Unsup=-0.153911 | TrainAcc=0.4581\n",
      "Fold 15 Epoch 500: TotalLoss=0.043534 | Sup=0.073876 | Unsup=-0.303423 | TrainAcc=0.9777\n",
      "Fold 15 Epoch 1000: TotalLoss=-0.000375 | Sup=0.029969 | Unsup=-0.303432 | TrainAcc=0.9916\n",
      "Fold 15 Epoch 1500: TotalLoss=-0.014127 | Sup=0.016992 | Unsup=-0.311192 | TrainAcc=0.9972\n",
      "Fold 15 Epoch 2000: TotalLoss=-0.019524 | Sup=0.011473 | Unsup=-0.309972 | TrainAcc=0.9972\n",
      "Fold 15 → Acc=0.9336 | Prec=0.9511 | Rec=0.9289 | F1=0.9399 | AUC=0.9826 | CE Loss=0.2921\n",
      "\n",
      "=== Fold 16 ===\n",
      "Fold 16 Epoch 1: TotalLoss=0.920767 | Sup=0.935640 | Unsup=-0.148736 | TrainAcc=0.4441\n",
      "Fold 16 Epoch 500: TotalLoss=0.035420 | Sup=0.065748 | Unsup=-0.303283 | TrainAcc=0.9777\n",
      "Fold 16 Epoch 1000: TotalLoss=0.001536 | Sup=0.032316 | Unsup=-0.307801 | TrainAcc=0.9860\n",
      "Fold 16 Epoch 1500: TotalLoss=-0.015101 | Sup=0.015906 | Unsup=-0.310069 | TrainAcc=0.9944\n",
      "Fold 16 Epoch 2000: TotalLoss=-0.018651 | Sup=0.012477 | Unsup=-0.311274 | TrainAcc=0.9972\n",
      "Fold 16 → Acc=0.9383 | Prec=0.9641 | Rec=0.9239 | F1=0.9435 | AUC=0.9848 | CE Loss=0.2405\n",
      "\n",
      "=== Fold 17 ===\n",
      "Fold 17 Epoch 1: TotalLoss=0.829577 | Sup=0.843934 | Unsup=-0.143575 | TrainAcc=0.4469\n",
      "Fold 17 Epoch 500: TotalLoss=0.038056 | Sup=0.068603 | Unsup=-0.305466 | TrainAcc=0.9693\n",
      "Fold 17 Epoch 1000: TotalLoss=-0.002167 | Sup=0.028497 | Unsup=-0.306642 | TrainAcc=0.9944\n",
      "Fold 17 Epoch 1500: TotalLoss=-0.007929 | Sup=0.023103 | Unsup=-0.310320 | TrainAcc=0.9916\n",
      "Fold 17 Epoch 2000: TotalLoss=-0.019131 | Sup=0.011965 | Unsup=-0.310951 | TrainAcc=0.9972\n",
      "Fold 17 → Acc=0.9501 | Prec=0.9643 | Rec=0.9456 | F1=0.9548 | AUC=0.9882 | CE Loss=0.2016\n",
      "\n",
      "=== Fold 18 ===\n",
      "Fold 18 Epoch 1: TotalLoss=0.639856 | Sup=0.654370 | Unsup=-0.145147 | TrainAcc=0.6145\n",
      "Fold 18 Epoch 500: TotalLoss=0.041794 | Sup=0.071443 | Unsup=-0.296492 | TrainAcc=0.9749\n",
      "Fold 18 Epoch 1000: TotalLoss=0.003247 | Sup=0.033601 | Unsup=-0.303535 | TrainAcc=0.9916\n",
      "Fold 18 Epoch 1500: TotalLoss=-0.010859 | Sup=0.019699 | Unsup=-0.305587 | TrainAcc=0.9944\n",
      "Fold 18 Epoch 2000: TotalLoss=-0.013608 | Sup=0.017139 | Unsup=-0.307475 | TrainAcc=0.9944\n",
      "Fold 18 → Acc=0.9423 | Prec=0.9676 | Rec=0.9278 | F1=0.9472 | AUC=0.9861 | CE Loss=0.2308\n",
      "\n",
      "=== Fold 19 ===\n",
      "Fold 19 Epoch 1: TotalLoss=0.937966 | Sup=0.955264 | Unsup=-0.172986 | TrainAcc=0.5615\n",
      "Fold 19 Epoch 500: TotalLoss=0.038238 | Sup=0.068425 | Unsup=-0.301874 | TrainAcc=0.9749\n",
      "Fold 19 Epoch 1000: TotalLoss=0.006450 | Sup=0.037044 | Unsup=-0.305939 | TrainAcc=0.9860\n",
      "Fold 19 Epoch 1500: TotalLoss=-0.007413 | Sup=0.023337 | Unsup=-0.307507 | TrainAcc=0.9944\n",
      "Fold 19 Epoch 2000: TotalLoss=-0.012864 | Sup=0.018097 | Unsup=-0.309609 | TrainAcc=0.9916\n",
      "Fold 19 → Acc=0.9389 | Prec=0.9641 | Rec=0.9250 | F1=0.9441 | AUC=0.9845 | CE Loss=0.2435\n",
      "\n",
      "=== Fold 20 ===\n",
      "Fold 20 Epoch 1: TotalLoss=0.694743 | Sup=0.708355 | Unsup=-0.136122 | TrainAcc=0.5391\n",
      "Fold 20 Epoch 500: TotalLoss=0.029929 | Sup=0.060150 | Unsup=-0.302207 | TrainAcc=0.9804\n",
      "Fold 20 Epoch 1000: TotalLoss=-0.004803 | Sup=0.025658 | Unsup=-0.304603 | TrainAcc=0.9944\n",
      "Fold 20 Epoch 1500: TotalLoss=-0.015587 | Sup=0.015059 | Unsup=-0.306461 | TrainAcc=0.9972\n",
      "Fold 20 Epoch 2000: TotalLoss=-0.022885 | Sup=0.007834 | Unsup=-0.307190 | TrainAcc=0.9972\n",
      "Fold 20 → Acc=0.9442 | Prec=0.9465 | Rec=0.9539 | F1=0.9502 | AUC=0.9862 | CE Loss=0.2102\n",
      "\n",
      "=== Average Results Across 20 Folds ===\n",
      "Accuracy:  0.9402 ± 0.0046\n",
      "Precision: 0.9560 ± 0.0098\n",
      "Recall:    0.9360 ± 0.0111\n",
      "F1-score:  0.9458 ± 0.0042\n",
      "AUC:       0.9846 ± 0.0020\n",
      "CE Loss:   0.2432 ± 0.0404\n"
     ]
    }
   ],
   "source": [
    "sss = StratifiedShuffleSplit(n_splits=20, test_size=0.9, random_state=42)\n",
    "\n",
    "accuracies, precisions, recalls, f1_scores, aucs, ce_losses = [], [], [], [], [], []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(sss.split(features, y_labels.astype(np.int64)), start=1):\n",
    "    print(f\"\\n=== Fold {fold} ===\")\n",
    "\n",
    "    # Convert to tensors\n",
    "    train_idx_t = torch.from_numpy(train_idx).long().to(device)\n",
    "    y_train_tensor = torch.from_numpy(y_labels[train_idx]).long().to(device)\n",
    "    A_tensor = torch.from_numpy(W).float().to(device)\n",
    "\n",
    "    # Initialize model\n",
    "    model = ARMA_SemiSupervised(feats_dim, hidden_dim, num_classes, device).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(data)\n",
    "        loss_sup = ce_loss(logits[train_idx_t], y_train_tensor)\n",
    "        loss_unsup = model.modularity_loss(A_tensor, logits)\n",
    "        total_loss = loss_sup + lambda_mod * loss_unsup\n",
    "\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % batch_print_freq == 0 or epoch == 1:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                preds_train = logits[train_idx_t].argmax(dim=1)\n",
    "                acc_train = accuracy_score(y_train_tensor.cpu(), preds_train.cpu())\n",
    "            print(f\"Fold {fold} Epoch {epoch}: \"\n",
    "                  f\"TotalLoss={total_loss.item():.6f} | Sup={loss_sup.item():.6f} | \"\n",
    "                  f\"Unsup={loss_unsup.item():.6f} | TrainAcc={acc_train:.4f}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data)\n",
    "        preds = out.argmax(dim=1).cpu().numpy()\n",
    "        probs = torch.softmax(out, dim=1)[:, 1].cpu().numpy()  # Probability for class 1\n",
    "\n",
    "    y_test = y_labels[test_idx]\n",
    "    y_pred_test = preds[test_idx]\n",
    "    y_prob_test = probs[test_idx]\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred_test)\n",
    "    prec = precision_score(y_test, y_pred_test, zero_division=0)\n",
    "    rec = recall_score(y_test, y_pred_test, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred_test, zero_division=0)\n",
    "    auc = roc_auc_score(y_test, y_prob_test)\n",
    "    ce = log_loss(y_test, y_prob_test)\n",
    "\n",
    "    accuracies.append(acc)\n",
    "    precisions.append(prec)\n",
    "    recalls.append(rec)\n",
    "    f1_scores.append(f1)\n",
    "    aucs.append(auc)\n",
    "    ce_losses.append(ce)\n",
    "\n",
    "    print(f\"Fold {fold} → \"\n",
    "          f\"Acc={acc:.4f} | Prec={prec:.4f} | Rec={rec:.4f} | \"\n",
    "          f\"F1={f1:.4f} | AUC={auc:.4f} | CE Loss={ce:.4f}\")\n",
    "\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n=== Average Results Across 20 Folds ===\")\n",
    "print(f\"Accuracy:  {np.mean(accuracies):.4f} ± {np.std(accuracies):.4f}\")\n",
    "print(f\"Precision: {np.mean(precisions):.4f} ± {np.std(precisions):.4f}\")\n",
    "print(f\"Recall:    {np.mean(recalls):.4f} ± {np.std(recalls):.4f}\")\n",
    "print(f\"F1-score:  {np.mean(f1_scores):.4f} ± {np.std(f1_scores):.4f}\")\n",
    "print(f\"AUC:       {np.mean(aucs):.4f} ± {np.std(aucs):.4f}\")\n",
    "print(f\"CE Loss:   {np.mean(ce_losses):.4f} ± {np.std(ce_losses):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
